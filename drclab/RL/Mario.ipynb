{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [TRAIN A MARIO-PLAYING RL AGENT](https://pytorch.org/tutorials/intermediate/mario_rl_tutorial.html#train-a-mario-playing-rl-agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%bash\n",
    "#pip install gym-super-mario-bros==7.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms as T\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import deque\n",
    "import random, datetime, os, copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym.spaces import Box\n",
    "from gym.wrappers import FrameStack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nes_py.wrappers import JoypadSpace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym_super_mario_bros"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [RL Definitions](https://pytorch.org/tutorials/intermediate/mario_rl_tutorial.html#rl-definitions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Environment** The world that an agent interacts with and learns from.\n",
    "\n",
    "**Action** $a$ : How the Agent responds to the Environment. The set of all possible Actions is called action-space.\n",
    "\n",
    "**State** $s$ : The current characteristic of the Environment. The set of all possible States the Environment can be in is called state-space.\n",
    "\n",
    "**Reward** $r$ : Reward is the key feedback from Environment to Agent. It is what drives the Agent to learn and to change its future action. An aggregation of rewards over multiple time steps is called Return.\n",
    "\n",
    "**Optimal Action-Value function** $Q^*(s,a)$ : Gives the expected return if you start in state ss, take an arbitrary action aa, and then for each future time step take the action that maximizes returns. QQ can be said to stand for the “quality” of the action in a state. We try to approximate this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v3\", render_mode='rgb', apply_api_compatibility=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = JoypadSpace(env, [['right'], ['right', 'A']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[104, 136, 252],\n",
       "         [104, 136, 252],\n",
       "         [104, 136, 252],\n",
       "         ...,\n",
       "         [104, 136, 252],\n",
       "         [104, 136, 252],\n",
       "         [104, 136, 252]],\n",
       " \n",
       "        [[104, 136, 252],\n",
       "         [104, 136, 252],\n",
       "         [104, 136, 252],\n",
       "         ...,\n",
       "         [104, 136, 252],\n",
       "         [104, 136, 252],\n",
       "         [104, 136, 252]],\n",
       " \n",
       "        [[104, 136, 252],\n",
       "         [104, 136, 252],\n",
       "         [104, 136, 252],\n",
       "         ...,\n",
       "         [104, 136, 252],\n",
       "         [104, 136, 252],\n",
       "         [104, 136, 252]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[228,  92,  16],\n",
       "         [228,  92,  16],\n",
       "         [228,  92,  16],\n",
       "         ...,\n",
       "         [228,  92,  16],\n",
       "         [228,  92,  16],\n",
       "         [228,  92,  16]],\n",
       " \n",
       "        [[228,  92,  16],\n",
       "         [228,  92,  16],\n",
       "         [228,  92,  16],\n",
       "         ...,\n",
       "         [228,  92,  16],\n",
       "         [228,  92,  16],\n",
       "         [228,  92,  16]],\n",
       " \n",
       "        [[228,  92,  16],\n",
       "         [228,  92,  16],\n",
       "         [228,  92,  16],\n",
       "         ...,\n",
       "         [228,  92,  16],\n",
       "         [228,  92,  16],\n",
       "         [228,  92,  16]]], dtype=uint8),\n",
       " {})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_state, reward, done, trunc, info = env.step(action=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240, 256, 3) \t 0.0 \t False\t{'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 400, 'world': 1, 'x_pos': 40, 'y_pos': 79}\n"
     ]
    }
   ],
   "source": [
    "print(f'{next_state.shape} \\t {reward} \\t {done}\\t{info}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
