{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [TRAIN A MARIO-PLAYING RL AGENT](https://pytorch.org/tutorials/intermediate/mario_rl_tutorial.html#train-a-mario-playing-rl-agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%bash\n",
    "#pip install gym-super-mario-bros==7.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms as T\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import deque\n",
    "import random, datetime, os, copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym.spaces import Box\n",
    "from gym.wrappers import FrameStack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nes_py.wrappers import JoypadSpace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym_super_mario_bros"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [RL Definitions](https://pytorch.org/tutorials/intermediate/mario_rl_tutorial.html#rl-definitions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Environment** The world that an agent interacts with and learns from.\n",
    "\n",
    "**Action** $a$ : How the Agent responds to the Environment. The set of all possible Actions is called action-space.\n",
    "\n",
    "**State** $s$ : The current characteristic of the Environment. The set of all possible States the Environment can be in is called state-space.\n",
    "\n",
    "**Reward** $r$ : Reward is the key feedback from Environment to Agent. It is what drives the Agent to learn and to change its future action. An aggregation of rewards over multiple time steps is called Return.\n",
    "\n",
    "**Optimal Action-Value function** $Q^*(s,a)$ : Gives the expected return if you start in state ss, take an arbitrary action aa, and then for each future time step take the action that maximizes returns. QQ can be said to stand for the “quality” of the action in a state. We try to approximate this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dulunche/.local/lib/python3.10/site-packages/gym/envs/registration.py:627: UserWarning: \u001b[33mWARN: The environment creator metadata doesn't include `render_modes`, contains: ['render.modes', 'video.frames_per_second']\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v3\", render_mode='rgb', apply_api_compatibility=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = JoypadSpace(env, [['right'], ['right', 'A']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[104, 136, 252],\n",
       "         [104, 136, 252],\n",
       "         [104, 136, 252],\n",
       "         ...,\n",
       "         [104, 136, 252],\n",
       "         [104, 136, 252],\n",
       "         [104, 136, 252]],\n",
       " \n",
       "        [[104, 136, 252],\n",
       "         [104, 136, 252],\n",
       "         [104, 136, 252],\n",
       "         ...,\n",
       "         [104, 136, 252],\n",
       "         [104, 136, 252],\n",
       "         [104, 136, 252]],\n",
       " \n",
       "        [[104, 136, 252],\n",
       "         [104, 136, 252],\n",
       "         [104, 136, 252],\n",
       "         ...,\n",
       "         [104, 136, 252],\n",
       "         [104, 136, 252],\n",
       "         [104, 136, 252]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[228,  92,  16],\n",
       "         [228,  92,  16],\n",
       "         [228,  92,  16],\n",
       "         ...,\n",
       "         [228,  92,  16],\n",
       "         [228,  92,  16],\n",
       "         [228,  92,  16]],\n",
       " \n",
       "        [[228,  92,  16],\n",
       "         [228,  92,  16],\n",
       "         [228,  92,  16],\n",
       "         ...,\n",
       "         [228,  92,  16],\n",
       "         [228,  92,  16],\n",
       "         [228,  92,  16]],\n",
       " \n",
       "        [[228,  92,  16],\n",
       "         [228,  92,  16],\n",
       "         [228,  92,  16],\n",
       "         ...,\n",
       "         [228,  92,  16],\n",
       "         [228,  92,  16],\n",
       "         [228,  92,  16]]], dtype=uint8),\n",
       " {})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_state, reward, done, trunc, info = env.step(action=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240, 256, 3) \t 0.0 \t False\t{'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 400, 'world': 1, 'x_pos': 40, 'y_pos': 79}\n"
     ]
    }
   ],
   "source": [
    "print(f'{next_state.shape} \\t {reward} \\t {done}\\t{info}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Preprocess Environment](https://pytorch.org/tutorials/intermediate/mario_rl_tutorial.html#preprocess-environment)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment data is returned to the agent in next_state. As you saw above, each state is represented by a [3, 240, 256] size array. Often that is more information than our agent needs; for instance, Mario’s actions do not depend on the color of the pipes or the sky!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipFrame(gym.Wrapper):\n",
    "    def __init__(self, env, skip):\n",
    "        super().__init__(env)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        total_rewards = 0.0\n",
    "        for i in range(self._skip):\n",
    "            obs, reward, done, trunk, info = self.env.step(action)\n",
    "            total_rewards += reward\n",
    "            if done:\n",
    "                break\n",
    "        return obs, total_rewards, done, trunk, info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SkipFrame(env, skip=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GrayScaleObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        obs_shape = self.observation_space.shape[:2]\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "    \n",
    "    def permute_orientation(self, observation):\n",
    "        observation = np.transpose(observation, (2,0,1))\n",
    "        observation = torch.tensor(observation.copy(), dtype=torch.float)\n",
    "        return observation\n",
    "\n",
    "    def observation(self, observation):\n",
    "        observation = self.permute_orientation(observation)\n",
    "        transform = T.Grayscale()\n",
    "        observation = transform(observation)\n",
    "        return observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GrayScaleObservation(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResizeObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env, shape):\n",
    "        super().__init__(env)\n",
    "        if isinstance(shape, int):\n",
    "            self.shape=(shape, shape)\n",
    "        else:\n",
    "            self.shape = tuple(shape)\n",
    "        \n",
    "        obs_shape = self.shape + self.observation_space.shape[2:]\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype = np.uint8)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        transforms = T.compose(\n",
    "            [T.Resize(self.shape), T.Normalize(0, 255)]\n",
    "        )\n",
    "\n",
    "        observation = transforms(observation).squeeze(0)\n",
    "        return observation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ResizeObservation(env, shape=84)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = FrameStack(env, num_stack=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://pytorch.org/tutorials/_images/mario_env.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Agent](https://pytorch.org/tutorials/intermediate/mario_rl_tutorial.html#agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarioNet(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        c, h, w = input_dim\n",
    "        \n",
    "        if h != 84:\n",
    "            raise ValueError(f\"Expecting input height: 84, got: {h}\")\n",
    "        if w != 84:\n",
    "            raise ValueError(f\"Expecting input width: 84, got: {w}\")\n",
    "\n",
    "        self.online = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=c, out_channels=32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
    "            nn.RelU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3136, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, output_dim)\n",
    "        )\n",
    "\n",
    "        self.target = copy.deepcopy(self.online)\n",
    "        \n",
    "        for p in self.target.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def forward(self, input, model):\n",
    "        if model == 'online':\n",
    "            return self.online(input)\n",
    "        elif model == 'target':\n",
    "            return self.target(input)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarioG1:\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.save_dir = save_dir\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "        self.net = MarioNet(self.state_dim, self.action_dim).float()\n",
    "        self.net = self.net.to(device=self.device)\n",
    "        \n",
    "        self.exploration_rate = 1\n",
    "        self.exploration_rate_decay = 0.9999975\n",
    "        self.exploration_rate_min = 0\n",
    "        self.curr_step = 0\n",
    "\n",
    "        self.save_every = 5e5\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"\n",
    "        For any given state, an agent can choose to do \n",
    "        the most optimal action (exploit) \n",
    "        or a random action (explore).\n",
    "        \"\"\"\n",
    "        ### Explore\n",
    "        if np.random.rand() < self.exploration_rate:\n",
    "            action_idx = np.random.randint(self.action_dim)\n",
    "\n",
    "        # Exploit\n",
    "        else:\n",
    "            state = state[0].__array__() if isinstance(state, tuple) else state.__array__()\n",
    "            state = torch.tensor(state, device=self.device).unsqueeze(0)\n",
    "            action_values = self.net(state, model='online')\n",
    "            action_idx = torch.argmax(action_values, axis=1).item()\n",
    "        \n",
    "        self.exploration_rate *= self.exploration_rate_decay\n",
    "        self.exploration_rate = max(self.exploration_rate_min, self.exploration_rate)\n",
    "\n",
    "        self.curr_step += 1\n",
    "        return action_idx\n",
    "\n",
    "    def cache(self, experience):\n",
    "        \"\"\"Add the experience to memory\"\"\"\n",
    "        pass\n",
    "    def recall(self):\n",
    "        \"\"\"\"Sample experiences from memory\"\"\"\n",
    "        pass\n",
    "    def learn(self):\n",
    "        \"\"\"Update online action value Q function with a batch of experiences\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarioG2(MarioG1):\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        super().__init__(state_dim, action_dim, save_dir)\n",
    "        self.memory = deque(maxlen=100000)\n",
    "        self.batch_size = 32\n",
    "\n",
    "    def cache(self, state, next_state, action, reward, done):\n",
    "        def first_if_tuple(x):\n",
    "            return x[0] if isinstance(x, tuple) else x\n",
    "        \n",
    "        state = first_if_tuple(state).__array__()\n",
    "        next_state = first_if_tuple(next_state).__array__()\n",
    "\n",
    "        state = torch.tensor(state, device=self.device)\n",
    "        next_state = torch.tensor(next_state, device=self.device)\n",
    "        action = torch.tensor([action], device=self.device)\n",
    "        reward = torch.tensor([reward], device=self.device)\n",
    "        done = torch.tensor([done], device=self.device)\n",
    "\n",
    "        self.memory.append((state, next_state, action, reward, done,))\n",
    "\n",
    "    def recall(self):\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        state, next_state, action, reward, done = map(torch.stack, zip(*batch))\n",
    "        return state, next_state, action.squeeze(), reward.squeeze(),  done.squeeze() "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Learn](https://pytorch.org/tutorials/intermediate/mario_rl_tutorial.html#learn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mario uses the DDQN algorithm under the hood. DDQN uses two ConvNets - $Q_{online}$ and $Q_{target}$ - that independently approximate the optimal action-value function.\n",
    "#### In our implementation, we share feature generator features across $Q_{online}$ and $Q_{target}$, but maintain separate FC classifiers for each. $\\theta_{target}$ (the parameters of $Q_{target}$) is frozen to prevent updation by backprop. Instead, it is periodically synced with $\\theta_{online}$ (more on this later)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$TD_e=Q^∗_{online} (s,a)$ \n",
    "\n",
    "$a^{'} =\\underset{a}{\\arg\\max} Q_{online}(s^{'},a)$ action and next state\n",
    "\n",
    "$TD_t=r + \\gamma Q^∗_{target} (s^{'},a^{'})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarioG3(MarioG2):\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        super().__init__(state_dim, action_dim, save_dir)\n",
    "        self.gamma = 0.9\n",
    "    def td_estimate(self, state, action):\n",
    "        current_Q = self.net(state, model='online')[\n",
    "            np.arange(0, self.batch_size), action\n",
    "        ]\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def td_target(self, reward, next_state, done):\n",
    "        next_state_Q = self.net(next_state, model='online')\n",
    "        best_action = torch.argmax(next_state_Q, axis=1)\n",
    "        next_Q = self.net(next_state, model='target')[\n",
    "            np.arange(0, self.batch_size), best_action\n",
    "        ]\n",
    "        return (reward + (1 - done.float()) * self.gamma * next_Q).float()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Updating the model](https://pytorch.org/tutorials/intermediate/mario_rl_tutorial.html#updating-the-model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$θ_{online} ← θ_{online} + α∇(TD_e−TD_t)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarioG4(MarioG3):\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        super().__init__(state_dim, action_dim, save_dir)\n",
    "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=0.00025)\n",
    "        self.loss_fn = torch.nn.SmoothL1Loss()\n",
    "\n",
    "    def update_Q_online(self, td_e, td_t):\n",
    "        loss = self.loss_fn(td_e, td_t)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "    def sync_Q_target(self):\n",
    "        self.net.target.load_state_dict(self.net.online.state_dict())\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Save checkpoint](https://pytorch.org/tutorials/intermediate/mario_rl_tutorial.html#save-checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "home = Path.home()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/dulunche')"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarioG5(MarioG4):\n",
    "    def save(self):\n",
    "        save_path = (\n",
    "           home / self.save_dir / f'mario_net{self.curr_step // self.save_every}.chkpt'\n",
    "        )\n",
    "        torch.save(\n",
    "            dict(model=self.net.state_dict(), exploration_rate = self.exploration_rate),\n",
    "            save_path,\n",
    "        )\n",
    "\n",
    "        print(f\"MarioNet saved to {save_path} at step {self.curr_step}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Final Mario](https://pytorch.org/tutorials/intermediate/mario_rl_tutorial.html#putting-it-all-together)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mario(MarioG5):\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        super().__init__(state_dim, action_dim, save_dir)\n",
    "        self.burnin = 1e4\n",
    "        self.learn_every = 3 \n",
    "        self.sync_every = 1e4\n",
    "    \n",
    "    def learn(self):\n",
    "        if self.curr_step % self.sync_every == 0:\n",
    "            self.sync_Q_target()\n",
    "        if self.curr_step % self.save_every == 0:\n",
    "            self.save()\n",
    "        if self.curr_step < self.burnin:\n",
    "            return None, None\n",
    "        if self.curr_step % self.learn_every != 0:\n",
    "            return None, None\n",
    "\n",
    "        state, next_state, action, reward, done = self.recall()\n",
    "        td_est = self.td_estimate(state, action)\n",
    "        td_tgt = self.td_target(reward, next_state, done)\n",
    "\n",
    "        loss = self.update_Q_online(td_est, td_tgt)\n",
    "\n",
    "        return (td_est.mean().item(), loss) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Logging](https://pytorch.org/tutorials/intermediate/mario_rl_tutorial.html#logging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, datetime\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricLogger:\n",
    "    def __init__(self, save_dir) -> None:\n",
    "        \n",
    "        save_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.save_log = save_dir / 'log'\n",
    "        with open(self.save_log, 'w') as f:\n",
    "            f.write(\n",
    "                f\"{'Episode':>8}{'Step':>8}{'Epsilon':>10}{'MeanReward':>15}\"\n",
    "                f\"{'MeanLength':>15}{'MeanLoss':>15}{'MeanQValue':>15}\"\n",
    "                f\"{'TimeDelta':>15}{'Time':>20}\\n\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = home / 'mario_net' / f\"{datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S'):<20}\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
