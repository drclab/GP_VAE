{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import constraints\n",
    "\n",
    "import pyro\n",
    "import pyro.contrib.examples.polyphonic_data_loader as poly\n",
    "import pyro.distributions as dist\n",
    "from pyro import poutine\n",
    "from pyro.infer import SVI, JitTraceEnum_ELBO, TraceEnum_ELBO, TraceTMC_ELBO\n",
    "from pyro.infer.autoguide import AutoDelta\n",
    "from pyro.ops.indexing import Vindex\n",
    "from pyro.optim import Adam\n",
    "from pyro.util import ignore_jit_warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = logging.getLogger()\n",
    "debug_handler = logging.StreamHandler(sys.stdout)\n",
    "debug_handler.setLevel(logging.DEBUG)\n",
    "debug_handler.addFilter(filter=lambda record: record.levelno <= logging.DEBUG)\n",
    "log.addHandler(debug_handler)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next let's make our simple model faster in two ways: first we'll support vectorized minibatches of data, and second we'll support the PyTorch jit compiler.  To add batch support, we'll introduce a second plate \"sequences\" and randomly subsample data to size batch_size.  To add jit support we silence some warnings and try to avoid dynamic program structure.\n",
    "\n",
    "#### Note that this is the \"HMM\" model in reference [1] (with the difference that in [1] the probabilities probs_x and probs_y are not MAP-regularized with Dirichlet and Beta distributions for any of the models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__name__': '__main__',\n",
       " '__doc__': 'Automatically created module for IPython interactive environment',\n",
       " '__package__': None,\n",
       " '__loader__': None,\n",
       " '__spec__': None,\n",
       " '__builtin__': <module 'builtins' (built-in)>,\n",
       " '__builtins__': <module 'builtins' (built-in)>,\n",
       " '_ih': ['',\n",
       "  'import argparse\\nimport logging\\nimport sys\\n\\nimport torch\\nimport torch.nn as nn\\nfrom torch.distributions import constraints\\n\\nimport pyro\\nimport pyro.contrib.examples.polyphonic_data_loader as poly\\nimport pyro.distributions as dist\\nfrom pyro import poutine\\nfrom pyro.infer import SVI, JitTraceEnum_ELBO, TraceEnum_ELBO, TraceTMC_ELBO\\nfrom pyro.infer.autoguide import AutoDelta\\nfrom pyro.ops.indexing import Vindex\\nfrom pyro.optim import Adam\\nfrom pyro.util import ignore_jit_warnings',\n",
       "  'log = logging.getLogger()\\ndebug_handler = logging.StreamHandler(sys.stdout)\\ndebug_handler.setLevel(logging.DEBUG)\\ndebug_handler.addFilter(filter=lambda record: record.levelno <= logging.DEBUG)\\nlog.addHandler(debug_handler)',\n",
       "  'globals()',\n",
       "  'models = {\\n    name[len(\"model_\") :]: model\\n    for name, model in globals().items()\\n    if name.startswith(\"model_\")\\n}',\n",
       "  'logging.info(\"-\" * 40)\\nmodel = models[args.model]\\nlogging.info(\\n    \"Training {} on {} sequences\".format(\\n        model.__name__, len(data[\"train\"][\"sequences\"])\\n    )\\n)',\n",
       "  'data = poly.load_data(poly.JSB_CHORALES)',\n",
       "  \"seqs = data['train']['sequences']\\nlengths = data['train']['sequence_lengths']\",\n",
       "  'seqs.shape',\n",
       "  'present_notes = (seqs == 1).sum(0).sum(0) > 0',\n",
       "  'seqs = seqs[..., present_notes].shape',\n",
       "  \"def model_1(seqs, lengths, hd=16, batch_size = None, include_prior=True):\\n    with ignore_jit_warnings():\\n        num_seqs, max_len, data_dim = map(int, seqs.shape)\\n        assert lengths.shape == (num_seqs,)\\n        assert lengths.max() == max_len\\n    with poutine.mask(mask=include_prior):\\n        probs_x = pyro.sample(\\n            'probs_x',\\n            dist.Dirichlet(0.9 * torch.eye(hd) + 0.1).to_event(1),\\n        )\\n\\n        probs_y = pyro.sample(\\n            'probs_y',\\n            dist.Beta(0.1, 0.9).expand([hd, data_dim]).to_event(2)\\n        )\\n\\n    tones_plate = pyro.plate('tones', data_dim, dim=-1)\\n    # We subsample batch_size items out of num_sequences items. Note that since\\n    # we're using dim=-1 for the notes plate, we need to batch over a different\\n    # dimension, here dim=-2.\\n    with pyro.plate('seqs', num_seqs, batch_size, dim=-2) as batch:\\n        lengths = lengths[batch]\\n        x = 0\\n        # If we are not using the jit, then we can vary the program structure\\n        # each call by running for a dynamically determined number of time\\n        # steps, lengths.max(). However if we are using the jit, then we try to\\n        # keep a single program structure for all minibatches; the fixed\\n        # structure ends up being faster since each program structure would\\n        # need to trigger a new jit compile stage.    \\n        for t in pyro.markov(range(lengths.max())):\\n            with poutine.mask(mask=(t < lengths).unsqueeze(-1)):\\n                x = pyro.sample(\\n                    'x_{}'.format(t),\\n                    dist.Categorical(probs_x[x]),\\n                    infer={'enumerate': 'parallel'}\\n                )\\n                with tones_plate:\\n                    pyro.sample(\\n                        'y_{}'.format(t),\\n                        dist.Bernoulli(probs_y[x.squeeze(-1)]),\\n                        obs = seqs[batch, t]\\n                    )\\n\\n        \",\n",
       "  \"guide = AutoDelta(\\n    poutine.block(model_1, expose_fn=lambda msg: msg['name'].startswith('probs_'))\\n)\",\n",
       "  'first_available_dim = -3',\n",
       "  'guide_trace = poutine.trace(guide).get_trace(\\n    seqs, lengths, hd=16, batch_size=10\\n)',\n",
       "  'data = poly.load_data(poly.JSB_CHORALES)',\n",
       "  \"seqs = data['train']['sequences']\\nlengths = data['train']['sequence_lengths']\",\n",
       "  'seqs.shape',\n",
       "  'present_notes = (seqs == 1).sum(0).sum(0) > 0',\n",
       "  'seqs = seqs[..., present_notes].shape',\n",
       "  \"def model_1(seqs, lengths, hd=16, batch_size = None, include_prior=True):\\n    with ignore_jit_warnings():\\n        num_seqs, max_len, data_dim = map(int, seqs.shape)\\n        assert lengths.shape == (num_seqs,)\\n        assert lengths.max() == max_len\\n    with poutine.mask(mask=include_prior):\\n        probs_x = pyro.sample(\\n            'probs_x',\\n            dist.Dirichlet(0.9 * torch.eye(hd) + 0.1).to_event(1),\\n        )\\n\\n        probs_y = pyro.sample(\\n            'probs_y',\\n            dist.Beta(0.1, 0.9).expand([hd, data_dim]).to_event(2)\\n        )\\n\\n    tones_plate = pyro.plate('tones', data_dim, dim=-1)\\n    # We subsample batch_size items out of num_sequences items. Note that since\\n    # we're using dim=-1 for the notes plate, we need to batch over a different\\n    # dimension, here dim=-2.\\n    with pyro.plate('seqs', num_seqs, batch_size, dim=-2) as batch:\\n        lengths = lengths[batch]\\n        x = 0\\n        # If we are not using the jit, then we can vary the program structure\\n        # each call by running for a dynamically determined number of time\\n        # steps, lengths.max(). However if we are using the jit, then we try to\\n        # keep a single program structure for all minibatches; the fixed\\n        # structure ends up being faster since each program structure would\\n        # need to trigger a new jit compile stage.    \\n        for t in pyro.markov(range(lengths.max())):\\n            with poutine.mask(mask=(t < lengths).unsqueeze(-1)):\\n                x = pyro.sample(\\n                    'x_{}'.format(t),\\n                    dist.Categorical(probs_x[x]),\\n                    infer={'enumerate': 'parallel'}\\n                )\\n                with tones_plate:\\n                    pyro.sample(\\n                        'y_{}'.format(t),\\n                        dist.Bernoulli(probs_y[x.squeeze(-1)]),\\n                        obs = seqs[batch, t]\\n                    )\\n\\n        \",\n",
       "  \"guide = AutoDelta(\\n    poutine.block(model_1, expose_fn=lambda msg: msg['name'].startswith('probs_'))\\n)\",\n",
       "  'first_available_dim = -3',\n",
       "  'guide_trace = poutine.trace(guide).get_trace(\\n    seqs, lengths, hd=16, batch_size=10\\n)',\n",
       "  \"def model_1(seqs, lengths, hd=16, batch_size = None, include_prior=True):\\n    with ignore_jit_warnings():\\n        num_seqs, max_len, data_dim = map(int, seqs.shape)\\n        assert lengths.shape == (num_seqs,)\\n        assert lengths.max() == max_len\\n    with poutine.mask(mask=include_prior):\\n        probs_x = pyro.sample(\\n            'probs_x',\\n            dist.Dirichlet(0.9 * torch.eye(hd) + 0.1).to_event(1),\\n        )\\n\\n        probs_y = pyro.sample(\\n            'probs_y',\\n            dist.Beta(0.1, 0.9).expand([hd, data_dim]).to_event(2)\\n        )\\n\\n    tones_plate = pyro.plate('tones', data_dim, dim=-1)\\n    # We subsample batch_size items out of num_sequences items. Note that since\\n    # we're using dim=-1 for the notes plate, we need to batch over a different\\n    # dimension, here dim=-2.\\n    with pyro.plate('seqs', num_seqs, batch_size, dim=-2) as batch:\\n        lengths = lengths[batch]\\n        x = 0\\n        # If we are not using the jit, then we can vary the program structure\\n        # each call by running for a dynamically determined number of time\\n        # steps, lengths.max(). However if we are using the jit, then we try to\\n        # keep a single program structure for all minibatches; the fixed\\n        # structure ends up being faster since each program structure would\\n        # need to trigger a new jit compile stage.    \\n        for t in pyro.markov(range(lengths.max())):\\n            with poutine.mask(mask=(t < lengths).unsqueeze(-1)):\\n                x = pyro.sample(\\n                    'x_{}'.format(t),\\n                    dist.Categorical(probs_x[x]),\\n                    infer={'enumerate': 'parallel'}\\n                )\\n                with tones_plate:\\n                    pyro.sample(\\n                        'y_{}'.format(t),\\n                        dist.Bernoulli(probs_y[x.squeeze(-1)]),\\n                        obs = seqs[batch, t]\\n                    )\\n\\n        \",\n",
       "  \"guide = AutoDelta(\\n    poutine.block(model_1, expose_fn=lambda msg: msg['name'].startswith('probs_'))\\n)\",\n",
       "  'first_available_dim = -3',\n",
       "  'guide_trace = poutine.trace(guide).get_trace(\\n    seqs, lengths, hd=16, batch_size=10\\n)',\n",
       "  'guide_trace = poutine.trace(guide).get_trace(\\n    seqs, lengths, hd=16, batch_size=10\\n)',\n",
       "  \"def model_1(seqs, lengths, hd=16, batch_size = None, include_prior=True):\\n    with ignore_jit_warnings():\\n        num_seqs, max_len, data_dim = map(int, seqs.shape)\\n        assert lengths.shape == (num_seqs,)\\n        assert lengths.max() == max_len\\n    with poutine.mask(mask=include_prior):\\n        probs_x = pyro.sample(\\n            'probs_x',\\n            dist.Dirichlet(0.9 * torch.eye(hd) + 0.1).to_event(1),\\n        )\\n\\n        probs_y = pyro.sample(\\n            'probs_y',\\n            dist.Beta(0.1, 0.9).expand([hd, data_dim]).to_event(2)\\n        )\\n\\n    tones_plate = pyro.plate('tones', data_dim, dim=-1)\\n    # We subsample batch_size items out of num_sequences items. Note that since\\n    # we're using dim=-1 for the notes plate, we need to batch over a different\\n    # dimension, here dim=-2.\\n    with pyro.plate('seqs', num_seqs, batch_size, dim=-2) as batch:\\n        lengths = lengths[batch]\\n        x = 0\\n        # If we are not using the jit, then we can vary the program structure\\n        # each call by running for a dynamically determined number of time\\n        # steps, lengths.max(). However if we are using the jit, then we try to\\n        # keep a single program structure for all minibatches; the fixed\\n        # structure ends up being faster since each program structure would\\n        # need to trigger a new jit compile stage.    \\n        for t in pyro.markov(range(lengths.max())):\\n            with poutine.mask(mask=(t < lengths).unsqueeze(-1)):\\n                x = pyro.sample(\\n                    'x_{}'.format(t),\\n                    dist.Categorical(probs_x[x]),\\n                    infer={'enumerate': 'parallel'}\\n                )\\n                with tones_plate:\\n                    pyro.sample(\\n                        'y_{}'.format(t),\\n                        dist.Bernoulli(probs_y[x.squeeze(-1)]),\\n                        obs = seqs[batch, t]\\n                    )\\n\\n        \",\n",
       "  \"guide = AutoDelta(\\n    poutine.block(model_1, expose_fn=lambda msg: msg['name'].startswith('probs_'))\\n)\",\n",
       "  'first_available_dim = -3',\n",
       "  'guide_trace = poutine.trace(guide).get_trace(\\n    seqs, lengths, hd=16, batch_size=10\\n)',\n",
       "  'seqs = seqs[..., present_notes]',\n",
       "  \"seqs = data['train']['sequences']\\nlengths = data['train']['sequence_lengths']\",\n",
       "  'seqs.shape',\n",
       "  \"def model_1(seqs, lengths, hd=16, batch_size = None, include_prior=True):\\n    with ignore_jit_warnings():\\n        num_seqs, max_len, data_dim = map(int, seqs.shape)\\n        assert lengths.shape == (num_seqs,)\\n        assert lengths.max() == max_len\\n    with poutine.mask(mask=include_prior):\\n        probs_x = pyro.sample(\\n            'probs_x',\\n            dist.Dirichlet(0.9 * torch.eye(hd) + 0.1).to_event(1),\\n        )\\n\\n        probs_y = pyro.sample(\\n            'probs_y',\\n            dist.Beta(0.1, 0.9).expand([hd, data_dim]).to_event(2)\\n        )\\n\\n    tones_plate = pyro.plate('tones', data_dim, dim=-1)\\n    # We subsample batch_size items out of num_sequences items. Note that since\\n    # we're using dim=-1 for the notes plate, we need to batch over a different\\n    # dimension, here dim=-2.\\n    with pyro.plate('seqs', num_seqs, batch_size, dim=-2) as batch:\\n        lengths = lengths[batch]\\n        x = 0\\n        # If we are not using the jit, then we can vary the program structure\\n        # each call by running for a dynamically determined number of time\\n        # steps, lengths.max(). However if we are using the jit, then we try to\\n        # keep a single program structure for all minibatches; the fixed\\n        # structure ends up being faster since each program structure would\\n        # need to trigger a new jit compile stage.    \\n        for t in pyro.markov(range(lengths.max())):\\n            with poutine.mask(mask=(t < lengths).unsqueeze(-1)):\\n                x = pyro.sample(\\n                    'x_{}'.format(t),\\n                    dist.Categorical(probs_x[x]),\\n                    infer={'enumerate': 'parallel'}\\n                )\\n                with tones_plate:\\n                    pyro.sample(\\n                        'y_{}'.format(t),\\n                        dist.Bernoulli(probs_y[x.squeeze(-1)]),\\n                        obs = seqs[batch, t]\\n                    )\\n\\n        \",\n",
       "  \"guide = AutoDelta(\\n    poutine.block(model_1, expose_fn=lambda msg: msg['name'].startswith('probs_'))\\n)\",\n",
       "  'first_available_dim = -3',\n",
       "  'guide_trace = poutine.trace(guide).get_trace(\\n    seqs, lengths, hd=16, batch_size=10\\n)',\n",
       "  'model_trace = poutine.trace(\\n    poutine.replay(poutine.enum(model_1, first_available_dim), guide_trace)\\n    ).get_trace(\\n        seqs, lengths, hd = 16, batch_size = 10\\n    )',\n",
       "  'print(model_trace.format_shapes())',\n",
       "  \"optim = Adam({'lr':0.01})\\nElbo = TraceEnum_ELBO\\nelbo = Elbo(\\n    max_plate_nesting = 2,\\n    strict_enumeration_warning=True,\\n    jit_options={'time_compilation': 'store_true'}\\n)\",\n",
       "  'svi = SVI(model_1, guide, optim, elbo)',\n",
       "  \"num_steps = 100\\npyro.set_rng_seed(111)\\npyro.clear_param_store()\\nnum_observations = float(lengths.sum())\\nfor step in range(num_steps):\\n    loss = svi.step(seqs, lengths, hd=16, batch_size=20)\\n    print('{:5d}\\\\t{}'.format(step, loss / num_observations))\",\n",
       "  'seqs = seqs[., present_notes]',\n",
       "  'seqs = seqs[..., present_notes]',\n",
       "  \"def model_1(seqs, lengths, hd=16, batch_size = None, include_prior=True):\\n    with ignore_jit_warnings():\\n        num_seqs, max_len, data_dim = map(int, seqs.shape)\\n        assert lengths.shape == (num_seqs,)\\n        assert lengths.max() == max_len\\n    with poutine.mask(mask=include_prior):\\n        probs_x = pyro.sample(\\n            'probs_x',\\n            dist.Dirichlet(0.9 * torch.eye(hd) + 0.1).to_event(1),\\n        )\\n\\n        probs_y = pyro.sample(\\n            'probs_y',\\n            dist.Beta(0.1, 0.9).expand([hd, data_dim]).to_event(2)\\n        )\\n\\n    tones_plate = pyro.plate('tones', data_dim, dim=-1)\\n    # We subsample batch_size items out of num_sequences items. Note that since\\n    # we're using dim=-1 for the notes plate, we need to batch over a different\\n    # dimension, here dim=-2.\\n    with pyro.plate('seqs', num_seqs, batch_size, dim=-2) as batch:\\n        lengths = lengths[batch]\\n        x = 0\\n        # If we are not using the jit, then we can vary the program structure\\n        # each call by running for a dynamically determined number of time\\n        # steps, lengths.max(). However if we are using the jit, then we try to\\n        # keep a single program structure for all minibatches; the fixed\\n        # structure ends up being faster since each program structure would\\n        # need to trigger a new jit compile stage.    \\n        for t in pyro.markov(range(lengths.max())):\\n            with poutine.mask(mask=(t < lengths).unsqueeze(-1)):\\n                x = pyro.sample(\\n                    'x_{}'.format(t),\\n                    dist.Categorical(probs_x[x]),\\n                    infer={'enumerate': 'parallel'}\\n                )\\n                with tones_plate:\\n                    pyro.sample(\\n                        'y_{}'.format(t),\\n                        dist.Bernoulli(probs_y[x.squeeze(-1)]),\\n                        obs = seqs[batch, t]\\n                    )\\n\\n        \",\n",
       "  \"guide = AutoDelta(\\n    poutine.block(model_1, expose_fn=lambda msg: msg['name'].startswith('probs_'))\\n)\",\n",
       "  'first_available_dim = -3',\n",
       "  'guide_trace = poutine.trace(guide).get_trace(\\n    seqs, lengths, hd=16, batch_size=10\\n)',\n",
       "  'model_trace = poutine.trace(\\n    poutine.replay(poutine.enum(model_1, first_available_dim), guide_trace)\\n    ).get_trace(\\n        seqs, lengths, hd = 16, batch_size = 10\\n    )',\n",
       "  'model_trace = poutine.trace(\\n    poutine.replay(poutine.enum(model_1, first_available_dim), guide_trace)\\n    ).get_trace(\\n        seqs, lengths, hd = 16, batch_size = 10\\n    )',\n",
       "  'seqs',\n",
       "  'seqs.shape',\n",
       "  \"def model_1(seqs, lengths, hd=16, batch_size = None, include_prior=True):\\n    with ignore_jit_warnings():\\n        num_seqs, max_len, data_dim = map(int, seqs.shape)\\n        assert lengths.shape == (num_seqs,)\\n        assert lengths.max() == max_len\\n    with poutine.mask(mask=include_prior):\\n        probs_x = pyro.sample(\\n            'probs_x',\\n            dist.Dirichlet(0.9 * torch.eye(hd) + 0.1).to_event(1),\\n        )\\n\\n        probs_y = pyro.sample(\\n            'probs_y',\\n            dist.Beta(0.1, 0.9).expand([hd, data_dim]).to_event(2)\\n        )\\n\\n    tones_plate = pyro.plate('tones', data_dim, dim=-1)\\n    # We subsample batch_size items out of num_sequences items. Note that since\\n    # we're using dim=-1 for the notes plate, we need to batch over a different\\n    # dimension, here dim=-2.\\n    with pyro.plate('seqs', num_seqs, batch_size, dim=-2) as batch:\\n        lengths = lengths[batch]\\n        x = 0\\n        # If we are not using the jit, then we can vary the program structure\\n        # each call by running for a dynamically determined number of time\\n        # steps, lengths.max(). However if we are using the jit, then we try to\\n        # keep a single program structure for all minibatches; the fixed\\n        # structure ends up being faster since each program structure would\\n        # need to trigger a new jit compile stage.    \\n        for t in pyro.markov(range(lengths.max())):\\n            with poutine.mask(mask=(t < lengths).unsqueeze(-1)):\\n                x = pyro.sample(\\n                    'x_{}'.format(t),\\n                    dist.Categorical(probs_x[x]),\\n                    infer={'enumerate': 'parallel'}\\n                )\\n                with tones_plate:\\n                    pyro.sample(\\n                        'y_{}'.format(t),\\n                        dist.Bernoulli(probs_y[x.squeeze(-1)]),\\n                        obs = seqs[batch, t]\\n                    )\\n\\n        \",\n",
       "  \"guide = AutoDelta(\\n    poutine.block(model_1, expose_fn=lambda msg: msg['name'].startswith('probs_'))\\n)\",\n",
       "  'first_available_dim = -3',\n",
       "  'guide_trace = poutine.trace(guide).get_trace(\\n    seqs, lengths, hd=16, batch_size=10\\n)',\n",
       "  'model_trace = poutine.trace(\\n    poutine.replay(poutine.enum(model_1, first_available_dim), guide_trace)\\n    ).get_trace(\\n        seqs, lengths, hd = 16, batch_size = 10\\n    )',\n",
       "  'guide_trace = poutine.trace(guide).get_trace(\\n    seqs, lengths, hd=16, batch_size=10\\n)',\n",
       "  'data = poly.load_data(poly.JSB_CHORALES)',\n",
       "  \"seqs = data['train']['sequences']\\nlengths = data['train']['sequence_lengths']\",\n",
       "  'seqs.shape',\n",
       "  'present_notes = (seqs == 1).sum(0).sum(0) > 0',\n",
       "  '#seqs = seqs[..., present_notes]',\n",
       "  \"def model_1(seqs, lengths, hd=16, batch_size = None, include_prior=True):\\n    with ignore_jit_warnings():\\n        num_seqs, max_len, data_dim = map(int, seqs.shape)\\n        assert lengths.shape == (num_seqs,)\\n        assert lengths.max() == max_len\\n    with poutine.mask(mask=include_prior):\\n        probs_x = pyro.sample(\\n            'probs_x',\\n            dist.Dirichlet(0.9 * torch.eye(hd) + 0.1).to_event(1),\\n        )\\n\\n        probs_y = pyro.sample(\\n            'probs_y',\\n            dist.Beta(0.1, 0.9).expand([hd, data_dim]).to_event(2)\\n        )\\n\\n    tones_plate = pyro.plate('tones', data_dim, dim=-1)\\n    # We subsample batch_size items out of num_sequences items. Note that since\\n    # we're using dim=-1 for the notes plate, we need to batch over a different\\n    # dimension, here dim=-2.\\n    with pyro.plate('seqs', num_seqs, batch_size, dim=-2) as batch:\\n        lengths = lengths[batch]\\n        x = 0\\n        # If we are not using the jit, then we can vary the program structure\\n        # each call by running for a dynamically determined number of time\\n        # steps, lengths.max(). However if we are using the jit, then we try to\\n        # keep a single program structure for all minibatches; the fixed\\n        # structure ends up being faster since each program structure would\\n        # need to trigger a new jit compile stage.    \\n        for t in pyro.markov(range(lengths.max())):\\n            with poutine.mask(mask=(t < lengths).unsqueeze(-1)):\\n                x = pyro.sample(\\n                    'x_{}'.format(t),\\n                    dist.Categorical(probs_x[x]),\\n                    infer={'enumerate': 'parallel'}\\n                )\\n                with tones_plate:\\n                    pyro.sample(\\n                        'y_{}'.format(t),\\n                        dist.Bernoulli(probs_y[x.squeeze(-1)]),\\n                        obs = seqs[batch, t]\\n                    )\\n\\n        \",\n",
       "  \"guide = AutoDelta(\\n    poutine.block(model_1, expose_fn=lambda msg: msg['name'].startswith('probs_'))\\n)\",\n",
       "  'first_available_dim = -3',\n",
       "  'guide_trace = poutine.trace(guide).get_trace(\\n    seqs, lengths, hd=16, batch_size=10\\n)',\n",
       "  'model_trace = poutine.trace(\\n    poutine.replay(poutine.enum(model_1, first_available_dim), guide_trace)\\n    ).get_trace(\\n        seqs, lengths, hd = 16, batch_size = 10\\n    )',\n",
       "  'print(model_trace.format_shapes())',\n",
       "  \"# Notice that we're now using dim=-2 as a batch dimension (of size 10),\\n# and that the enumeration dimensions are now dims -3 and -4.\",\n",
       "  \"optim = Adam({'lr':0.01})\\nElbo = TraceEnum_ELBO\\nelbo = Elbo(\\n    max_plate_nesting = 2,\\n    strict_enumeration_warning=True,\\n    jit_options={'time_compilation': 'store_true'}\\n)\",\n",
       "  'svi = SVI(model_1, guide, optim, elbo)',\n",
       "  \"num_steps = 100\\npyro.set_rng_seed(111)\\npyro.clear_param_store()\\nnum_observations = float(lengths.sum())\\nfor step in range(num_steps):\\n    loss = svi.step(seqs, lengths, hd=16, batch_size=20)\\n    print('{:5d}\\\\t{}'.format(step, loss / num_observations))\",\n",
       "  \"test_sequences = data['test']['sequences']\\ntest_lengths = data['test']['sequence_lengths']\",\n",
       "  'test_loss = elbo.loss(\\n    model_0,\\n    guide,\\n    test_sequences,\\n    test_lengths,\\n    hidden_dim = 16\\n)',\n",
       "  'test_loss = elbo.loss(\\n    model_1,\\n    guide,\\n    test_sequences,\\n    test_lengths,\\n    hidden_dim = 16\\n)',\n",
       "  'test_loss = elbo.loss(\\n    model_1,\\n    guide,\\n    test_sequences,\\n    test_lengths,\\n    hd = 16\\n)',\n",
       "  'test_loss',\n",
       "  'test_loss / num_observations',\n",
       "  'num_observations = float(test_lengths.sum())',\n",
       "  'test_loss / num_observations',\n",
       "  'import argparse\\nimport logging\\nimport sys\\n\\nimport torch\\nimport torch.nn as nn\\nfrom torch.distributions import constraints\\n\\nimport pyro\\nimport pyro.contrib.examples.polyphonic_data_loader as poly\\nimport pyro.distributions as dist\\nfrom pyro import poutine\\nfrom pyro.infer import SVI, JitTraceEnum_ELBO, TraceEnum_ELBO, TraceTMC_ELBO\\nfrom pyro.infer.autoguide import AutoDelta\\nfrom pyro.ops.indexing import Vindex\\nfrom pyro.optim import Adam\\nfrom pyro.util import ignore_jit_warnings',\n",
       "  'log = logging.getLogger()\\ndebug_handler = logging.StreamHandler(sys.stdout)\\ndebug_handler.setLevel(logging.DEBUG)\\ndebug_handler.addFilter(filter=lambda record: record.levelno <= logging.DEBUG)\\nlog.addHandler(debug_handler)',\n",
       "  'globals()'],\n",
       " '_oh': {3: {...},\n",
       "  8: torch.Size([229, 129, 88]),\n",
       "  17: torch.Size([229, 129, 88]),\n",
       "  35: torch.Size([229, 129, 88]),\n",
       "  53: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 1.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 1.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 1.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]]),\n",
       "  54: torch.Size([229, 129, 51]),\n",
       "  63: torch.Size([229, 129, 88]),\n",
       "  80: 65367.390625,\n",
       "  81: 4.734365946621279,\n",
       "  83: 13.834368386243387},\n",
       " '_dh': [PosixPath('/home/dulunche/GP_VAE/drclab/HMM')],\n",
       " 'In': ['',\n",
       "  'import argparse\\nimport logging\\nimport sys\\n\\nimport torch\\nimport torch.nn as nn\\nfrom torch.distributions import constraints\\n\\nimport pyro\\nimport pyro.contrib.examples.polyphonic_data_loader as poly\\nimport pyro.distributions as dist\\nfrom pyro import poutine\\nfrom pyro.infer import SVI, JitTraceEnum_ELBO, TraceEnum_ELBO, TraceTMC_ELBO\\nfrom pyro.infer.autoguide import AutoDelta\\nfrom pyro.ops.indexing import Vindex\\nfrom pyro.optim import Adam\\nfrom pyro.util import ignore_jit_warnings',\n",
       "  'log = logging.getLogger()\\ndebug_handler = logging.StreamHandler(sys.stdout)\\ndebug_handler.setLevel(logging.DEBUG)\\ndebug_handler.addFilter(filter=lambda record: record.levelno <= logging.DEBUG)\\nlog.addHandler(debug_handler)',\n",
       "  'globals()',\n",
       "  'models = {\\n    name[len(\"model_\") :]: model\\n    for name, model in globals().items()\\n    if name.startswith(\"model_\")\\n}',\n",
       "  'logging.info(\"-\" * 40)\\nmodel = models[args.model]\\nlogging.info(\\n    \"Training {} on {} sequences\".format(\\n        model.__name__, len(data[\"train\"][\"sequences\"])\\n    )\\n)',\n",
       "  'data = poly.load_data(poly.JSB_CHORALES)',\n",
       "  \"seqs = data['train']['sequences']\\nlengths = data['train']['sequence_lengths']\",\n",
       "  'seqs.shape',\n",
       "  'present_notes = (seqs == 1).sum(0).sum(0) > 0',\n",
       "  'seqs = seqs[..., present_notes].shape',\n",
       "  \"def model_1(seqs, lengths, hd=16, batch_size = None, include_prior=True):\\n    with ignore_jit_warnings():\\n        num_seqs, max_len, data_dim = map(int, seqs.shape)\\n        assert lengths.shape == (num_seqs,)\\n        assert lengths.max() == max_len\\n    with poutine.mask(mask=include_prior):\\n        probs_x = pyro.sample(\\n            'probs_x',\\n            dist.Dirichlet(0.9 * torch.eye(hd) + 0.1).to_event(1),\\n        )\\n\\n        probs_y = pyro.sample(\\n            'probs_y',\\n            dist.Beta(0.1, 0.9).expand([hd, data_dim]).to_event(2)\\n        )\\n\\n    tones_plate = pyro.plate('tones', data_dim, dim=-1)\\n    # We subsample batch_size items out of num_sequences items. Note that since\\n    # we're using dim=-1 for the notes plate, we need to batch over a different\\n    # dimension, here dim=-2.\\n    with pyro.plate('seqs', num_seqs, batch_size, dim=-2) as batch:\\n        lengths = lengths[batch]\\n        x = 0\\n        # If we are not using the jit, then we can vary the program structure\\n        # each call by running for a dynamically determined number of time\\n        # steps, lengths.max(). However if we are using the jit, then we try to\\n        # keep a single program structure for all minibatches; the fixed\\n        # structure ends up being faster since each program structure would\\n        # need to trigger a new jit compile stage.    \\n        for t in pyro.markov(range(lengths.max())):\\n            with poutine.mask(mask=(t < lengths).unsqueeze(-1)):\\n                x = pyro.sample(\\n                    'x_{}'.format(t),\\n                    dist.Categorical(probs_x[x]),\\n                    infer={'enumerate': 'parallel'}\\n                )\\n                with tones_plate:\\n                    pyro.sample(\\n                        'y_{}'.format(t),\\n                        dist.Bernoulli(probs_y[x.squeeze(-1)]),\\n                        obs = seqs[batch, t]\\n                    )\\n\\n        \",\n",
       "  \"guide = AutoDelta(\\n    poutine.block(model_1, expose_fn=lambda msg: msg['name'].startswith('probs_'))\\n)\",\n",
       "  'first_available_dim = -3',\n",
       "  'guide_trace = poutine.trace(guide).get_trace(\\n    seqs, lengths, hd=16, batch_size=10\\n)',\n",
       "  'data = poly.load_data(poly.JSB_CHORALES)',\n",
       "  \"seqs = data['train']['sequences']\\nlengths = data['train']['sequence_lengths']\",\n",
       "  'seqs.shape',\n",
       "  'present_notes = (seqs == 1).sum(0).sum(0) > 0',\n",
       "  'seqs = seqs[..., present_notes].shape',\n",
       "  \"def model_1(seqs, lengths, hd=16, batch_size = None, include_prior=True):\\n    with ignore_jit_warnings():\\n        num_seqs, max_len, data_dim = map(int, seqs.shape)\\n        assert lengths.shape == (num_seqs,)\\n        assert lengths.max() == max_len\\n    with poutine.mask(mask=include_prior):\\n        probs_x = pyro.sample(\\n            'probs_x',\\n            dist.Dirichlet(0.9 * torch.eye(hd) + 0.1).to_event(1),\\n        )\\n\\n        probs_y = pyro.sample(\\n            'probs_y',\\n            dist.Beta(0.1, 0.9).expand([hd, data_dim]).to_event(2)\\n        )\\n\\n    tones_plate = pyro.plate('tones', data_dim, dim=-1)\\n    # We subsample batch_size items out of num_sequences items. Note that since\\n    # we're using dim=-1 for the notes plate, we need to batch over a different\\n    # dimension, here dim=-2.\\n    with pyro.plate('seqs', num_seqs, batch_size, dim=-2) as batch:\\n        lengths = lengths[batch]\\n        x = 0\\n        # If we are not using the jit, then we can vary the program structure\\n        # each call by running for a dynamically determined number of time\\n        # steps, lengths.max(). However if we are using the jit, then we try to\\n        # keep a single program structure for all minibatches; the fixed\\n        # structure ends up being faster since each program structure would\\n        # need to trigger a new jit compile stage.    \\n        for t in pyro.markov(range(lengths.max())):\\n            with poutine.mask(mask=(t < lengths).unsqueeze(-1)):\\n                x = pyro.sample(\\n                    'x_{}'.format(t),\\n                    dist.Categorical(probs_x[x]),\\n                    infer={'enumerate': 'parallel'}\\n                )\\n                with tones_plate:\\n                    pyro.sample(\\n                        'y_{}'.format(t),\\n                        dist.Bernoulli(probs_y[x.squeeze(-1)]),\\n                        obs = seqs[batch, t]\\n                    )\\n\\n        \",\n",
       "  \"guide = AutoDelta(\\n    poutine.block(model_1, expose_fn=lambda msg: msg['name'].startswith('probs_'))\\n)\",\n",
       "  'first_available_dim = -3',\n",
       "  'guide_trace = poutine.trace(guide).get_trace(\\n    seqs, lengths, hd=16, batch_size=10\\n)',\n",
       "  \"def model_1(seqs, lengths, hd=16, batch_size = None, include_prior=True):\\n    with ignore_jit_warnings():\\n        num_seqs, max_len, data_dim = map(int, seqs.shape)\\n        assert lengths.shape == (num_seqs,)\\n        assert lengths.max() == max_len\\n    with poutine.mask(mask=include_prior):\\n        probs_x = pyro.sample(\\n            'probs_x',\\n            dist.Dirichlet(0.9 * torch.eye(hd) + 0.1).to_event(1),\\n        )\\n\\n        probs_y = pyro.sample(\\n            'probs_y',\\n            dist.Beta(0.1, 0.9).expand([hd, data_dim]).to_event(2)\\n        )\\n\\n    tones_plate = pyro.plate('tones', data_dim, dim=-1)\\n    # We subsample batch_size items out of num_sequences items. Note that since\\n    # we're using dim=-1 for the notes plate, we need to batch over a different\\n    # dimension, here dim=-2.\\n    with pyro.plate('seqs', num_seqs, batch_size, dim=-2) as batch:\\n        lengths = lengths[batch]\\n        x = 0\\n        # If we are not using the jit, then we can vary the program structure\\n        # each call by running for a dynamically determined number of time\\n        # steps, lengths.max(). However if we are using the jit, then we try to\\n        # keep a single program structure for all minibatches; the fixed\\n        # structure ends up being faster since each program structure would\\n        # need to trigger a new jit compile stage.    \\n        for t in pyro.markov(range(lengths.max())):\\n            with poutine.mask(mask=(t < lengths).unsqueeze(-1)):\\n                x = pyro.sample(\\n                    'x_{}'.format(t),\\n                    dist.Categorical(probs_x[x]),\\n                    infer={'enumerate': 'parallel'}\\n                )\\n                with tones_plate:\\n                    pyro.sample(\\n                        'y_{}'.format(t),\\n                        dist.Bernoulli(probs_y[x.squeeze(-1)]),\\n                        obs = seqs[batch, t]\\n                    )\\n\\n        \",\n",
       "  \"guide = AutoDelta(\\n    poutine.block(model_1, expose_fn=lambda msg: msg['name'].startswith('probs_'))\\n)\",\n",
       "  'first_available_dim = -3',\n",
       "  'guide_trace = poutine.trace(guide).get_trace(\\n    seqs, lengths, hd=16, batch_size=10\\n)',\n",
       "  'guide_trace = poutine.trace(guide).get_trace(\\n    seqs, lengths, hd=16, batch_size=10\\n)',\n",
       "  \"def model_1(seqs, lengths, hd=16, batch_size = None, include_prior=True):\\n    with ignore_jit_warnings():\\n        num_seqs, max_len, data_dim = map(int, seqs.shape)\\n        assert lengths.shape == (num_seqs,)\\n        assert lengths.max() == max_len\\n    with poutine.mask(mask=include_prior):\\n        probs_x = pyro.sample(\\n            'probs_x',\\n            dist.Dirichlet(0.9 * torch.eye(hd) + 0.1).to_event(1),\\n        )\\n\\n        probs_y = pyro.sample(\\n            'probs_y',\\n            dist.Beta(0.1, 0.9).expand([hd, data_dim]).to_event(2)\\n        )\\n\\n    tones_plate = pyro.plate('tones', data_dim, dim=-1)\\n    # We subsample batch_size items out of num_sequences items. Note that since\\n    # we're using dim=-1 for the notes plate, we need to batch over a different\\n    # dimension, here dim=-2.\\n    with pyro.plate('seqs', num_seqs, batch_size, dim=-2) as batch:\\n        lengths = lengths[batch]\\n        x = 0\\n        # If we are not using the jit, then we can vary the program structure\\n        # each call by running for a dynamically determined number of time\\n        # steps, lengths.max(). However if we are using the jit, then we try to\\n        # keep a single program structure for all minibatches; the fixed\\n        # structure ends up being faster since each program structure would\\n        # need to trigger a new jit compile stage.    \\n        for t in pyro.markov(range(lengths.max())):\\n            with poutine.mask(mask=(t < lengths).unsqueeze(-1)):\\n                x = pyro.sample(\\n                    'x_{}'.format(t),\\n                    dist.Categorical(probs_x[x]),\\n                    infer={'enumerate': 'parallel'}\\n                )\\n                with tones_plate:\\n                    pyro.sample(\\n                        'y_{}'.format(t),\\n                        dist.Bernoulli(probs_y[x.squeeze(-1)]),\\n                        obs = seqs[batch, t]\\n                    )\\n\\n        \",\n",
       "  \"guide = AutoDelta(\\n    poutine.block(model_1, expose_fn=lambda msg: msg['name'].startswith('probs_'))\\n)\",\n",
       "  'first_available_dim = -3',\n",
       "  'guide_trace = poutine.trace(guide).get_trace(\\n    seqs, lengths, hd=16, batch_size=10\\n)',\n",
       "  'seqs = seqs[..., present_notes]',\n",
       "  \"seqs = data['train']['sequences']\\nlengths = data['train']['sequence_lengths']\",\n",
       "  'seqs.shape',\n",
       "  \"def model_1(seqs, lengths, hd=16, batch_size = None, include_prior=True):\\n    with ignore_jit_warnings():\\n        num_seqs, max_len, data_dim = map(int, seqs.shape)\\n        assert lengths.shape == (num_seqs,)\\n        assert lengths.max() == max_len\\n    with poutine.mask(mask=include_prior):\\n        probs_x = pyro.sample(\\n            'probs_x',\\n            dist.Dirichlet(0.9 * torch.eye(hd) + 0.1).to_event(1),\\n        )\\n\\n        probs_y = pyro.sample(\\n            'probs_y',\\n            dist.Beta(0.1, 0.9).expand([hd, data_dim]).to_event(2)\\n        )\\n\\n    tones_plate = pyro.plate('tones', data_dim, dim=-1)\\n    # We subsample batch_size items out of num_sequences items. Note that since\\n    # we're using dim=-1 for the notes plate, we need to batch over a different\\n    # dimension, here dim=-2.\\n    with pyro.plate('seqs', num_seqs, batch_size, dim=-2) as batch:\\n        lengths = lengths[batch]\\n        x = 0\\n        # If we are not using the jit, then we can vary the program structure\\n        # each call by running for a dynamically determined number of time\\n        # steps, lengths.max(). However if we are using the jit, then we try to\\n        # keep a single program structure for all minibatches; the fixed\\n        # structure ends up being faster since each program structure would\\n        # need to trigger a new jit compile stage.    \\n        for t in pyro.markov(range(lengths.max())):\\n            with poutine.mask(mask=(t < lengths).unsqueeze(-1)):\\n                x = pyro.sample(\\n                    'x_{}'.format(t),\\n                    dist.Categorical(probs_x[x]),\\n                    infer={'enumerate': 'parallel'}\\n                )\\n                with tones_plate:\\n                    pyro.sample(\\n                        'y_{}'.format(t),\\n                        dist.Bernoulli(probs_y[x.squeeze(-1)]),\\n                        obs = seqs[batch, t]\\n                    )\\n\\n        \",\n",
       "  \"guide = AutoDelta(\\n    poutine.block(model_1, expose_fn=lambda msg: msg['name'].startswith('probs_'))\\n)\",\n",
       "  'first_available_dim = -3',\n",
       "  'guide_trace = poutine.trace(guide).get_trace(\\n    seqs, lengths, hd=16, batch_size=10\\n)',\n",
       "  'model_trace = poutine.trace(\\n    poutine.replay(poutine.enum(model_1, first_available_dim), guide_trace)\\n    ).get_trace(\\n        seqs, lengths, hd = 16, batch_size = 10\\n    )',\n",
       "  'print(model_trace.format_shapes())',\n",
       "  \"optim = Adam({'lr':0.01})\\nElbo = TraceEnum_ELBO\\nelbo = Elbo(\\n    max_plate_nesting = 2,\\n    strict_enumeration_warning=True,\\n    jit_options={'time_compilation': 'store_true'}\\n)\",\n",
       "  'svi = SVI(model_1, guide, optim, elbo)',\n",
       "  \"num_steps = 100\\npyro.set_rng_seed(111)\\npyro.clear_param_store()\\nnum_observations = float(lengths.sum())\\nfor step in range(num_steps):\\n    loss = svi.step(seqs, lengths, hd=16, batch_size=20)\\n    print('{:5d}\\\\t{}'.format(step, loss / num_observations))\",\n",
       "  'seqs = seqs[., present_notes]',\n",
       "  'seqs = seqs[..., present_notes]',\n",
       "  \"def model_1(seqs, lengths, hd=16, batch_size = None, include_prior=True):\\n    with ignore_jit_warnings():\\n        num_seqs, max_len, data_dim = map(int, seqs.shape)\\n        assert lengths.shape == (num_seqs,)\\n        assert lengths.max() == max_len\\n    with poutine.mask(mask=include_prior):\\n        probs_x = pyro.sample(\\n            'probs_x',\\n            dist.Dirichlet(0.9 * torch.eye(hd) + 0.1).to_event(1),\\n        )\\n\\n        probs_y = pyro.sample(\\n            'probs_y',\\n            dist.Beta(0.1, 0.9).expand([hd, data_dim]).to_event(2)\\n        )\\n\\n    tones_plate = pyro.plate('tones', data_dim, dim=-1)\\n    # We subsample batch_size items out of num_sequences items. Note that since\\n    # we're using dim=-1 for the notes plate, we need to batch over a different\\n    # dimension, here dim=-2.\\n    with pyro.plate('seqs', num_seqs, batch_size, dim=-2) as batch:\\n        lengths = lengths[batch]\\n        x = 0\\n        # If we are not using the jit, then we can vary the program structure\\n        # each call by running for a dynamically determined number of time\\n        # steps, lengths.max(). However if we are using the jit, then we try to\\n        # keep a single program structure for all minibatches; the fixed\\n        # structure ends up being faster since each program structure would\\n        # need to trigger a new jit compile stage.    \\n        for t in pyro.markov(range(lengths.max())):\\n            with poutine.mask(mask=(t < lengths).unsqueeze(-1)):\\n                x = pyro.sample(\\n                    'x_{}'.format(t),\\n                    dist.Categorical(probs_x[x]),\\n                    infer={'enumerate': 'parallel'}\\n                )\\n                with tones_plate:\\n                    pyro.sample(\\n                        'y_{}'.format(t),\\n                        dist.Bernoulli(probs_y[x.squeeze(-1)]),\\n                        obs = seqs[batch, t]\\n                    )\\n\\n        \",\n",
       "  \"guide = AutoDelta(\\n    poutine.block(model_1, expose_fn=lambda msg: msg['name'].startswith('probs_'))\\n)\",\n",
       "  'first_available_dim = -3',\n",
       "  'guide_trace = poutine.trace(guide).get_trace(\\n    seqs, lengths, hd=16, batch_size=10\\n)',\n",
       "  'model_trace = poutine.trace(\\n    poutine.replay(poutine.enum(model_1, first_available_dim), guide_trace)\\n    ).get_trace(\\n        seqs, lengths, hd = 16, batch_size = 10\\n    )',\n",
       "  'model_trace = poutine.trace(\\n    poutine.replay(poutine.enum(model_1, first_available_dim), guide_trace)\\n    ).get_trace(\\n        seqs, lengths, hd = 16, batch_size = 10\\n    )',\n",
       "  'seqs',\n",
       "  'seqs.shape',\n",
       "  \"def model_1(seqs, lengths, hd=16, batch_size = None, include_prior=True):\\n    with ignore_jit_warnings():\\n        num_seqs, max_len, data_dim = map(int, seqs.shape)\\n        assert lengths.shape == (num_seqs,)\\n        assert lengths.max() == max_len\\n    with poutine.mask(mask=include_prior):\\n        probs_x = pyro.sample(\\n            'probs_x',\\n            dist.Dirichlet(0.9 * torch.eye(hd) + 0.1).to_event(1),\\n        )\\n\\n        probs_y = pyro.sample(\\n            'probs_y',\\n            dist.Beta(0.1, 0.9).expand([hd, data_dim]).to_event(2)\\n        )\\n\\n    tones_plate = pyro.plate('tones', data_dim, dim=-1)\\n    # We subsample batch_size items out of num_sequences items. Note that since\\n    # we're using dim=-1 for the notes plate, we need to batch over a different\\n    # dimension, here dim=-2.\\n    with pyro.plate('seqs', num_seqs, batch_size, dim=-2) as batch:\\n        lengths = lengths[batch]\\n        x = 0\\n        # If we are not using the jit, then we can vary the program structure\\n        # each call by running for a dynamically determined number of time\\n        # steps, lengths.max(). However if we are using the jit, then we try to\\n        # keep a single program structure for all minibatches; the fixed\\n        # structure ends up being faster since each program structure would\\n        # need to trigger a new jit compile stage.    \\n        for t in pyro.markov(range(lengths.max())):\\n            with poutine.mask(mask=(t < lengths).unsqueeze(-1)):\\n                x = pyro.sample(\\n                    'x_{}'.format(t),\\n                    dist.Categorical(probs_x[x]),\\n                    infer={'enumerate': 'parallel'}\\n                )\\n                with tones_plate:\\n                    pyro.sample(\\n                        'y_{}'.format(t),\\n                        dist.Bernoulli(probs_y[x.squeeze(-1)]),\\n                        obs = seqs[batch, t]\\n                    )\\n\\n        \",\n",
       "  \"guide = AutoDelta(\\n    poutine.block(model_1, expose_fn=lambda msg: msg['name'].startswith('probs_'))\\n)\",\n",
       "  'first_available_dim = -3',\n",
       "  'guide_trace = poutine.trace(guide).get_trace(\\n    seqs, lengths, hd=16, batch_size=10\\n)',\n",
       "  'model_trace = poutine.trace(\\n    poutine.replay(poutine.enum(model_1, first_available_dim), guide_trace)\\n    ).get_trace(\\n        seqs, lengths, hd = 16, batch_size = 10\\n    )',\n",
       "  'guide_trace = poutine.trace(guide).get_trace(\\n    seqs, lengths, hd=16, batch_size=10\\n)',\n",
       "  'data = poly.load_data(poly.JSB_CHORALES)',\n",
       "  \"seqs = data['train']['sequences']\\nlengths = data['train']['sequence_lengths']\",\n",
       "  'seqs.shape',\n",
       "  'present_notes = (seqs == 1).sum(0).sum(0) > 0',\n",
       "  '#seqs = seqs[..., present_notes]',\n",
       "  \"def model_1(seqs, lengths, hd=16, batch_size = None, include_prior=True):\\n    with ignore_jit_warnings():\\n        num_seqs, max_len, data_dim = map(int, seqs.shape)\\n        assert lengths.shape == (num_seqs,)\\n        assert lengths.max() == max_len\\n    with poutine.mask(mask=include_prior):\\n        probs_x = pyro.sample(\\n            'probs_x',\\n            dist.Dirichlet(0.9 * torch.eye(hd) + 0.1).to_event(1),\\n        )\\n\\n        probs_y = pyro.sample(\\n            'probs_y',\\n            dist.Beta(0.1, 0.9).expand([hd, data_dim]).to_event(2)\\n        )\\n\\n    tones_plate = pyro.plate('tones', data_dim, dim=-1)\\n    # We subsample batch_size items out of num_sequences items. Note that since\\n    # we're using dim=-1 for the notes plate, we need to batch over a different\\n    # dimension, here dim=-2.\\n    with pyro.plate('seqs', num_seqs, batch_size, dim=-2) as batch:\\n        lengths = lengths[batch]\\n        x = 0\\n        # If we are not using the jit, then we can vary the program structure\\n        # each call by running for a dynamically determined number of time\\n        # steps, lengths.max(). However if we are using the jit, then we try to\\n        # keep a single program structure for all minibatches; the fixed\\n        # structure ends up being faster since each program structure would\\n        # need to trigger a new jit compile stage.    \\n        for t in pyro.markov(range(lengths.max())):\\n            with poutine.mask(mask=(t < lengths).unsqueeze(-1)):\\n                x = pyro.sample(\\n                    'x_{}'.format(t),\\n                    dist.Categorical(probs_x[x]),\\n                    infer={'enumerate': 'parallel'}\\n                )\\n                with tones_plate:\\n                    pyro.sample(\\n                        'y_{}'.format(t),\\n                        dist.Bernoulli(probs_y[x.squeeze(-1)]),\\n                        obs = seqs[batch, t]\\n                    )\\n\\n        \",\n",
       "  \"guide = AutoDelta(\\n    poutine.block(model_1, expose_fn=lambda msg: msg['name'].startswith('probs_'))\\n)\",\n",
       "  'first_available_dim = -3',\n",
       "  'guide_trace = poutine.trace(guide).get_trace(\\n    seqs, lengths, hd=16, batch_size=10\\n)',\n",
       "  'model_trace = poutine.trace(\\n    poutine.replay(poutine.enum(model_1, first_available_dim), guide_trace)\\n    ).get_trace(\\n        seqs, lengths, hd = 16, batch_size = 10\\n    )',\n",
       "  'print(model_trace.format_shapes())',\n",
       "  \"# Notice that we're now using dim=-2 as a batch dimension (of size 10),\\n# and that the enumeration dimensions are now dims -3 and -4.\",\n",
       "  \"optim = Adam({'lr':0.01})\\nElbo = TraceEnum_ELBO\\nelbo = Elbo(\\n    max_plate_nesting = 2,\\n    strict_enumeration_warning=True,\\n    jit_options={'time_compilation': 'store_true'}\\n)\",\n",
       "  'svi = SVI(model_1, guide, optim, elbo)',\n",
       "  \"num_steps = 100\\npyro.set_rng_seed(111)\\npyro.clear_param_store()\\nnum_observations = float(lengths.sum())\\nfor step in range(num_steps):\\n    loss = svi.step(seqs, lengths, hd=16, batch_size=20)\\n    print('{:5d}\\\\t{}'.format(step, loss / num_observations))\",\n",
       "  \"test_sequences = data['test']['sequences']\\ntest_lengths = data['test']['sequence_lengths']\",\n",
       "  'test_loss = elbo.loss(\\n    model_0,\\n    guide,\\n    test_sequences,\\n    test_lengths,\\n    hidden_dim = 16\\n)',\n",
       "  'test_loss = elbo.loss(\\n    model_1,\\n    guide,\\n    test_sequences,\\n    test_lengths,\\n    hidden_dim = 16\\n)',\n",
       "  'test_loss = elbo.loss(\\n    model_1,\\n    guide,\\n    test_sequences,\\n    test_lengths,\\n    hd = 16\\n)',\n",
       "  'test_loss',\n",
       "  'test_loss / num_observations',\n",
       "  'num_observations = float(test_lengths.sum())',\n",
       "  'test_loss / num_observations',\n",
       "  'import argparse\\nimport logging\\nimport sys\\n\\nimport torch\\nimport torch.nn as nn\\nfrom torch.distributions import constraints\\n\\nimport pyro\\nimport pyro.contrib.examples.polyphonic_data_loader as poly\\nimport pyro.distributions as dist\\nfrom pyro import poutine\\nfrom pyro.infer import SVI, JitTraceEnum_ELBO, TraceEnum_ELBO, TraceTMC_ELBO\\nfrom pyro.infer.autoguide import AutoDelta\\nfrom pyro.ops.indexing import Vindex\\nfrom pyro.optim import Adam\\nfrom pyro.util import ignore_jit_warnings',\n",
       "  'log = logging.getLogger()\\ndebug_handler = logging.StreamHandler(sys.stdout)\\ndebug_handler.setLevel(logging.DEBUG)\\ndebug_handler.addFilter(filter=lambda record: record.levelno <= logging.DEBUG)\\nlog.addHandler(debug_handler)',\n",
       "  'globals()'],\n",
       " 'Out': {3: {...},\n",
       "  8: torch.Size([229, 129, 88]),\n",
       "  17: torch.Size([229, 129, 88]),\n",
       "  35: torch.Size([229, 129, 88]),\n",
       "  53: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 1.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 1.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 1.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]]),\n",
       "  54: torch.Size([229, 129, 51]),\n",
       "  63: torch.Size([229, 129, 88]),\n",
       "  80: 65367.390625,\n",
       "  81: 4.734365946621279,\n",
       "  83: 13.834368386243387},\n",
       " 'get_ipython': <bound method InteractiveShell.get_ipython of <ipykernel.zmqshell.ZMQInteractiveShell object at 0x7f12b1cf8e80>>,\n",
       " 'exit': <IPython.core.autocall.ZMQExitAutocall at 0x7f12b1cf9a50>,\n",
       " 'quit': <IPython.core.autocall.ZMQExitAutocall at 0x7f12b1cf9a50>,\n",
       " '_': 13.834368386243387,\n",
       " '__': 4.734365946621279,\n",
       " '___': 65367.390625,\n",
       " 'os': <module 'os' from '/usr/lib/python3.10/os.py'>,\n",
       " 'sys': <module 'sys' (built-in)>,\n",
       " '__vsc_ipynb_file__': '/home/dulunche/GP_VAE/drclab/HMM/hmm_1.ipynb',\n",
       " '_i': 'log = logging.getLogger()\\ndebug_handler = logging.StreamHandler(sys.stdout)\\ndebug_handler.setLevel(logging.DEBUG)\\ndebug_handler.addFilter(filter=lambda record: record.levelno <= logging.DEBUG)\\nlog.addHandler(debug_handler)',\n",
       " '_ii': 'import argparse\\nimport logging\\nimport sys\\n\\nimport torch\\nimport torch.nn as nn\\nfrom torch.distributions import constraints\\n\\nimport pyro\\nimport pyro.contrib.examples.polyphonic_data_loader as poly\\nimport pyro.distributions as dist\\nfrom pyro import poutine\\nfrom pyro.infer import SVI, JitTraceEnum_ELBO, TraceEnum_ELBO, TraceTMC_ELBO\\nfrom pyro.infer.autoguide import AutoDelta\\nfrom pyro.ops.indexing import Vindex\\nfrom pyro.optim import Adam\\nfrom pyro.util import ignore_jit_warnings',\n",
       " '_iii': 'test_loss / num_observations',\n",
       " '_i1': 'import argparse\\nimport logging\\nimport sys\\n\\nimport torch\\nimport torch.nn as nn\\nfrom torch.distributions import constraints\\n\\nimport pyro\\nimport pyro.contrib.examples.polyphonic_data_loader as poly\\nimport pyro.distributions as dist\\nfrom pyro import poutine\\nfrom pyro.infer import SVI, JitTraceEnum_ELBO, TraceEnum_ELBO, TraceTMC_ELBO\\nfrom pyro.infer.autoguide import AutoDelta\\nfrom pyro.ops.indexing import Vindex\\nfrom pyro.optim import Adam\\nfrom pyro.util import ignore_jit_warnings',\n",
       " 'argparse': <module 'argparse' from '/usr/lib/python3.10/argparse.py'>,\n",
       " 'logging': <module 'logging' from '/usr/lib/python3.10/logging/__init__.py'>,\n",
       " 'torch': <module 'torch' from '/home/dulunche/.local/lib/python3.10/site-packages/torch/__init__.py'>,\n",
       " 'nn': <module 'torch.nn' from '/home/dulunche/.local/lib/python3.10/site-packages/torch/nn/__init__.py'>,\n",
       " 'constraints': <module 'torch.distributions.constraints' from '/home/dulunche/.local/lib/python3.10/site-packages/torch/distributions/constraints.py'>,\n",
       " 'pyro': <module 'pyro' from '/home/dulunche/.local/lib/python3.10/site-packages/pyro/__init__.py'>,\n",
       " 'poly': <module 'pyro.contrib.examples.polyphonic_data_loader' from '/home/dulunche/.local/lib/python3.10/site-packages/pyro/contrib/examples/polyphonic_data_loader.py'>,\n",
       " 'dist': <module 'pyro.distributions' from '/home/dulunche/.local/lib/python3.10/site-packages/pyro/distributions/__init__.py'>,\n",
       " 'poutine': <module 'pyro.poutine' from '/home/dulunche/.local/lib/python3.10/site-packages/pyro/poutine/__init__.py'>,\n",
       " 'SVI': pyro.infer.svi.SVI,\n",
       " 'JitTraceEnum_ELBO': pyro.infer.traceenum_elbo.JitTraceEnum_ELBO,\n",
       " 'TraceEnum_ELBO': pyro.infer.traceenum_elbo.TraceEnum_ELBO,\n",
       " 'TraceTMC_ELBO': pyro.infer.tracetmc_elbo.TraceTMC_ELBO,\n",
       " 'AutoDelta': pyro.infer.autoguide.guides.AutoDelta,\n",
       " 'Vindex': pyro.ops.indexing.Vindex,\n",
       " 'Adam': <function pyro.optim.pytorch_optimizers.<lambda>.<locals>.<lambda>(optim_args, clip_args=None)>,\n",
       " 'ignore_jit_warnings': <function pyro.util.ignore_jit_warnings(filter=None)>,\n",
       " '_i2': 'log = logging.getLogger()\\ndebug_handler = logging.StreamHandler(sys.stdout)\\ndebug_handler.setLevel(logging.DEBUG)\\ndebug_handler.addFilter(filter=lambda record: record.levelno <= logging.DEBUG)\\nlog.addHandler(debug_handler)',\n",
       " 'log': <RootLogger root (WARNING)>,\n",
       " 'debug_handler': <StreamHandler stdout (DEBUG)>,\n",
       " '_i3': 'globals()',\n",
       " '_3': {...},\n",
       " '_i4': 'models = {\\n    name[len(\"model_\") :]: model\\n    for name, model in globals().items()\\n    if name.startswith(\"model_\")\\n}',\n",
       " 'models': {},\n",
       " '_i5': '    logging.info(\"-\" * 40)\\n    model = models[args.model]\\n    logging.info(\\n        \"Training {} on {} sequences\".format(\\n            model.__name__, len(data[\"train\"][\"sequences\"])\\n        )\\n    )',\n",
       " '_i6': 'data = poly.load_data(poly.JSB_CHORALES)',\n",
       " 'data': {'test': {'sequence_lengths': tensor([ 84,  61,  57,  39,  32, 101,  57,  57,  48,  71,  33,  49,  69,  65,\n",
       "           101,  81,  53,  57,  61,  49,  41,  57,  37,  33,  56, 112,  36,  64,\n",
       "            39,  65,  49,  49, 145,  48,  57,  49,  52,  49,  64,  68,  90,  49,\n",
       "            60,  68,  41,  41,  33,  48,  49, 117,  49, 160,  57,  45,  41,  49,\n",
       "            32,  64,  65,  76,  49,  49,  45,  49,  93,  57,  87,  57,  80,  56,\n",
       "            65, 109,  41,  69,  73,  44,  73]),\n",
       "   'sequences': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "   \n",
       "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "   \n",
       "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "   \n",
       "           ...,\n",
       "   \n",
       "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "   \n",
       "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "   \n",
       "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]]])},\n",
       "  'train': {'sequence_lengths': tensor([129,  65,  49,  65, 114,  33,  57,  49,  64,  33, 108,  48,  49,  48,\n",
       "            61,  48,  65,  53,  41,  52,  33,  61,  41,  45,  69,  39,  57,  80,\n",
       "            86,  57,  61, 105,  68,  65,  48,  57,  57,  52,  48,  33,  93,  41,\n",
       "            65,  49,  73,  48,  33,  45,  65,  52,  49, 109,  49,  52,  65,  41,\n",
       "            49,  65,  77,  73,  57,  41,  65,  57,  44,  33,  85,  72,  60,  41,\n",
       "            49,  33,  56,  52,  56,  83,  57,  57,  52,  41,  53,  64,  61,  65,\n",
       "            65,  48,  25,  96,  41,  37,  76,  65,  65,  72,  41,  65,  41,  37,\n",
       "            77,  48,  85,  57,  72,  48,  45,  53,  49,  84,  68,  49,  73,  50,\n",
       "            96,  55,  66,  98,  37,  76,  65,  33,  49,  77,  76,  65, 128,  41,\n",
       "            45,  61,  33,  88,  61,  63,  65,  68,  60,  49,  48,  45,  65,  45,\n",
       "            33,  64,  64,  65,  49,  63,  61,  48,  45,  65,  84, 120,  41, 102,\n",
       "            49,  57,  69,  57,  82,  48,  76,  48,  52, 113,  97,  83,  33,  68,\n",
       "            41,  49,  65, 109, 108,  60,  65,  57,  49,  33,  57,  33,  61, 113,\n",
       "            58,  64,  65,  33,  57,  49,  64,  68,  60,  65,  41,  72,  53,  49,\n",
       "            57,  33,  49,  89,  65,  44,  49,  61,  65,  69,  52,  76,  33,  57,\n",
       "            76,  57,  33,  76, 101,  40,  41,  44,  76,  65,  49,  54,  49,  49,\n",
       "            64,  51, 109,  65,  65]),\n",
       "   'sequences': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "   \n",
       "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "   \n",
       "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "   \n",
       "           ...,\n",
       "   \n",
       "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "   \n",
       "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "   \n",
       "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]]])},\n",
       "  'valid': {'sequence_lengths': tensor([ 52, 105,  65,  49,  85,  45,  52,  33,  68,  65,  41,  41,  49,  64,\n",
       "            56,  49,  41,  90,  40,  54,  65,  59,  56,  33, 112,  52,  65,  97,\n",
       "            52,  57,  98,  69, 138,  41,  49,  41,  61,  54,  57, 144,  48,  64,\n",
       "            69, 103,  37, 144,  65,  45, 108,  61,  65,  73,  32,  49,  37,  49,\n",
       "            37,  37,  57,  69,  61,  49,  37,  49,  57,  41,  41,  45,  61,  45,\n",
       "            37,  45,  88,  40,  69,  44]),\n",
       "   'sequences': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "   \n",
       "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "   \n",
       "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "   \n",
       "           ...,\n",
       "   \n",
       "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "   \n",
       "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "   \n",
       "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]]])}},\n",
       " '_i7': \"seqs = data['train']['sequences']\\nlengths = data['train']['sequence_lengths']\",\n",
       " 'seqs': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]]),\n",
       " 'lengths': tensor([129,  65,  49,  65, 114,  33,  57,  49,  64,  33, 108,  48,  49,  48,\n",
       "          61,  48,  65,  53,  41,  52,  33,  61,  41,  45,  69,  39,  57,  80,\n",
       "          86,  57,  61, 105,  68,  65,  48,  57,  57,  52,  48,  33,  93,  41,\n",
       "          65,  49,  73,  48,  33,  45,  65,  52,  49, 109,  49,  52,  65,  41,\n",
       "          49,  65,  77,  73,  57,  41,  65,  57,  44,  33,  85,  72,  60,  41,\n",
       "          49,  33,  56,  52,  56,  83,  57,  57,  52,  41,  53,  64,  61,  65,\n",
       "          65,  48,  25,  96,  41,  37,  76,  65,  65,  72,  41,  65,  41,  37,\n",
       "          77,  48,  85,  57,  72,  48,  45,  53,  49,  84,  68,  49,  73,  50,\n",
       "          96,  55,  66,  98,  37,  76,  65,  33,  49,  77,  76,  65, 128,  41,\n",
       "          45,  61,  33,  88,  61,  63,  65,  68,  60,  49,  48,  45,  65,  45,\n",
       "          33,  64,  64,  65,  49,  63,  61,  48,  45,  65,  84, 120,  41, 102,\n",
       "          49,  57,  69,  57,  82,  48,  76,  48,  52, 113,  97,  83,  33,  68,\n",
       "          41,  49,  65, 109, 108,  60,  65,  57,  49,  33,  57,  33,  61, 113,\n",
       "          58,  64,  65,  33,  57,  49,  64,  68,  60,  65,  41,  72,  53,  49,\n",
       "          57,  33,  49,  89,  65,  44,  49,  61,  65,  69,  52,  76,  33,  57,\n",
       "          76,  57,  33,  76, 101,  40,  41,  44,  76,  65,  49,  54,  49,  49,\n",
       "          64,  51, 109,  65,  65]),\n",
       " '_i8': 'seqs.shape',\n",
       " '_8': torch.Size([229, 129, 88]),\n",
       " '_i9': 'present_notes = (seqs == 1).sum(0).sum(0) > 0',\n",
       " 'present_notes': tensor([False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False,  True, False, False,  True, False,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False]),\n",
       " '_i10': 'seqs = seqs[..., present_notes].shape',\n",
       " '_i11': \"def model_1(seqs, lengths, hd=16, batch_size = None, include_prior=True):\\n    with ignore_jit_warnings():\\n        num_seqs, max_len, data_dim = map(int, seqs.shape)\\n        assert lengths.shape == (num_seqs,)\\n        assert lengths.max() == max_len\\n    with poutine.mask(mask=include_prior):\\n        probs_x = pyro.sample(\\n            'probs_x',\\n            dist.Dirichlet(0.9 * torch.eye(hd) + 0.1).to_event(1),\\n        )\\n\\n        probs_y = pyro.sample(\\n            'probs_y',\\n            dist.Beta(0.1, 0.9).expand([hd, data_dim]).to_event(2)\\n        )\\n\\n    tones_plate = pyro.plate('tones', data_dim, dim=-1)\\n    # We subsample batch_size items out of num_sequences items. Note that since\\n    # we're using dim=-1 for the notes plate, we need to batch over a different\\n    # dimension, here dim=-2.\\n    with pyro.plate('seqs', num_seqs, batch_size, dim=-2) as batch:\\n        lengths = lengths[batch]\\n        x = 0\\n        # If we are not using the jit, then we can vary the program structure\\n        # each call by running for a dynamically determined number of time\\n        # steps, lengths.max(). However if we are using the jit, then we try to\\n        # keep a single program structure for all minibatches; the fixed\\n        # structure ends up being faster since each program structure would\\n        # need to trigger a new jit compile stage.    \\n        for t in pyro.markov(range(lengths.max())):\\n            with poutine.mask(mask=(t < lengths).unsqueeze(-1)):\\n                x = pyro.sample(\\n                    'x_{}'.format(t),\\n                    dist.Categorical(probs_x[x]),\\n                    infer={'enumerate': 'parallel'}\\n                )\\n                with tones_plate:\\n                    pyro.sample(\\n                        'y_{}'.format(t),\\n                        dist.Bernoulli(probs_y[x.squeeze(-1)]),\\n                        obs = seqs[batch, t]\\n                    )\\n\\n        \",\n",
       " 'model_1': <function __main__.model_1(seqs, lengths, hd=16, batch_size=None, include_prior=True)>,\n",
       " '_i12': \"guide = AutoDelta(\\n    poutine.block(model_1, expose_fn=lambda msg: msg['name'].startswith('probs_'))\\n)\",\n",
       " 'guide': AutoDelta(),\n",
       " '_i13': 'first_available_dim = -3',\n",
       " 'first_available_dim': -3,\n",
       " '_i14': 'guide_trace = poutine.trace(guide).get_trace(\\n    seqs, lengths, hd=16, batch_size=10\\n)',\n",
       " '_i15': 'data = poly.load_data(poly.JSB_CHORALES)',\n",
       " '_i16': \"seqs = data['train']['sequences']\\nlengths = data['train']['sequence_lengths']\",\n",
       " '_i17': 'seqs.shape',\n",
       " '_17': torch.Size([229, 129, 88]),\n",
       " '_i18': 'present_notes = (seqs == 1).sum(0).sum(0) > 0',\n",
       " '_i19': 'seqs = seqs[..., present_notes].shape',\n",
       " '_i20': \"def model_1(seqs, lengths, hd=16, batch_size = None, include_prior=True):\\n    with ignore_jit_warnings():\\n        num_seqs, max_len, data_dim = map(int, seqs.shape)\\n        assert lengths.shape == (num_seqs,)\\n        assert lengths.max() == max_len\\n    with poutine.mask(mask=include_prior):\\n        probs_x = pyro.sample(\\n            'probs_x',\\n            dist.Dirichlet(0.9 * torch.eye(hd) + 0.1).to_event(1),\\n        )\\n\\n        probs_y = pyro.sample(\\n            'probs_y',\\n            dist.Beta(0.1, 0.9).expand([hd, data_dim]).to_event(2)\\n        )\\n\\n    tones_plate = pyro.plate('tones', data_dim, dim=-1)\\n    # We subsample batch_size items out of num_sequences items. Note that since\\n    # we're using dim=-1 for the notes plate, we need to batch over a different\\n    # dimension, here dim=-2.\\n    with pyro.plate('seqs', num_seqs, batch_size, dim=-2) as batch:\\n        lengths = lengths[batch]\\n        x = 0\\n        # If we are not using the jit, then we can vary the program structure\\n        # each call by running for a dynamically determined number of time\\n        # steps, lengths.max(). However if we are using the jit, then we try to\\n        # keep a single program structure for all minibatches; the fixed\\n        # structure ends up being faster since each program structure would\\n        # need to trigger a new jit compile stage.    \\n        for t in pyro.markov(range(lengths.max())):\\n            with poutine.mask(mask=(t < lengths).unsqueeze(-1)):\\n                x = pyro.sample(\\n                    'x_{}'.format(t),\\n                    dist.Categorical(probs_x[x]),\\n                    infer={'enumerate': 'parallel'}\\n                )\\n                with tones_plate:\\n                    pyro.sample(\\n                        'y_{}'.format(t),\\n                        dist.Bernoulli(probs_y[x.squeeze(-1)]),\\n                        obs = seqs[batch, t]\\n                    )\\n\\n        \",\n",
       " '_i21': \"guide = AutoDelta(\\n    poutine.block(model_1, expose_fn=lambda msg: msg['name'].startswith('probs_'))\\n)\",\n",
       " '_i22': 'first_available_dim = -3',\n",
       " '_i23': 'guide_trace = poutine.trace(guide).get_trace(\\n    seqs, lengths, hd=16, batch_size=10\\n)',\n",
       " '_i24': \"def model_1(seqs, lengths, hd=16, batch_size = None, include_prior=True):\\n    with ignore_jit_warnings():\\n        num_seqs, max_len, data_dim = map(int, seqs.shape)\\n        assert lengths.shape == (num_seqs,)\\n        assert lengths.max() == max_len\\n    with poutine.mask(mask=include_prior):\\n        probs_x = pyro.sample(\\n            'probs_x',\\n            dist.Dirichlet(0.9 * torch.eye(hd) + 0.1).to_event(1),\\n        )\\n\\n        probs_y = pyro.sample(\\n            'probs_y',\\n            dist.Beta(0.1, 0.9).expand([hd, data_dim]).to_event(2)\\n        )\\n\\n    tones_plate = pyro.plate('tones', data_dim, dim=-1)\\n    # We subsample batch_size items out of num_sequences items. Note that since\\n    # we're using dim=-1 for the notes plate, we need to batch over a different\\n    # dimension, here dim=-2.\\n    with pyro.plate('seqs', num_seqs, batch_size, dim=-2) as batch:\\n        lengths = lengths[batch]\\n        x = 0\\n        # If we are not using the jit, then we can vary the program structure\\n        # each call by running for a dynamically determined number of time\\n        # steps, lengths.max(). However if we are using the jit, then we try to\\n        # keep a single program structure for all minibatches; the fixed\\n        # structure ends up being faster since each program structure would\\n        # need to trigger a new jit compile stage.    \\n        for t in pyro.markov(range(lengths.max())):\\n            with poutine.mask(mask=(t < lengths).unsqueeze(-1)):\\n                x = pyro.sample(\\n                    'x_{}'.format(t),\\n                    dist.Categorical(probs_x[x]),\\n                    infer={'enumerate': 'parallel'}\\n                )\\n                with tones_plate:\\n                    pyro.sample(\\n                        'y_{}'.format(t),\\n                        dist.Bernoulli(probs_y[x.squeeze(-1)]),\\n                        obs = seqs[batch, t]\\n                    )\\n\\n        \",\n",
       " '_i25': \"guide = AutoDelta(\\n    poutine.block(model_1, expose_fn=lambda msg: msg['name'].startswith('probs_'))\\n)\",\n",
       " '_i26': 'first_available_dim = -3',\n",
       " '_i27': 'guide_trace = poutine.trace(guide).get_trace(\\n    seqs, lengths, hd=16, batch_size=10\\n)',\n",
       " '_i28': 'guide_trace = poutine.trace(guide).get_trace(\\n    seqs, lengths, hd=16, batch_size=10\\n)',\n",
       " '_i29': \"def model_1(seqs, lengths, hd=16, batch_size = None, include_prior=True):\\n    with ignore_jit_warnings():\\n        num_seqs, max_len, data_dim = map(int, seqs.shape)\\n        assert lengths.shape == (num_seqs,)\\n        assert lengths.max() == max_len\\n    with poutine.mask(mask=include_prior):\\n        probs_x = pyro.sample(\\n            'probs_x',\\n            dist.Dirichlet(0.9 * torch.eye(hd) + 0.1).to_event(1),\\n        )\\n\\n        probs_y = pyro.sample(\\n            'probs_y',\\n            dist.Beta(0.1, 0.9).expand([hd, data_dim]).to_event(2)\\n        )\\n\\n    tones_plate = pyro.plate('tones', data_dim, dim=-1)\\n    # We subsample batch_size items out of num_sequences items. Note that since\\n    # we're using dim=-1 for the notes plate, we need to batch over a different\\n    # dimension, here dim=-2.\\n    with pyro.plate('seqs', num_seqs, batch_size, dim=-2) as batch:\\n        lengths = lengths[batch]\\n        x = 0\\n        # If we are not using the jit, then we can vary the program structure\\n        # each call by running for a dynamically determined number of time\\n        # steps, lengths.max(). However if we are using the jit, then we try to\\n        # keep a single program structure for all minibatches; the fixed\\n        # structure ends up being faster since each program structure would\\n        # need to trigger a new jit compile stage.    \\n        for t in pyro.markov(range(lengths.max())):\\n            with poutine.mask(mask=(t < lengths).unsqueeze(-1)):\\n                x = pyro.sample(\\n                    'x_{}'.format(t),\\n                    dist.Categorical(probs_x[x]),\\n                    infer={'enumerate': 'parallel'}\\n                )\\n                with tones_plate:\\n                    pyro.sample(\\n                        'y_{}'.format(t),\\n                        dist.Bernoulli(probs_y[x.squeeze(-1)]),\\n                        obs = seqs[batch, t]\\n                    )\\n\\n        \",\n",
       " '_i30': \"guide = AutoDelta(\\n    poutine.block(model_1, expose_fn=lambda msg: msg['name'].startswith('probs_'))\\n)\",\n",
       " '_i31': 'first_available_dim = -3',\n",
       " '_i32': 'guide_trace = poutine.trace(guide).get_trace(\\n    seqs, lengths, hd=16, batch_size=10\\n)',\n",
       " '_i33': 'seqs = seqs[..., present_notes]',\n",
       " '_i34': \"seqs = data['train']['sequences']\\nlengths = data['train']['sequence_lengths']\",\n",
       " '_i35': 'seqs.shape',\n",
       " '_35': torch.Size([229, 129, 88]),\n",
       " '_i36': \"def model_1(seqs, lengths, hd=16, batch_size = None, include_prior=True):\\n    with ignore_jit_warnings():\\n        num_seqs, max_len, data_dim = map(int, seqs.shape)\\n        assert lengths.shape == (num_seqs,)\\n        assert lengths.max() == max_len\\n    with poutine.mask(mask=include_prior):\\n        probs_x = pyro.sample(\\n            'probs_x',\\n            dist.Dirichlet(0.9 * torch.eye(hd) + 0.1).to_event(1),\\n        )\\n\\n        probs_y = pyro.sample(\\n            'probs_y',\\n            dist.Beta(0.1, 0.9).expand([hd, data_dim]).to_event(2)\\n        )\\n\\n    tones_plate = pyro.plate('tones', data_dim, dim=-1)\\n    # We subsample batch_size items out of num_sequences items. Note that since\\n    # we're using dim=-1 for the notes plate, we need to batch over a different\\n    # dimension, here dim=-2.\\n    with pyro.plate('seqs', num_seqs, batch_size, dim=-2) as batch:\\n        lengths = lengths[batch]\\n        x = 0\\n        # If we are not using the jit, then we can vary the program structure\\n        # each call by running for a dynamically determined number of time\\n        # steps, lengths.max(). However if we are using the jit, then we try to\\n        # keep a single program structure for all minibatches; the fixed\\n        # structure ends up being faster since each program structure would\\n        # need to trigger a new jit compile stage.    \\n        for t in pyro.markov(range(lengths.max())):\\n            with poutine.mask(mask=(t < lengths).unsqueeze(-1)):\\n                x = pyro.sample(\\n                    'x_{}'.format(t),\\n                    dist.Categorical(probs_x[x]),\\n                    infer={'enumerate': 'parallel'}\\n                )\\n                with tones_plate:\\n                    pyro.sample(\\n                        'y_{}'.format(t),\\n                        dist.Bernoulli(probs_y[x.squeeze(-1)]),\\n                        obs = seqs[batch, t]\\n                    )\\n\\n        \",\n",
       " '_i37': \"guide = AutoDelta(\\n    poutine.block(model_1, expose_fn=lambda msg: msg['name'].startswith('probs_'))\\n)\",\n",
       " '_i38': 'first_available_dim = -3',\n",
       " '_i39': 'guide_trace = poutine.trace(guide).get_trace(\\n    seqs, lengths, hd=16, batch_size=10\\n)',\n",
       " 'guide_trace': <pyro.poutine.trace_struct.Trace at 0x7f11fa6e23e0>,\n",
       " '_i40': 'model_trace = poutine.trace(\\n    poutine.replay(poutine.enum(model_1, first_available_dim), guide_trace)\\n    ).get_trace(\\n        seqs, lengths, hd = 16, batch_size = 10\\n    )',\n",
       " 'model_trace': <pyro.poutine.trace_struct.Trace at 0x7f11fa6e3a00>,\n",
       " '_i41': 'print(model_trace.format_shapes())',\n",
       " '_i42': \"optim = Adam({'lr':0.01})\\nElbo = TraceEnum_ELBO\\nelbo = Elbo(\\n    max_plate_nesting = 2,\\n    strict_enumeration_warning=True,\\n    jit_options={'time_compilation': 'store_true'}\\n)\",\n",
       " 'optim': <pyro.optim.optim.PyroOptim at 0x7f11fc7bd2a0>,\n",
       " 'Elbo': pyro.infer.traceenum_elbo.TraceEnum_ELBO,\n",
       " 'elbo': <pyro.infer.traceenum_elbo.TraceEnum_ELBO at 0x7f11fc7bc850>,\n",
       " '_i43': 'svi = SVI(model_1, guide, optim, elbo)',\n",
       " 'svi': <pyro.infer.svi.SVI at 0x7f11fc7bcb50>,\n",
       " '_i44': \"num_steps = 100\\npyro.set_rng_seed(111)\\npyro.clear_param_store()\\nnum_observations = float(lengths.sum())\\nfor step in range(num_steps):\\n    loss = svi.step(seqs, lengths, hd=16, batch_size=20)\\n    print('{:5d}\\\\t{}'.format(step, loss / num_observations))\",\n",
       " 'num_steps': 100,\n",
       " 'num_observations': 4725.0,\n",
       " 'step': 99,\n",
       " 'loss': 188498.859375,\n",
       " '_i45': 'seqs = seqs[., present_notes]',\n",
       " '_i46': 'seqs = seqs[..., present_notes]',\n",
       " '_i47': \"def model_1(seqs, lengths, hd=16, batch_size = None, include_prior=True):\\n    with ignore_jit_warnings():\\n        num_seqs, max_len, data_dim = map(int, seqs.shape)\\n        assert lengths.shape == (num_seqs,)\\n        assert lengths.max() == max_len\\n    with poutine.mask(mask=include_prior):\\n        probs_x = pyro.sample(\\n            'probs_x',\\n            dist.Dirichlet(0.9 * torch.eye(hd) + 0.1).to_event(1),\\n        )\\n\\n        probs_y = pyro.sample(\\n            'probs_y',\\n            dist.Beta(0.1, 0.9).expand([hd, data_dim]).to_event(2)\\n        )\\n\\n    tones_plate = pyro.plate('tones', data_dim, dim=-1)\\n    # We subsample batch_size items out of num_sequences items. Note that since\\n    # we're using dim=-1 for the notes plate, we need to batch over a different\\n    # dimension, here dim=-2.\\n    with pyro.plate('seqs', num_seqs, batch_size, dim=-2) as batch:\\n        lengths = lengths[batch]\\n        x = 0\\n        # If we are not using the jit, then we can vary the program structure\\n        # each call by running for a dynamically determined number of time\\n        # steps, lengths.max(). However if we are using the jit, then we try to\\n        # keep a single program structure for all minibatches; the fixed\\n        # structure ends up being faster since each program structure would\\n        # need to trigger a new jit compile stage.    \\n        for t in pyro.markov(range(lengths.max())):\\n            with poutine.mask(mask=(t < lengths).unsqueeze(-1)):\\n                x = pyro.sample(\\n                    'x_{}'.format(t),\\n                    dist.Categorical(probs_x[x]),\\n                    infer={'enumerate': 'parallel'}\\n                )\\n                with tones_plate:\\n                    pyro.sample(\\n                        'y_{}'.format(t),\\n                        dist.Bernoulli(probs_y[x.squeeze(-1)]),\\n                        obs = seqs[batch, t]\\n                    )\\n\\n        \",\n",
       " '_i48': \"guide = AutoDelta(\\n    poutine.block(model_1, expose_fn=lambda msg: msg['name'].startswith('probs_'))\\n)\",\n",
       " '_i49': 'first_available_dim = -3',\n",
       " '_i50': 'guide_trace = poutine.trace(guide).get_trace(\\n    seqs, lengths, hd=16, batch_size=10\\n)',\n",
       " '_i51': 'model_trace = poutine.trace(\\n    poutine.replay(poutine.enum(model_1, first_available_dim), guide_trace)\\n    ).get_trace(\\n        seqs, lengths, hd = 16, batch_size = 10\\n    )',\n",
       " '_i52': 'model_trace = poutine.trace(\\n    poutine.replay(poutine.enum(model_1, first_available_dim), guide_trace)\\n    ).get_trace(\\n        seqs, lengths, hd = 16, batch_size = 10\\n    )',\n",
       " '_i53': 'seqs',\n",
       " '_53': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 1.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 1.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 1.,  ..., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]]),\n",
       " '_i54': 'seqs.shape',\n",
       " '_54': torch.Size([229, 129, 51]),\n",
       " '_i55': \"def model_1(seqs, lengths, hd=16, batch_size = None, include_prior=True):\\n    with ignore_jit_warnings():\\n        num_seqs, max_len, data_dim = map(int, seqs.shape)\\n        assert lengths.shape == (num_seqs,)\\n        assert lengths.max() == max_len\\n    with poutine.mask(mask=include_prior):\\n        probs_x = pyro.sample(\\n            'probs_x',\\n            dist.Dirichlet(0.9 * torch.eye(hd) + 0.1).to_event(1),\\n        )\\n\\n        probs_y = pyro.sample(\\n            'probs_y',\\n            dist.Beta(0.1, 0.9).expand([hd, data_dim]).to_event(2)\\n        )\\n\\n    tones_plate = pyro.plate('tones', data_dim, dim=-1)\\n    # We subsample batch_size items out of num_sequences items. Note that since\\n    # we're using dim=-1 for the notes plate, we need to batch over a different\\n    # dimension, here dim=-2.\\n    with pyro.plate('seqs', num_seqs, batch_size, dim=-2) as batch:\\n        lengths = lengths[batch]\\n        x = 0\\n        # If we are not using the jit, then we can vary the program structure\\n        # each call by running for a dynamically determined number of time\\n        # steps, lengths.max(). However if we are using the jit, then we try to\\n        # keep a single program structure for all minibatches; the fixed\\n        # structure ends up being faster since each program structure would\\n        # need to trigger a new jit compile stage.    \\n        for t in pyro.markov(range(lengths.max())):\\n            with poutine.mask(mask=(t < lengths).unsqueeze(-1)):\\n                x = pyro.sample(\\n                    'x_{}'.format(t),\\n                    dist.Categorical(probs_x[x]),\\n                    infer={'enumerate': 'parallel'}\\n                )\\n                with tones_plate:\\n                    pyro.sample(\\n                        'y_{}'.format(t),\\n                        dist.Bernoulli(probs_y[x.squeeze(-1)]),\\n                        obs = seqs[batch, t]\\n                    )\\n\\n        \",\n",
       " '_i56': \"guide = AutoDelta(\\n    poutine.block(model_1, expose_fn=lambda msg: msg['name'].startswith('probs_'))\\n)\",\n",
       " '_i57': 'first_available_dim = -3',\n",
       " '_i58': 'guide_trace = poutine.trace(guide).get_trace(\\n    seqs, lengths, hd=16, batch_size=10\\n)',\n",
       " '_i59': 'model_trace = poutine.trace(\\n    poutine.replay(poutine.enum(model_1, first_available_dim), guide_trace)\\n    ).get_trace(\\n        seqs, lengths, hd = 16, batch_size = 10\\n    )',\n",
       " '_i60': 'guide_trace = poutine.trace(guide).get_trace(\\n    seqs, lengths, hd=16, batch_size=10\\n)',\n",
       " '_i61': 'data = poly.load_data(poly.JSB_CHORALES)',\n",
       " '_i62': \"seqs = data['train']['sequences']\\nlengths = data['train']['sequence_lengths']\",\n",
       " '_i63': 'seqs.shape',\n",
       " '_63': torch.Size([229, 129, 88]),\n",
       " '_i64': 'present_notes = (seqs == 1).sum(0).sum(0) > 0',\n",
       " '_i65': '#seqs = seqs[..., present_notes]',\n",
       " '_i66': \"def model_1(seqs, lengths, hd=16, batch_size = None, include_prior=True):\\n    with ignore_jit_warnings():\\n        num_seqs, max_len, data_dim = map(int, seqs.shape)\\n        assert lengths.shape == (num_seqs,)\\n        assert lengths.max() == max_len\\n    with poutine.mask(mask=include_prior):\\n        probs_x = pyro.sample(\\n            'probs_x',\\n            dist.Dirichlet(0.9 * torch.eye(hd) + 0.1).to_event(1),\\n        )\\n\\n        probs_y = pyro.sample(\\n            'probs_y',\\n            dist.Beta(0.1, 0.9).expand([hd, data_dim]).to_event(2)\\n        )\\n\\n    tones_plate = pyro.plate('tones', data_dim, dim=-1)\\n    # We subsample batch_size items out of num_sequences items. Note that since\\n    # we're using dim=-1 for the notes plate, we need to batch over a different\\n    # dimension, here dim=-2.\\n    with pyro.plate('seqs', num_seqs, batch_size, dim=-2) as batch:\\n        lengths = lengths[batch]\\n        x = 0\\n        # If we are not using the jit, then we can vary the program structure\\n        # each call by running for a dynamically determined number of time\\n        # steps, lengths.max(). However if we are using the jit, then we try to\\n        # keep a single program structure for all minibatches; the fixed\\n        # structure ends up being faster since each program structure would\\n        # need to trigger a new jit compile stage.    \\n        for t in pyro.markov(range(lengths.max())):\\n            with poutine.mask(mask=(t < lengths).unsqueeze(-1)):\\n                x = pyro.sample(\\n                    'x_{}'.format(t),\\n                    dist.Categorical(probs_x[x]),\\n                    infer={'enumerate': 'parallel'}\\n                )\\n                with tones_plate:\\n                    pyro.sample(\\n                        'y_{}'.format(t),\\n                        dist.Bernoulli(probs_y[x.squeeze(-1)]),\\n                        obs = seqs[batch, t]\\n                    )\\n\\n        \",\n",
       " '_i67': \"guide = AutoDelta(\\n    poutine.block(model_1, expose_fn=lambda msg: msg['name'].startswith('probs_'))\\n)\",\n",
       " '_i68': 'first_available_dim = -3',\n",
       " '_i69': 'guide_trace = poutine.trace(guide).get_trace(\\n    seqs, lengths, hd=16, batch_size=10\\n)',\n",
       " '_i70': 'model_trace = poutine.trace(\\n    poutine.replay(poutine.enum(model_1, first_available_dim), guide_trace)\\n    ).get_trace(\\n        seqs, lengths, hd = 16, batch_size = 10\\n    )',\n",
       " '_i71': 'print(model_trace.format_shapes())',\n",
       " '_i72': \"# Notice that we're now using dim=-2 as a batch dimension (of size 10),\\n# and that the enumeration dimensions are now dims -3 and -4.\",\n",
       " '_i73': \"optim = Adam({'lr':0.01})\\nElbo = TraceEnum_ELBO\\nelbo = Elbo(\\n    max_plate_nesting = 2,\\n    strict_enumeration_warning=True,\\n    jit_options={'time_compilation': 'store_true'}\\n)\",\n",
       " '_i74': 'svi = SVI(model_1, guide, optim, elbo)',\n",
       " '_i75': \"num_steps = 100\\npyro.set_rng_seed(111)\\npyro.clear_param_store()\\nnum_observations = float(lengths.sum())\\nfor step in range(num_steps):\\n    loss = svi.step(seqs, lengths, hd=16, batch_size=20)\\n    print('{:5d}\\\\t{}'.format(step, loss / num_observations))\",\n",
       " '_i76': \"test_sequences = data['test']['sequences']\\ntest_lengths = data['test']['sequence_lengths']\",\n",
       " 'test_sequences': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]]),\n",
       " 'test_lengths': tensor([ 84,  61,  57,  39,  32, 101,  57,  57,  48,  71,  33,  49,  69,  65,\n",
       "         101,  81,  53,  57,  61,  49,  41,  57,  37,  33,  56, 112,  36,  64,\n",
       "          39,  65,  49,  49, 145,  48,  57,  49,  52,  49,  64,  68,  90,  49,\n",
       "          60,  68,  41,  41,  33,  48,  49, 117,  49, 160,  57,  45,  41,  49,\n",
       "          32,  64,  65,  76,  49,  49,  45,  49,  93,  57,  87,  57,  80,  56,\n",
       "          65, 109,  41,  69,  73,  44,  73]),\n",
       " '_i77': 'test_loss = elbo.loss(\\n    model_0,\\n    guide,\\n    test_sequences,\\n    test_lengths,\\n    hidden_dim = 16\\n)',\n",
       " '_i78': 'test_loss = elbo.loss(\\n    model_1,\\n    guide,\\n    test_sequences,\\n    test_lengths,\\n    hidden_dim = 16\\n)',\n",
       " '_i79': 'test_loss = elbo.loss(\\n    model_1,\\n    guide,\\n    test_sequences,\\n    test_lengths,\\n    hd = 16\\n)',\n",
       " 'test_loss': 65367.390625,\n",
       " '_i80': 'test_loss',\n",
       " '_80': 65367.390625,\n",
       " '_i81': 'test_loss / num_observations',\n",
       " '_81': 4.734365946621279,\n",
       " '_i82': 'num_observations = float(test_lengths.sum())',\n",
       " '_i83': 'test_loss / num_observations',\n",
       " '_83': 13.834368386243387,\n",
       " '_i84': 'import argparse\\nimport logging\\nimport sys\\n\\nimport torch\\nimport torch.nn as nn\\nfrom torch.distributions import constraints\\n\\nimport pyro\\nimport pyro.contrib.examples.polyphonic_data_loader as poly\\nimport pyro.distributions as dist\\nfrom pyro import poutine\\nfrom pyro.infer import SVI, JitTraceEnum_ELBO, TraceEnum_ELBO, TraceTMC_ELBO\\nfrom pyro.infer.autoguide import AutoDelta\\nfrom pyro.ops.indexing import Vindex\\nfrom pyro.optim import Adam\\nfrom pyro.util import ignore_jit_warnings',\n",
       " '_i85': 'log = logging.getLogger()\\ndebug_handler = logging.StreamHandler(sys.stdout)\\ndebug_handler.setLevel(logging.DEBUG)\\ndebug_handler.addFilter(filter=lambda record: record.levelno <= logging.DEBUG)\\nlog.addHandler(debug_handler)',\n",
       " '_i86': 'globals()'}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "globals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    name[len(\"model_\") :]: model\n",
    "    for name, model in globals().items()\n",
    "    if name.startswith(\"model_\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # logging.info(\"-\" * 40)\n",
    "    # model = models[args.model]\n",
    "    # logging.info(\n",
    "    #     \"Training {} on {} sequences\".format(\n",
    "    #         model.__name__, len(data[\"train\"][\"sequences\"])\n",
    "    #     )\n",
    "    # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = poly.load_data(poly.JSB_CHORALES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs = data['train']['sequences']\n",
    "lengths = data['train']['sequence_lengths']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([229, 129, 88])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "present_notes = (seqs == 1).sum(0).sum(0) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False,  True, False, False,  True, False,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "present_notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seqs = seqs[..., present_notes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_1(seqs, lengths, hd=16, batch_size = None, include_prior=True):\n",
    "    with ignore_jit_warnings():\n",
    "        num_seqs, max_len, data_dim = map(int, seqs.shape)\n",
    "        assert lengths.shape == (num_seqs,)\n",
    "        assert lengths.max() == max_len\n",
    "    with poutine.mask(mask=include_prior):\n",
    "        probs_x = pyro.sample(\n",
    "            'probs_x',\n",
    "            dist.Dirichlet(0.9 * torch.eye(hd) + 0.1).to_event(1),\n",
    "        )\n",
    "\n",
    "        probs_y = pyro.sample(\n",
    "            'probs_y',\n",
    "            dist.Beta(0.1, 0.9).expand([hd, data_dim]).to_event(2)\n",
    "        )\n",
    "\n",
    "    tones_plate = pyro.plate('tones', data_dim, dim=-1)\n",
    "    # We subsample batch_size items out of num_sequences items. Note that since\n",
    "    # we're using dim=-1 for the notes plate, we need to batch over a different\n",
    "    # dimension, here dim=-2.\n",
    "    with pyro.plate('seqs', num_seqs, batch_size, dim=-2) as batch:\n",
    "        lengths = lengths[batch]\n",
    "        x = 0\n",
    "        # If we are not using the jit, then we can vary the program structure\n",
    "        # each call by running for a dynamically determined number of time\n",
    "        # steps, lengths.max(). However if we are using the jit, then we try to\n",
    "        # keep a single program structure for all minibatches; the fixed\n",
    "        # structure ends up being faster since each program structure would\n",
    "        # need to trigger a new jit compile stage.    \n",
    "        for t in pyro.markov(range(lengths.max())):\n",
    "            with poutine.mask(mask=(t < lengths).unsqueeze(-1)):\n",
    "                x = pyro.sample(\n",
    "                    'x_{}'.format(t),\n",
    "                    dist.Categorical(probs_x[x]),\n",
    "                    infer={'enumerate': 'parallel'}\n",
    "                )\n",
    "                with tones_plate:\n",
    "                    pyro.sample(\n",
    "                        'y_{}'.format(t),\n",
    "                        dist.Bernoulli(probs_y[x.squeeze(-1)]),\n",
    "                        obs = seqs[batch, t]\n",
    "                    )\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "guide = AutoDelta(\n",
    "    poutine.block(model_1, expose_fn=lambda msg: msg['name'].startswith('probs_'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_available_dim = -3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "guide_trace = poutine.trace(guide).get_trace(\n",
    "    seqs, lengths, hd=16, batch_size=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trace = poutine.trace(\n",
    "    poutine.replay(poutine.enum(model_1, first_available_dim), guide_trace)\n",
    "    ).get_trace(\n",
    "        seqs, lengths, hd = 16, batch_size = 10\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trace Shapes:                    \n",
      " Param Sites:                    \n",
      "Sample Sites:                    \n",
      " probs_x dist             | 16 16\n",
      "        value             | 16 16\n",
      " probs_y dist             | 16 88\n",
      "        value             | 16 88\n",
      "   tones dist             |      \n",
      "        value          88 |      \n",
      "    seqs dist             |      \n",
      "        value          10 |      \n",
      "     x_0 dist       10  1 |      \n",
      "        value    16  1  1 |      \n",
      "     y_0 dist    16 10 88 |      \n",
      "        value       10 88 |      \n",
      "     x_1 dist    16 10  1 |      \n",
      "        value 16  1  1  1 |      \n",
      "     y_1 dist 16  1 10 88 |      \n",
      "        value       10 88 |      \n",
      "     x_2 dist 16  1 10  1 |      \n",
      "        value    16  1  1 |      \n",
      "     y_2 dist    16 10 88 |      \n",
      "        value       10 88 |      \n",
      "     x_3 dist    16 10  1 |      \n",
      "        value 16  1  1  1 |      \n",
      "     y_3 dist 16  1 10 88 |      \n",
      "        value       10 88 |      \n",
      "     x_4 dist 16  1 10  1 |      \n",
      "        value    16  1  1 |      \n",
      "     y_4 dist    16 10 88 |      \n",
      "        value       10 88 |      \n",
      "     x_5 dist    16 10  1 |      \n",
      "        value 16  1  1  1 |      \n",
      "     y_5 dist 16  1 10 88 |      \n",
      "        value       10 88 |      \n",
      "     x_6 dist 16  1 10  1 |      \n",
      "        value    16  1  1 |      \n",
      "     y_6 dist    16 10 88 |      \n",
      "        value       10 88 |      \n",
      "     x_7 dist    16 10  1 |      \n",
      "        value 16  1  1  1 |      \n",
      "     y_7 dist 16  1 10 88 |      \n",
      "        value       10 88 |      \n",
      "     x_8 dist 16  1 10  1 |      \n",
      "        value    16  1  1 |      \n",
      "     y_8 dist    16 10 88 |      \n",
      "        value       10 88 |      \n",
      "     x_9 dist    16 10  1 |      \n",
      "        value 16  1  1  1 |      \n",
      "     y_9 dist 16  1 10 88 |      \n",
      "        value       10 88 |      \n",
      "    x_10 dist 16  1 10  1 |      \n",
      "        value    16  1  1 |      \n",
      "    y_10 dist    16 10 88 |      \n",
      "        value       10 88 |      \n",
      "    x_11 dist    16 10  1 |      \n",
      "        value 16  1  1  1 |      \n",
      "    y_11 dist 16  1 10 88 |      \n",
      "        value       10 88 |      \n",
      "    x_12 dist 16  1 10  1 |      \n",
      "        value    16  1  1 |      \n",
      "    y_12 dist    16 10 88 |      \n",
      "        value       10 88 |      \n",
      "    x_13 dist    16 10  1 |      \n",
      "        value 16  1  1  1 |      \n",
      "    y_13 dist 16  1 10 88 |      \n",
      "        value       10 88 |      \n",
      "    x_14 dist 16  1 10  1 |      \n",
      "        value    16  1  1 |      \n",
      "    y_14 dist    16 10 88 |      \n",
      "        value       10 88 |      \n",
      "    x_15 dist    16 10  1 |      \n",
      "        value 16  1  1  1 |      \n",
      "    y_15 dist 16  1 10 88 |      \n",
      "        value       10 88 |      \n",
      "    x_16 dist 16  1 10  1 |      \n",
      "        value    16  1  1 |      \n",
      "    y_16 dist    16 10 88 |      \n",
      "        value       10 88 |      \n",
      "    x_17 dist    16 10  1 |      \n",
      "        value 16  1  1  1 |      \n",
      "    y_17 dist 16  1 10 88 |      \n",
      "        value       10 88 |      \n",
      "    x_18 dist 16  1 10  1 |      \n",
      "        value    16  1  1 |      \n",
      "    y_18 dist    16 10 88 |      \n",
      "        value       10 88 |      \n",
      "    x_19 dist    16 10  1 |      \n",
      "        value 16  1  1  1 |      \n",
      "    y_19 dist 16  1 10 88 |      \n",
      "        value       10 88 |      \n",
      "    x_20 dist 16  1 10  1 |      \n",
      "        value    16  1  1 |      \n",
      "    y_20 dist    16 10 88 |      \n",
      "        value       10 88 |      \n",
      "    x_21 dist    16 10  1 |      \n",
      "        value 16  1  1  1 |      \n",
      "    y_21 dist 16  1 10 88 |      \n",
      "        value       10 88 |      \n",
      "    x_22 dist 16  1 10  1 |      \n",
      "        value    16  1  1 |      \n",
      "    y_22 dist    16 10 88 |      \n",
      "        value       10 88 |      \n",
      "    x_23 dist    16 10  1 |      \n",
      "        value 16  1  1  1 |      \n",
      "    y_23 dist 16  1 10 88 |      \n",
      "        value       10 88 |      \n",
      "    x_24 dist 16  1 10  1 |      \n",
      "        value    16  1  1 |      \n",
      "    y_24 dist    16 10 88 |      \n",
      "        value       10 88 |      \n",
      "    x_25 dist    16 10  1 |      \n",
      "        value 16  1  1  1 |      \n",
      "    y_25 dist 16  1 10 88 |      \n",
      "        value       10 88 |      \n",
      "    x_26 dist 16  1 10  1 |      \n",
      "        value    16  1  1 |      \n",
      "    y_26 dist    16 10 88 |      \n",
      "        value       10 88 |      \n",
      "    x_27 dist    16 10  1 |      \n",
      "        value 16  1  1  1 |      \n",
      "    y_27 dist 16  1 10 88 |      \n",
      "        value       10 88 |      \n",
      "    x_28 dist 16  1 10  1 |      \n",
      "        value    16  1  1 |      \n",
      "    y_28 dist    16 10 88 |      \n",
      "        value       10 88 |      \n",
      "    x_29 dist    16 10  1 |      \n",
      "        value 16  1  1  1 |      \n",
      "    y_29 dist 16  1 10 88 |      \n",
      "        value       10 88 |      \n",
      "    x_30 dist 16  1 10  1 |      \n",
      "        value    16  1  1 |      \n",
      "    y_30 dist    16 10 88 |      \n",
      "        value       10 88 |      \n",
      "    x_31 dist    16 10  1 |      \n",
      "        value 16  1  1  1 |      \n",
      "    y_31 dist 16  1 10 88 |      \n",
      "        value       10 88 |      \n",
      "    x_32 dist 16  1 10  1 |      \n",
      "        value    16  1  1 |      \n",
      "    y_32 dist    16 10 88 |      \n",
      "        value       10 88 |      \n",
      "    x_33 dist    16 10  1 |      \n",
      "        value 16  1  1  1 |      \n",
      "    y_33 dist 16  1 10 88 |      \n",
      "        value       10 88 |      \n",
      "    x_34 dist 16  1 10  1 |      \n",
      "        value    16  1  1 |      \n",
      "    y_34 dist    16 10 88 |      \n",
      "        value       10 88 |      \n",
      "    x_35 dist    16 10  1 |      \n",
      "        value 16  1  1  1 |      \n",
      "    y_35 dist 16  1 10 88 |      \n",
      "        value       10 88 |      \n",
      "    x_36 dist 16  1 10  1 |      \n",
      "        value    16  1  1 |      \n",
      "    y_36 dist    16 10 88 |      \n",
      "        value       10 88 |      \n",
      "    x_37 dist    16 10  1 |      \n",
      "        value 16  1  1  1 |      \n",
      "    y_37 dist 16  1 10 88 |      \n",
      "        value       10 88 |      \n",
      "    x_38 dist 16  1 10  1 |      \n",
      "        value    16  1  1 |      \n",
      "    y_38 dist    16 10 88 |      \n",
      "        value       10 88 |      \n",
      "    x_39 dist    16 10  1 |      \n",
      "        value 16  1  1  1 |      \n",
      "    y_39 dist 16  1 10 88 |      \n",
      "        value       10 88 |      \n",
      "    x_40 dist 16  1 10  1 |      \n",
      "        value    16  1  1 |      \n",
      "    y_40 dist    16 10 88 |      \n",
      "        value       10 88 |      \n",
      "    x_41 dist    16 10  1 |      \n",
      "        value 16  1  1  1 |      \n",
      "    y_41 dist 16  1 10 88 |      \n",
      "        value       10 88 |      \n",
      "    x_42 dist 16  1 10  1 |      \n",
      "        value    16  1  1 |      \n",
      "    y_42 dist    16 10 88 |      \n",
      "        value       10 88 |      \n",
      "    x_43 dist    16 10  1 |      \n",
      "        value 16  1  1  1 |      \n",
      "    y_43 dist 16  1 10 88 |      \n",
      "        value       10 88 |      \n",
      "    x_44 dist 16  1 10  1 |      \n",
      "        value    16  1  1 |      \n",
      "    y_44 dist    16 10 88 |      \n",
      "        value       10 88 |      \n",
      "    x_45 dist    16 10  1 |      \n",
      "        value 16  1  1  1 |      \n",
      "    y_45 dist 16  1 10 88 |      \n",
      "        value       10 88 |      \n",
      "    x_46 dist 16  1 10  1 |      \n",
      "        value    16  1  1 |      \n",
      "    y_46 dist    16 10 88 |      \n",
      "        value       10 88 |      \n",
      "    x_47 dist    16 10  1 |      \n",
      "        value 16  1  1  1 |      \n",
      "    y_47 dist 16  1 10 88 |      \n",
      "        value       10 88 |      \n",
      "    x_48 dist 16  1 10  1 |      \n",
      "        value    16  1  1 |      \n",
      "    y_48 dist    16 10 88 |      \n",
      "        value       10 88 |      \n",
      "    x_49 dist    16 10  1 |      \n",
      "        value 16  1  1  1 |      \n",
      "    y_49 dist 16  1 10 88 |      \n",
      "        value       10 88 |      \n",
      "    x_50 dist 16  1 10  1 |      \n",
      "        value    16  1  1 |      \n",
      "    y_50 dist    16 10 88 |      \n",
      "        value       10 88 |      \n",
      "    x_51 dist    16 10  1 |      \n",
      "        value 16  1  1  1 |      \n",
      "    y_51 dist 16  1 10 88 |      \n",
      "        value       10 88 |      \n",
      "    x_52 dist 16  1 10  1 |      \n",
      "        value    16  1  1 |      \n",
      "    y_52 dist    16 10 88 |      \n",
      "        value       10 88 |      \n",
      "    x_53 dist    16 10  1 |      \n",
      "        value 16  1  1  1 |      \n",
      "    y_53 dist 16  1 10 88 |      \n",
      "        value       10 88 |      \n",
      "    x_54 dist 16  1 10  1 |      \n",
      "        value    16  1  1 |      \n",
      "    y_54 dist    16 10 88 |      \n",
      "        value       10 88 |      \n",
      "    x_55 dist    16 10  1 |      \n",
      "        value 16  1  1  1 |      \n",
      "    y_55 dist 16  1 10 88 |      \n",
      "        value       10 88 |      \n",
      "    x_56 dist 16  1 10  1 |      \n",
      "        value    16  1  1 |      \n",
      "    y_56 dist    16 10 88 |      \n",
      "        value       10 88 |      \n",
      "    x_57 dist    16 10  1 |      \n",
      "        value 16  1  1  1 |      \n",
      "    y_57 dist 16  1 10 88 |      \n",
      "        value       10 88 |      \n",
      "    x_58 dist 16  1 10  1 |      \n",
      "        value    16  1  1 |      \n",
      "    y_58 dist    16 10 88 |      \n",
      "        value       10 88 |      \n",
      "    x_59 dist    16 10  1 |      \n",
      "        value 16  1  1  1 |      \n",
      "    y_59 dist 16  1 10 88 |      \n",
      "        value       10 88 |      \n",
      "    x_60 dist 16  1 10  1 |      \n",
      "        value    16  1  1 |      \n",
      "    y_60 dist    16 10 88 |      \n",
      "        value       10 88 |      \n",
      "    x_61 dist    16 10  1 |      \n",
      "        value 16  1  1  1 |      \n",
      "    y_61 dist 16  1 10 88 |      \n",
      "        value       10 88 |      \n",
      "    x_62 dist 16  1 10  1 |      \n",
      "        value    16  1  1 |      \n",
      "    y_62 dist    16 10 88 |      \n",
      "        value       10 88 |      \n",
      "    x_63 dist    16 10  1 |      \n",
      "        value 16  1  1  1 |      \n",
      "    y_63 dist 16  1 10 88 |      \n",
      "        value       10 88 |      \n",
      "    x_64 dist 16  1 10  1 |      \n",
      "        value    16  1  1 |      \n",
      "    y_64 dist    16 10 88 |      \n",
      "        value       10 88 |      \n",
      "    x_65 dist    16 10  1 |      \n",
      "        value 16  1  1  1 |      \n",
      "    y_65 dist 16  1 10 88 |      \n",
      "        value       10 88 |      \n",
      "    x_66 dist 16  1 10  1 |      \n",
      "        value    16  1  1 |      \n",
      "    y_66 dist    16 10 88 |      \n",
      "        value       10 88 |      \n",
      "    x_67 dist    16 10  1 |      \n",
      "        value 16  1  1  1 |      \n",
      "    y_67 dist 16  1 10 88 |      \n",
      "        value       10 88 |      \n",
      "    x_68 dist 16  1 10  1 |      \n",
      "        value    16  1  1 |      \n",
      "    y_68 dist    16 10 88 |      \n",
      "        value       10 88 |      \n",
      "    x_69 dist    16 10  1 |      \n",
      "        value 16  1  1  1 |      \n",
      "    y_69 dist 16  1 10 88 |      \n",
      "        value       10 88 |      \n",
      "    x_70 dist 16  1 10  1 |      \n",
      "        value    16  1  1 |      \n",
      "    y_70 dist    16 10 88 |      \n",
      "        value       10 88 |      \n",
      "    x_71 dist    16 10  1 |      \n",
      "        value 16  1  1  1 |      \n",
      "    y_71 dist 16  1 10 88 |      \n",
      "        value       10 88 |      \n",
      "    x_72 dist 16  1 10  1 |      \n",
      "        value    16  1  1 |      \n",
      "    y_72 dist    16 10 88 |      \n",
      "        value       10 88 |      \n",
      "    x_73 dist    16 10  1 |      \n",
      "        value 16  1  1  1 |      \n",
      "    y_73 dist 16  1 10 88 |      \n",
      "        value       10 88 |      \n",
      "    x_74 dist 16  1 10  1 |      \n",
      "        value    16  1  1 |      \n",
      "    y_74 dist    16 10 88 |      \n",
      "        value       10 88 |      \n",
      "    x_75 dist    16 10  1 |      \n",
      "        value 16  1  1  1 |      \n",
      "    y_75 dist 16  1 10 88 |      \n",
      "        value       10 88 |      \n",
      "    x_76 dist 16  1 10  1 |      \n",
      "        value    16  1  1 |      \n",
      "    y_76 dist    16 10 88 |      \n",
      "        value       10 88 |      \n",
      "    x_77 dist    16 10  1 |      \n",
      "        value 16  1  1  1 |      \n",
      "    y_77 dist 16  1 10 88 |      \n",
      "        value       10 88 |      \n",
      "    x_78 dist 16  1 10  1 |      \n",
      "        value    16  1  1 |      \n",
      "    y_78 dist    16 10 88 |      \n",
      "        value       10 88 |      \n",
      "    x_79 dist    16 10  1 |      \n",
      "        value 16  1  1  1 |      \n",
      "    y_79 dist 16  1 10 88 |      \n",
      "        value       10 88 |      \n",
      "    x_80 dist 16  1 10  1 |      \n",
      "        value    16  1  1 |      \n",
      "    y_80 dist    16 10 88 |      \n",
      "        value       10 88 |      \n",
      "    x_81 dist    16 10  1 |      \n",
      "        value 16  1  1  1 |      \n",
      "    y_81 dist 16  1 10 88 |      \n",
      "        value       10 88 |      \n",
      "    x_82 dist 16  1 10  1 |      \n",
      "        value    16  1  1 |      \n",
      "    y_82 dist    16 10 88 |      \n",
      "        value       10 88 |      \n",
      "    x_83 dist    16 10  1 |      \n",
      "        value 16  1  1  1 |      \n",
      "    y_83 dist 16  1 10 88 |      \n",
      "        value       10 88 |      \n",
      "    x_84 dist 16  1 10  1 |      \n",
      "        value    16  1  1 |      \n",
      "    y_84 dist    16 10 88 |      \n",
      "        value       10 88 |      \n",
      "    x_85 dist    16 10  1 |      \n",
      "        value 16  1  1  1 |      \n",
      "    y_85 dist 16  1 10 88 |      \n",
      "        value       10 88 |      \n",
      "    x_86 dist 16  1 10  1 |      \n",
      "        value    16  1  1 |      \n",
      "    y_86 dist    16 10 88 |      \n",
      "        value       10 88 |      \n",
      "    x_87 dist    16 10  1 |      \n",
      "        value 16  1  1  1 |      \n",
      "    y_87 dist 16  1 10 88 |      \n",
      "        value       10 88 |      \n",
      "    x_88 dist 16  1 10  1 |      \n",
      "        value    16  1  1 |      \n",
      "    y_88 dist    16 10 88 |      \n",
      "        value       10 88 |      \n",
      "    x_89 dist    16 10  1 |      \n",
      "        value 16  1  1  1 |      \n",
      "    y_89 dist 16  1 10 88 |      \n",
      "        value       10 88 |      \n",
      "    x_90 dist 16  1 10  1 |      \n",
      "        value    16  1  1 |      \n",
      "    y_90 dist    16 10 88 |      \n",
      "        value       10 88 |      \n",
      "    x_91 dist    16 10  1 |      \n",
      "        value 16  1  1  1 |      \n",
      "    y_91 dist 16  1 10 88 |      \n",
      "        value       10 88 |      \n",
      "    x_92 dist 16  1 10  1 |      \n",
      "        value    16  1  1 |      \n",
      "    y_92 dist    16 10 88 |      \n",
      "        value       10 88 |      \n",
      "    x_93 dist    16 10  1 |      \n",
      "        value 16  1  1  1 |      \n",
      "    y_93 dist 16  1 10 88 |      \n",
      "        value       10 88 |      \n",
      "    x_94 dist 16  1 10  1 |      \n",
      "        value    16  1  1 |      \n",
      "    y_94 dist    16 10 88 |      \n",
      "        value       10 88 |      \n",
      "    x_95 dist    16 10  1 |      \n",
      "        value 16  1  1  1 |      \n",
      "    y_95 dist 16  1 10 88 |      \n",
      "        value       10 88 |      \n",
      "    x_96 dist 16  1 10  1 |      \n",
      "        value    16  1  1 |      \n",
      "    y_96 dist    16 10 88 |      \n",
      "        value       10 88 |      \n",
      "    x_97 dist    16 10  1 |      \n",
      "        value 16  1  1  1 |      \n",
      "    y_97 dist 16  1 10 88 |      \n",
      "        value       10 88 |      \n",
      "    x_98 dist 16  1 10  1 |      \n",
      "        value    16  1  1 |      \n",
      "    y_98 dist    16 10 88 |      \n",
      "        value       10 88 |      \n",
      "    x_99 dist    16 10  1 |      \n",
      "        value 16  1  1  1 |      \n",
      "    y_99 dist 16  1 10 88 |      \n",
      "        value       10 88 |      \n",
      "   x_100 dist 16  1 10  1 |      \n",
      "        value    16  1  1 |      \n",
      "   y_100 dist    16 10 88 |      \n",
      "        value       10 88 |      \n",
      "   x_101 dist    16 10  1 |      \n",
      "        value 16  1  1  1 |      \n",
      "   y_101 dist 16  1 10 88 |      \n",
      "        value       10 88 |      \n",
      "   x_102 dist 16  1 10  1 |      \n",
      "        value    16  1  1 |      \n",
      "   y_102 dist    16 10 88 |      \n",
      "        value       10 88 |      \n",
      "   x_103 dist    16 10  1 |      \n",
      "        value 16  1  1  1 |      \n",
      "   y_103 dist 16  1 10 88 |      \n",
      "        value       10 88 |      \n",
      "   x_104 dist 16  1 10  1 |      \n",
      "        value    16  1  1 |      \n",
      "   y_104 dist    16 10 88 |      \n",
      "        value       10 88 |      \n",
      "   x_105 dist    16 10  1 |      \n",
      "        value 16  1  1  1 |      \n",
      "   y_105 dist 16  1 10 88 |      \n",
      "        value       10 88 |      \n",
      "   x_106 dist 16  1 10  1 |      \n",
      "        value    16  1  1 |      \n",
      "   y_106 dist    16 10 88 |      \n",
      "        value       10 88 |      \n",
      "   x_107 dist    16 10  1 |      \n",
      "        value 16  1  1  1 |      \n",
      "   y_107 dist 16  1 10 88 |      \n",
      "        value       10 88 |      \n"
     ]
    }
   ],
   "source": [
    "print(model_trace.format_shapes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice that we're now using dim=-2 as a batch dimension (of size 10),\n",
    "# and that the enumeration dimensions are now dims -3 and -4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = Adam({'lr':0.01})\n",
    "Elbo = TraceEnum_ELBO\n",
    "elbo = Elbo(\n",
    "    max_plate_nesting = 2,\n",
    "    strict_enumeration_warning=True,\n",
    "    jit_options={'time_compilation': 'store_true'}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "svi = SVI(model_1, guide, optim, elbo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0\t11.25366548670964\n",
      "    1\t11.154480064460056\n",
      "    2\t14.366844037444775\n",
      "    3\t10.84172996487289\n",
      "    4\t12.35001946476425\n",
      "    5\t11.265643106757443\n",
      "    6\t12.534415287535309\n",
      "    7\t12.395510429492287\n",
      "    8\t13.153589438328384\n",
      "    9\t13.04273194756283\n",
      "   10\t11.651775367567176\n",
      "   11\t12.644023412037372\n",
      "   12\t13.04648344137032\n",
      "   13\t12.399373958861448\n",
      "   14\t12.423139078003912\n",
      "   15\t13.84249950206417\n",
      "   16\t12.357624302889839\n",
      "   17\t12.813258220467878\n",
      "   18\t12.750212754399943\n",
      "   19\t12.819715542840589\n",
      "   20\t12.382948866516983\n",
      "   21\t12.744561182733396\n",
      "   22\t11.976045891576737\n",
      "   23\t13.548979910552617\n",
      "   24\t10.964897787354241\n",
      "   25\t11.199694901137104\n",
      "   26\t11.954057498008257\n",
      "   27\t10.272297340117332\n",
      "   28\t12.355637086260593\n",
      "   29\t11.913723564134134\n",
      "   30\t11.729987506337364\n",
      "   31\t11.510302744984429\n",
      "   32\t11.706931945752155\n",
      "   33\t10.793706317447672\n",
      "   34\t11.96227909755921\n",
      "   35\t12.137931619830521\n",
      "   36\t11.811151046570581\n",
      "   37\t12.171738067646846\n",
      "   38\t11.689217878612299\n",
      "   39\t11.821370047801839\n",
      "   40\t11.352852040631564\n",
      "   41\t10.983583961034258\n",
      "   42\t11.881708553632215\n",
      "   43\t11.93431660570725\n",
      "   44\t11.572773321503584\n",
      "   45\t10.36022828094445\n",
      "   46\t11.803883446802347\n",
      "   47\t11.368466855580502\n",
      "   48\t10.78838632577678\n",
      "   49\t10.711883012240168\n",
      "   50\t11.148023873759687\n",
      "   51\t12.507318525023539\n",
      "   52\t11.783207793148403\n",
      "   53\t11.24739715361773\n",
      "   54\t12.646433874121822\n",
      "   55\t11.25213093901644\n",
      "   56\t10.089378349750127\n",
      "   57\t12.448991000941552\n",
      "   58\t10.09964714456435\n",
      "   59\t11.787780881074816\n",
      "   60\t11.742993816542334\n",
      "   61\t12.02411367422322\n",
      "   62\t11.769012095313972\n",
      "   63\t11.213234229014269\n",
      "   64\t11.04117929311219\n",
      "   65\t11.596894917433186\n",
      "   66\t10.903476271094373\n",
      "   67\t11.761346146882016\n",
      "   68\t11.477286204461505\n",
      "   69\t9.315578714601289\n",
      "   70\t11.554994749040342\n",
      "   71\t11.491234066053451\n",
      "   72\t10.935315872383574\n",
      "   73\t10.87510411385529\n",
      "   74\t13.161416084232636\n",
      "   75\t12.73631242304628\n",
      "   76\t10.19396411240675\n",
      "   77\t12.280617395161874\n",
      "   78\t12.438291038965742\n",
      "   79\t10.92534131237778\n",
      "   80\t11.377960454841746\n",
      "   81\t11.287681293908888\n",
      "   82\t10.978847912290867\n",
      "   83\t10.483765028608676\n",
      "   84\t11.408628775258927\n",
      "   85\t10.186750832910842\n",
      "   86\t11.507206489461867\n",
      "   87\t12.430282193814731\n",
      "   88\t10.201266794017528\n",
      "   89\t12.91791754182661\n",
      "   90\t10.235807697182588\n",
      "   91\t10.512326175128559\n",
      "   92\t10.255607436445281\n",
      "   93\t11.356939641124068\n",
      "   94\t11.110372003331644\n",
      "   95\t10.057694919243861\n",
      "   96\t10.650933403346128\n",
      "   97\t11.805631880567828\n",
      "   98\t10.616248777793873\n",
      "   99\t9.844060078221192\n"
     ]
    }
   ],
   "source": [
    "num_steps = 100\n",
    "pyro.set_rng_seed(111)\n",
    "pyro.clear_param_store()\n",
    "num_observations = float(lengths.sum())\n",
    "for step in range(num_steps):\n",
    "    loss = svi.step(seqs, lengths, hd=16, batch_size=20)\n",
    "    print('{:5d}\\t{}'.format(step, loss / num_observations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequences = data['test']['sequences']\n",
    "test_lengths = data['test']['sequence_lengths']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = elbo.loss(\n",
    "    model_1,\n",
    "    guide,\n",
    "    test_sequences,\n",
    "    test_lengths,\n",
    "    hd = 16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_observations = float(test_lengths.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.59161789021164"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss / num_observations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
