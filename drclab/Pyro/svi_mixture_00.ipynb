{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Inference with Discrete Latent Variables](http://pyro.ai/examples/enumeration.html#Inference-with-Discrete-Latent-Variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from torch.distributions import constraints\n",
    "from pyro import poutine\n",
    "from pyro.infer import SVI, Trace_ELBO, TraceEnum_ELBO, config_enumerate, infer_discrete\n",
    "from pyro.infer.autoguide import AutoNormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.ops.indexing import Vindex\n",
    "\n",
    "smoke_test = ('CI' in os.environ)\n",
    "assert pyro.__version__.startswith('1.8.2')\n",
    "pyro.set_rng_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pyro’s enumeration strategy (Obermeyer et al. 2019) encompasses popular algorithms including variable elimination, exact message passing, forward-filter-backward-sample, inside-out, Baum-Welch, and many other special-case algorithms. Aside from enumeration, Pyro implements a number of inference strategies including variational inference (SVI) and monte carlo (HMC and NUTS). \n",
    "\n",
    "##### Enumeration can be used either as a stand-alone strategy via infer_discrete, or as a component of other strategies. Thus enumeration allows Pyro to marginalize out discrete latent variables in HMC and SVI models, and to use variational enumeration of discrete variables in SVI guides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The core idea of enumeration is to interpret discrete pyro.sample statements as full enumeration rather than random sampling. Other inference algorithms can then sum out the enumerated values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model():\n",
    "    z = pyro.sample(\"z\", dist.Categorical(torch.ones(5)))\n",
    "    print(f\"model z ={z}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def guide():\n",
    "    z = pyro.sample(\"z\", dist.Categorical(torch.ones(5)))\n",
    "    print(f\"guide z ={z}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "elbo = Trace_ELBO()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "guide z =4\n",
      "model z =4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elbo.loss(model, guide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "guide z =tensor([0, 1, 2, 3, 4])\n",
      "model z =tensor([0, 1, 2, 3, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elbo = TraceEnum_ELBO(max_plate_nesting=0)\n",
    "elbo.loss(model, config_enumerate(guide, \"parallel\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "guide z =4\n",
      "model z =4\n",
      "guide z =3\n",
      "model z =3\n",
      "guide z =2\n",
      "model z =2\n",
      "guide z =1\n",
      "model z =1\n",
      "guide z =0\n",
      "model z =0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elbo = TraceEnum_ELBO(max_plate_nesting=0)\n",
    "elbo.loss(model, config_enumerate(guide, \"sequential\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Parallel enumeration is cheaper but more complex than sequential enumeration, so we’ll focus the rest of this tutorial on the parallel variant. Note that both forms can be interleaved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  A model with a single discrete latent variable is a mixture model. Models with multiple discrete latent variables can be more complex, including HMMs, CRFs, DBNs, and other structured models. In models with multiple discrete latent variables, Pyro enumerates each variable in a different tensor dimension (counting from the right; see Tensor Shapes Tutorial). This allows Pyro to determine the dependency graph among variables and then perform cheap exact inference using variable elimination algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@config_enumerate\n",
    "def model():\n",
    "    p = pyro.param(\"p\", torch.randn(3, 3).exp(), constraint=constraints.simplex)\n",
    "\n",
    "    x = pyro.sample(\"x\", dist.Categorical(p[0]))\n",
    "    y = pyro.sample(\"y\", dist.Categorical(p[x]))\n",
    "    z = pyro.sample(\"z\", dist.Categorical(p[y]))\n",
    "\n",
    "    print(f\"  model x.shape = {x.shape}\")\n",
    "    print(f\"  model y.shape = {y.shape}\")\n",
    "    print(f\"  model z.shape = {z.shape}\")\n",
    "\n",
    "    return x,y,z\n",
    "\n",
    "def guide():\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling\n",
      "  model x.shape = torch.Size([])\n",
      "  model y.shape = torch.Size([])\n",
      "  model z.shape = torch.Size([])\n",
      "Enumerated Inference:\n",
      "  model x.shape = torch.Size([3])\n",
      "  model y.shape = torch.Size([3, 1])\n",
      "  model z.shape = torch.Size([3, 1, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyro.clear_param_store()\n",
    "print(\"Sampling\")\n",
    "model()\n",
    "print(\"Enumerated Inference:\")\n",
    "elbo = TraceEnum_ELBO(max_plate_nesting=0)\n",
    "elbo.loss(model, guide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  model x.shape = torch.Size([3])\n",
      "  model y.shape = torch.Size([3, 1])\n",
      "  model z.shape = torch.Size([3, 1, 1])\n",
      "  model x.shape = torch.Size([])\n",
      "  model y.shape = torch.Size([])\n",
      "  model z.shape = torch.Size([])\n",
      "x = 0\n",
      "y = 0\n",
      "z = 1\n"
     ]
    }
   ],
   "source": [
    "serving_model = infer_discrete(model, first_available_dim=-1)\n",
    "x,y,z = serving_model() # takes the same args as model()\n",
    "print(f\"x = {x}\")\n",
    "print(f\"y = {y}\")\n",
    "print(f\"z = {z}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Notice that under the hood infer_discrete runs the model twice: first in forward-filter mode where sites are enumerated, then in replay-backward-sample model where sites are sampled. infer_discrete can also perform MAP inference by passing temperature=0. Note that while infer_discrete produces correct posterior samples, it does not currently produce correct logprobs, and should not be used in other gradient-based inference algorthms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Indexing with enumerated variables](http://pyro.ai/examples/enumeration.html#Indexing-with-enumerated-variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@config_enumerate\n",
    "def model():\n",
    "    p = pyro.param(\"p\", torch.randn(5, 4, 3, 2).exp(), constraint=constraints.simplex)\n",
    "    x = pyro.sample(\"x\", dist.Categorical(torch.ones(4)))\n",
    "    y = pyro.sample(\"y\", dist.Categorical(torch.ones(3)))\n",
    "    with pyro.plate(\"z_plate\", 5):\n",
    "        p_xy = Vindex(p)[..., x, y, :]\n",
    "        z = pyro.sample(\"z\", dist.Categorical(p_xy))\n",
    "    print(f\"     p.shape = {p.shape}\")\n",
    "    print(f\"     x.shape = {x.shape}\")\n",
    "    print(f\"     y.shape = {y.shape}\")\n",
    "    print(f\"  p_xy.shape = {p_xy.shape}\")\n",
    "    print(f\"     z.shape = {z.shape}\")\n",
    "    return x, y, z\n",
    "\n",
    "def guide():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling:\n",
      "     p.shape = torch.Size([5, 4, 3, 2])\n",
      "     x.shape = torch.Size([])\n",
      "     y.shape = torch.Size([])\n",
      "  p_xy.shape = torch.Size([5, 2])\n",
      "     z.shape = torch.Size([5])\n",
      "Enumerated Inference:\n",
      "     p.shape = torch.Size([5, 4, 3, 2])\n",
      "     x.shape = torch.Size([4, 1])\n",
      "     y.shape = torch.Size([3, 1, 1])\n",
      "  p_xy.shape = torch.Size([3, 4, 5, 2])\n",
      "     z.shape = torch.Size([2, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "pyro.clear_param_store()\n",
    "print(\"Sampling:\")\n",
    "model()\n",
    "print(\"Enumerated Inference:\")\n",
    "elbo = TraceEnum_ELBO(max_plate_nesting=1)\n",
    "elbo.loss(model, guide);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling:\n",
      "    p.shape = torch.Size([5, 4, 3])\n",
      "    c.shape = torch.Size([6])\n",
      "  vdx.shape = torch.Size([5])\n",
      "    pc.shape = torch.Size([5, 6, 3])\n",
      "    x.shape = torch.Size([5, 6])\n",
      "Enumerated Inference:\n",
      "    p.shape = torch.Size([5, 4, 3])\n",
      "    c.shape = torch.Size([4, 1, 1])\n",
      "  vdx.shape = torch.Size([5])\n",
      "    pc.shape = torch.Size([4, 5, 1, 3])\n",
      "    x.shape = torch.Size([5, 6])\n"
     ]
    }
   ],
   "source": [
    "@config_enumerate\n",
    "def model():\n",
    "    data_plate = pyro.plate(\"data_plate\", 6, dim=-1)\n",
    "    feature_plate = pyro.plate(\"feature_plate\", 5, dim=-2)\n",
    "    component_plate = pyro.plate(\"component_plate\", 4, dim=-1)\n",
    "    with feature_plate:\n",
    "        with component_plate:\n",
    "            p = pyro.sample(\"p\", dist.Dirichlet(torch.ones(3)))\n",
    "    with data_plate:\n",
    "        c = pyro.sample(\"c\", dist.Categorical(torch.ones(4)))\n",
    "        with feature_plate as vdx:                # Capture plate index.\n",
    "            pc = Vindex(p)[vdx[..., None], c, :]  # Reshape it and use in Vindex.\n",
    "            x = pyro.sample(\"x\", dist.Categorical(pc),\n",
    "                            obs=torch.zeros(5, 6, dtype=torch.long))\n",
    "    print(f\"    p.shape = {p.shape}\")\n",
    "    print(f\"    c.shape = {c.shape}\")\n",
    "    print(f\"  vdx.shape = {vdx.shape}\")\n",
    "    print(f\"    pc.shape = {pc.shape}\")\n",
    "    print(f\"    x.shape = {x.shape}\")\n",
    "\n",
    "def guide():\n",
    "    feature_plate = pyro.plate(\"feature_plate\", 5, dim=-2)\n",
    "    component_plate = pyro.plate(\"component_plate\", 4, dim=-1)\n",
    "    with feature_plate, component_plate:\n",
    "        pyro.sample(\"p\", dist.Dirichlet(torch.ones(3)))\n",
    "\n",
    "pyro.clear_param_store()\n",
    "print(\"Sampling:\")\n",
    "model()\n",
    "print(\"Enumerated Inference:\")\n",
    "elbo = TraceEnum_ELBO(max_plate_nesting=2)\n",
    "elbo.loss(model, guide);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Plates and enumeration](http://pyro.ai/examples/enumeration.html#Plates-and-enumeration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pyro plates express conditional independence among random variables. Pyro’s enumeration strategy can take advantage of plates to reduce the high cost (exponential in the size of the plate) of enumerating a **cartesian product** down to a low cost (linear in the size of the plate) of enumerating conditionally independent random variables in lock-step. This is especially important for e.g. minibatched data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@config_enumerate\n",
    "def model(data, num_components=3):\n",
    "    print(f\"  Running model with {len(data)} data points\")\n",
    "    p = pyro.sample(\"p\", dist.Dirichlet(0.5 * torch.ones(num_components)))\n",
    "    scale = pyro.sample(\"scale\", dist.LogNormal(0, num_components))\n",
    "    with pyro.plate(\"components\", num_components):\n",
    "        loc = pyro.sample(\"loc\", dist.Normal(0, 10))\n",
    "    with pyro.plate(\"data\", len(data)):\n",
    "        x = pyro.sample(\"x\", dist.Categorical(p))\n",
    "        print(\"    x.shape = {}\".format(x.shape))\n",
    "        pyro.sample(\"obs\", dist.Normal(loc[x], scale), obs=data)\n",
    "        print(\"    dist.Normal(loc[x], scale).batch_shape = {}\".format(\n",
    "            dist.Normal(loc[x], scale).batch_shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling:\n",
      "  Running model with 10 data points\n",
      "    x.shape = torch.Size([10])\n",
      "    dist.Normal(loc[x], scale).batch_shape = torch.Size([10])\n",
      "Enumerated Inference:\n",
      "  Running model with 10 data points\n",
      "    x.shape = torch.Size([10])\n",
      "    dist.Normal(loc[x], scale).batch_shape = torch.Size([10])\n",
      "  Running model with 10 data points\n",
      "    x.shape = torch.Size([3, 1])\n",
      "    dist.Normal(loc[x], scale).batch_shape = torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "guide = AutoNormal(poutine.block(model, hide=[\"x\", \"data\"]))\n",
    "\n",
    "data = torch.randn(10)\n",
    "\n",
    "pyro.clear_param_store()\n",
    "print(\"Sampling:\")\n",
    "model(data)\n",
    "print(\"Enumerated Inference:\")\n",
    "elbo = TraceEnum_ELBO(max_plate_nesting=1)\n",
    "elbo.loss(model, guide, data);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
