{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [High-dimensional Bayesian workflow, with applications to SARS-CoV-2 strains](http://pyro.ai/examples/workflow.html#High-dimensional-Bayesian-workflow,-with-applications-to-SARS-CoV-2-strains)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The fastest way to find a good model of your data is to quickly discard many bad models, i.e. to iterate. In statistics we call this iterative workflow Box’s loop. \n",
    "#### An efficient workflow allows us to discard bad models as quickly as possible. Workflow efficiency demands that code changes to upstream components don’t break previous coding effort on downstream components. \n",
    "#### Pyro’s approaches to this challenge include strategies for variational approximations (pyro.infer.autoguide) and strategies for transforming model coordinate systems to improve geometry (pyro.infer.reparam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Clean the data.\n",
    "\n",
    "2. Create a generative model.\n",
    "\n",
    "3. Sanity check using MAP or mean-field inference.\n",
    "\n",
    "4. Create an initialization heuristic.\n",
    "\n",
    "5. Reparameterize the model, evaluating results under mean field VI.\n",
    "\n",
    "6. Customize the variational family (autoguides, easyguides, custom guides)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The model is a high-dimensional regression model with around 1000 coefficients, a multivariate logistic growth function (using a simple torch.softmax()) and a Multinomial likelihood. While the number of coefficients is relatively small, there are about 500,000 local latent variables to estimate, and plate structure in the model should lead to an approximately block diagonal posterior covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from pprint import pprint\n",
    "import functools\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pyro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro.distributions as dist\n",
    "import pyro.poutine as poutine\n",
    "from pyro.distributions import constraints\n",
    "from pyro.infer import SVI, Trace_ELBO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.infer.autoguide import (\n",
    "    AutoDelta,\n",
    "    AutoNormal,\n",
    "    AutoMultivariateNormal,\n",
    "    AutoLowRankMultivariateNormal,\n",
    "    AutoGuideList,\n",
    "    init_to_feasible\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.infer.reparam import AutoReparam, LocScaleReparam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.nn.module import PyroParam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.optim import ClippedAdam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.ops.special import sparse_multinomial_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    torch.set_default_tensor_type(\"torch.cuda.FloatTensor\")\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.contrib.examples.nextstrain import load_nextstrain_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = load_nextstrain_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(data_set, dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_date <class 'datetime.datetime'>\n",
      "time_step_days <class 'int'>\n",
      "locations <class 'list'>\n",
      "lineages <class 'list'>\n",
      "mutations <class 'list'>\n",
      "features <class 'torch.Tensor'>\n",
      "counts <class 'torch.Tensor'>\n",
      "sparse_counts <class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "for k, v in data_set.items():\n",
    "    print(k, type(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(data_set, torch.Tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The first step to using Pyro is creating a generative model, either a python function or a pyro.nn.Module. Start simple. Start with a shallow hierarchy and later add latent variables to share statistical strength. Start with a slice of your data then add a plate over multiple slices. Start with simple distributions like Normal, LogNormal, Poisson and Multinomial, then consider overdispersed versions like StudentT, Gamma, GammaPoisson/NegativeBinomial, and DirichletMultinomial. Keep your model simple and readable so you can share it and get feedback from domain experts. Use weakly informative priors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note we scale coef by 1/100 because we want to model a very small number, but the automatic parts of Pyro and PyTorch work best for numbers on the **order of 1.0 rather than very small numbers**. When we later interpret coef in a volcano plot we’ll need to duplicate this scaling factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['start_date', 'time_step_days', 'locations', 'lineages', 'mutations', 'features', 'counts', 'sparse_counts'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7177464"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set['counts'].numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(57129)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set['counts'].count_nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set['time_step_days']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1316, 2634])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set['features'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([27, 202, 1316])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set['counts'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(data_set):\n",
    "    features = data_set['features']\n",
    "    counts = data_set['counts']\n",
    "\n",
    "    assert features.shape[0] == counts.shape[-1]\n",
    "    S, M = features.shape # 1316, 2634 mutations\n",
    "    T, P, S = counts.shape # 27 time, 202 places, 1326 clusters/strain\n",
    "\n",
    "    time = torch.arange(float(T)) * data_set['time_step_days'] / 5.5\n",
    "    time -= time.mean()\n",
    "\n",
    "    strain_plate = pyro.plate(\"strain\", S, dim = -1)\n",
    "    place_plate = pyro.plate(\"place\", P, dim= -2)\n",
    "    time_plate = pyro.plate(\"time\", T, dim = -3)\n",
    "\n",
    "    rate_scale = pyro.sample(\"rate_scale\", dist.LogNormal(-4,2))\n",
    "    init_sacle = pyro.sample('init_scale', dist.LogNormal(0, 2))\n",
    "\n",
    "    with pyro.plate(\"mutation\", M, dim = -1):\n",
    "        coef = pyro.sample('coef', dist.Laplace(0,0.5))\n",
    "\n",
    "    with strain_plate:\n",
    "        rate_loc = pyro.deterministic(\"rate_loc\", 0.01 * coef @ features.T) \n",
    "    \n",
    "    with place_plate, strain_plate:\n",
    "        rate = pyro.sample(\"rate\", dist.Normal(rate_loc, rate_scale))\n",
    "        init = pyro.sample(\"init\", dist.Normal(0, init_sacle))\n",
    "\n",
    "    logits = init + rate * time[:, None, None]\n",
    "\n",
    "    with time_plate, place_plate:\n",
    "        pyro.sample(\n",
    "            \"obs\",\n",
    "            dist.Multinomial(logits=logits.unsqueeze(-2), validate_args=False),\n",
    "            obs = counts.unsqueeze(-2),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_svi(model, guide, lr=0.01, num_steps = 1001, log_every = 100, plot=True):\n",
    "    pyro.clear_param_store()\n",
    "    pyro.set_rng_seed(20221114)\n",
    "\n",
    "    num_latents = sum(\n",
    "        site['value'].numel()\n",
    "        for name, site in poutine.trace(guide).get_trace(data_set).iter_stochastic_nodes()\n",
    "        if not site['infer'].get(\"is_auxiliary\")\n",
    "    )\n",
    "\n",
    "    num_params = sum(\n",
    "        p.unconstrained().numel() for p in pyro.get_param_store().values()\n",
    "    )\n",
    "\n",
    "    print(f\"found {num_latents} latent variables and {num_params} learnable parameters\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 538452 latent variables and 1068600 learnable parameters\n",
      "CPU times: user 345 ms, sys: 19.6 ms, total: 365 ms\n",
      "Wall time: 99.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "guide = AutoNormal(model, init_scale=0.01)\n",
    "fit_svi(model, guide)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
