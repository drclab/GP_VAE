{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [High-dimensional Bayesian workflow, with applications to SARS-CoV-2 strains](http://pyro.ai/examples/workflow.html#High-dimensional-Bayesian-workflow,-with-applications-to-SARS-CoV-2-strains)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The fastest way to find a good model of your data is to quickly discard many bad models, i.e. to iterate. In statistics we call this iterative workflow Box’s loop. \n",
    "#### An efficient workflow allows us to discard bad models as quickly as possible. Workflow efficiency demands that code changes to upstream components don’t break previous coding effort on downstream components. \n",
    "#### Pyro’s approaches to this challenge include strategies for variational approximations (pyro.infer.autoguide) and strategies for transforming model coordinate systems to improve geometry (pyro.infer.reparam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Clean the data.\n",
    "\n",
    "2. Create a generative model.\n",
    "\n",
    "3. Sanity check using MAP or mean-field inference.\n",
    "\n",
    "4. Create an initialization heuristic.\n",
    "\n",
    "5. Reparameterize the model, evaluating results under mean field VI.\n",
    "\n",
    "6. Customize the variational family (autoguides, easyguides, custom guides)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The model is a high-dimensional regression model with around 1000 coefficients, a multivariate logistic growth function (using a simple torch.softmax()) and a Multinomial likelihood. While the number of coefficients is relatively small, there are about 500,000 local latent variables to estimate, and plate structure in the model should lead to an approximately block diagonal posterior covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from pprint import pprint\n",
    "import functools\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pyro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro.distributions as dist\n",
    "import pyro.poutine as poutine\n",
    "from pyro.distributions import constraints\n",
    "from pyro.infer import SVI, Trace_ELBO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.infer.autoguide import (\n",
    "    AutoDelta,\n",
    "    AutoNormal,\n",
    "    AutoMultivariateNormal,\n",
    "    AutoLowRankMultivariateNormal,\n",
    "    AutoGuideList,\n",
    "    init_to_feasible\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.infer.reparam import AutoReparam, LocScaleReparam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.nn.module import PyroParam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.optim import ClippedAdam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.ops.special import sparse_multinomial_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    torch.set_default_tensor_type(\"torch.cuda.FloatTensor\")\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.contrib.examples.nextstrain import load_nextstrain_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = load_nextstrain_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isinstance(data_set, dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in data_set.items():\n",
    "    print(k, type(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isinstance(data_set, torch.Tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The first step to using Pyro is creating a generative model, either a python function or a pyro.nn.Module. Start simple. Start with a shallow hierarchy and later add latent variables to share statistical strength. Start with a slice of your data then add a plate over multiple slices. Start with simple distributions like Normal, LogNormal, Poisson and Multinomial, then consider overdispersed versions like StudentT, Gamma, GammaPoisson/NegativeBinomial, and DirichletMultinomial. Keep your model simple and readable so you can share it and get feedback from domain experts. Use weakly informative priors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note we scale coef by 1/100 because we want to model a very small number, but the automatic parts of Pyro and PyTorch work best for numbers on the **order of 1.0 rather than very small numbers**. When we later interpret coef in a volcano plot we’ll need to duplicate this scaling factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set['counts'].numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set['counts'].count_nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set['time_step_days']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set['features'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set['counts'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(data_set):\n",
    "    features = data_set['features']\n",
    "    counts = data_set['counts']\n",
    "\n",
    "    assert features.shape[0] == counts.shape[-1]\n",
    "    S, M = features.shape # 1316, 2634 mutations\n",
    "    T, P, S = counts.shape # 27 time, 202 places, 1326 clusters/strain\n",
    "\n",
    "    time = torch.arange(float(T)) * data_set['time_step_days'] / 5.5\n",
    "    time -= time.mean()\n",
    "\n",
    "    strain_plate = pyro.plate(\"strain\", S, dim = -1)\n",
    "    place_plate = pyro.plate(\"place\", P, dim= -2)\n",
    "    time_plate = pyro.plate(\"time\", T, dim = -3)\n",
    "\n",
    "    rate_scale = pyro.sample(\"rate_scale\", dist.LogNormal(-4,2))\n",
    "    init_sacle = pyro.sample('init_scale', dist.LogNormal(0, 2))\n",
    "\n",
    "    with pyro.plate(\"mutation\", M, dim = -1):\n",
    "        coef = pyro.sample('coef', dist.Laplace(0,0.5))\n",
    "\n",
    "    with strain_plate:\n",
    "        rate_loc = pyro.deterministic(\"rate_loc\", 0.01 * coef @ features.T) \n",
    "    \n",
    "    with place_plate, strain_plate:\n",
    "        rate = pyro.sample(\"rate\", dist.Normal(rate_loc, rate_scale))\n",
    "        init = pyro.sample(\"init\", dist.Normal(0, init_sacle))\n",
    "\n",
    "    logits = init + rate * time[:, None, None]\n",
    "\n",
    "    with time_plate, place_plate:\n",
    "        pyro.sample(\n",
    "            \"obs\",\n",
    "            dist.Multinomial(logits=logits.unsqueeze(-2), validate_args=False),\n",
    "            obs = counts.unsqueeze(-2),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(data_set, predict=None):\n",
    "    features = data_set[\"features\"]\n",
    "    counts = data_set[\"counts\"]\n",
    "    sparse_counts = data_set[\"sparse_counts\"]\n",
    "    assert features.shape[0] == counts.shape[-1]\n",
    "    S, M = features.shape # 1316, 2634 mutations\n",
    "    T, P, S = counts.shape # 27 time, 202 places, 1326 clusters/strain\n",
    "\n",
    "    time = torch.arange(float(T)) * data_set['time_step_days'] / 5.5\n",
    "    time -= time.mean()\n",
    "\n",
    "    rate_scale = pyro.sample(\"rate_scale\", dist.LogNormal(-4,2))\n",
    "    init_sacle = pyro.sample('init_scale', dist.LogNormal(0, 2))\n",
    "\n",
    "    with pyro.plate(\"mutation\", M, dim = -1):\n",
    "        coef = pyro.sample('coef', dist.Laplace(0,0.5))\n",
    "\n",
    "    with pyro.plate(\"strain\", S, dim=-1):\n",
    "        rate_loc = pyro.deterministic(\"rate_loc\", 0.01 * coef @ features.T)\n",
    "        with pyro.plate(\"place\", P, dim=-2):\n",
    "            rate = pyro.sample(\"rate\", dist.Normal(rate_loc, rate_scale))\n",
    "            init = pyro.sample(\"init\", dist.Normal(0, init_sacle))\n",
    "    if predict is not None:\n",
    "        probs = (init+rate * time[predict]).softmax(-1)\n",
    "        return probs\n",
    "\n",
    "    logits = (init + rate * time[:, None, None]).log_softmax(-1)\n",
    "\n",
    "    t,p,s = sparse_counts['index']\n",
    "\n",
    "    pyro.factor(\n",
    "        \"obs\",\n",
    "        sparse_multinomial_likelihood(\n",
    "            sparse_counts[\"total\"], logits[t,p,s], sparse_counts[\"value\"]\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_svi(model, guide, lr=0.01, num_steps = 1001, log_every = 100, plot=True):\n",
    "    pyro.clear_param_store()\n",
    "    pyro.set_rng_seed(20221114)\n",
    "\n",
    "    num_latents = sum(\n",
    "        site['value'].numel()\n",
    "        for name, site in poutine.trace(guide).get_trace(data_set).iter_stochastic_nodes()\n",
    "        if not site['infer'].get(\"is_auxiliary\")\n",
    "    )\n",
    "\n",
    "    num_params = sum(\n",
    "        p.unconstrained().numel() for p in pyro.get_param_store().values()\n",
    "    )\n",
    "\n",
    "    print(f\"found {num_latents} latent variables and {num_params} learnable parameters\")\n",
    "\n",
    "    series =  defaultdict(list)\n",
    "    def hook(g, series):\n",
    "        series.append(torch.linalg.norm(g.reshape(-1), math.inf).item())\n",
    "    for name, value in pyro.get_param_store().named_parameters():\n",
    "        value.register_hook(\n",
    "            functools.partial(hook, series=series[name+\" grad\"])\n",
    "        )\n",
    "    \n",
    "    optim = ClippedAdam({\"lr\": lr, \"lrd\": 0.1 ** (1/num_steps)})\n",
    "\n",
    "    svi = SVI(model, guide, optim, Trace_ELBO())\n",
    "\n",
    "    num_obs = int(data_set['counts'].count_nonzero())\n",
    "\n",
    "    for step in range(num_steps):\n",
    "        loss = svi.step(data_set) / num_obs\n",
    "        series[\"loss\"].append(loss)\n",
    "        median = guide.median()\n",
    "        for name, value in median.items():\n",
    "            if value.numel() == 1:\n",
    "                series[name+\" mean\"].append(float(value))\n",
    "        if step % log_every == 0:\n",
    "            print(f\"step {step: >4d} loss= {loss:0.6g}\")\n",
    "\n",
    "    if plot:\n",
    "        plt.figure(figsize=(18,12))\n",
    "        for name, Y in series.items():\n",
    "            if name == \"loss\":\n",
    "                plt.plot(Y, \"k--\", label=name, zorder =0)\n",
    "            elif name.endswith(\"mean\"):\n",
    "                plt.plot(Y, label=name, zorder=-1)\n",
    "            else:\n",
    "                plt.plot(Y, label=name, alpha=0.5, lw=1, zorder=-2)\n",
    "        plt.yscale(\"log\")\n",
    "        plt.xscale(\"symlog\")\n",
    "        plt.xlim(0, None)\n",
    "        plt.legend(loc=\"best\", fontsize=8)\n",
    "        plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "guide = AutoNormal(model, init_scale=0.01)\n",
    "fit_svi(model, guide)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Tutorial Code](https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/logistic-growth.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### After each change to the model or inference, you’ll validate model outputs, closing Box’s loop. In our running example we’ll quantitiatively evaluate using the mean average error (MAE) over the last fully-observed time step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### [Paper code](https://github.com/broadinstitute/pyro-cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae(true_counts, pred_probs):\n",
    "    pred_counts = pred_probs * true_counts.sum(-1, True)\n",
    "    error = (true_counts - pred_counts).abs().sum(-1)\n",
    "    total = true_counts.sum(-1).clamp(min=1)\n",
    "    return (error / total).mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, guide, num_particles=100, location=\"USA / Massachusetts\", time=-2):\n",
    "    with torch.no_grad(), poutine.mask(mask=False):\n",
    "        with pyro.plate(\"particle\", num_particles, dim=-3):\n",
    "            guide_trace = poutine.trace(guide).get_trace(data_set)\n",
    "            probs = poutine.replay(model, guide_trace)(data_set, predict=time)\n",
    "        probs = probs.squeeze().mean(0) # average over MC samples\n",
    "        true_counts = data_set['counts'][time]\n",
    "\n",
    "        global_mae = mae(true_counts, probs)\n",
    "        i = data_set['locations'].index(location)\n",
    "        local_mae = mae(true_counts[i], probs[i])\n",
    "    return {\"MAE (global)\":global_mae, f\"MAE ({location})\": local_mae}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model, guide)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We’ll also qualitatively evaluate using a volcano plot showing the effect size and statistical significance of each mutation’s coefficient, and labeling the mutation with the most significant positive effect. We expect: - most mutations have very little effect (they are near zero in log space, so their multiplicative effect is near 1x) - more mutations have positive effect than netagive effect - effect sizes are on the order of 1.1 or 0.9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_volcano(guide, num_particles=100):\n",
    "    with torch.no_grad(), poutine.mask(mask=False):  # makes computations cheaper\n",
    "        with pyro.plate(\"particle\", num_particles, dim=-3):  # vectorizes\n",
    "            trace = poutine.trace(guide).get_trace(data_set)\n",
    "            trace = poutine.trace(poutine.replay(model, trace)).get_trace(data_set, -1) # model(dataset, predict=-1)\n",
    "            coef = trace.nodes[\"coef\"][\"value\"].cpu()\n",
    "    coef = coef.squeeze() * 0.01  # Scale factor as in the model.\n",
    "    mean = coef.mean(0)\n",
    "    std = coef.std(0)\n",
    "    z_score = mean.abs() / std\n",
    "    effect_size = mean.exp().numpy()\n",
    "    plt.figure(figsize=(16, 13))\n",
    "    plt.scatter(effect_size, z_score.numpy(), lw=0, s=5, alpha=0.5, color=\"darkred\")\n",
    "    plt.yscale(\"symlog\")\n",
    "    plt.ylim(0, None)\n",
    "    plt.xlabel(\"$R_m/R_{wt}$\")\n",
    "    plt.ylabel(\"z-score\")\n",
    "    i = int((mean / std).max(0).indices)\n",
    "    plt.text(effect_size[i], z_score[i] * 1.1, data_set[\"mutations\"][i], ha=\"center\", fontsize=8)\n",
    "    plt.title(f\"Volcano plot of {len(mean)} mutations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_volcano(guide)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Create an initialization heuristic](http://pyro.ai/examples/workflow.html#Create-an-initialization-heuristic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In high-dimensional models, convergence can be slow and NANs arise easily, even when sampling from weakly informative priors. We recommend heuristically initializing a point estimate for each latent variable, aiming to initialize at something that is the right order of magnitude. Often you can initialize to a simple statistic of the data, e.g. a mean or standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_loc_fn(site):\n",
    "    shape = site[\"fn\"].shape()\n",
    "    if site[\"name\"] == \"coef\":\n",
    "        return torch.randn(shape).sub_(0.5).mul(0.01)\n",
    "    if site[\"name\"] == \"init\":\n",
    "        return data_set[\"counts\"].mean(0).add(0.01).log()\n",
    "    return init_to_feasible(site)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### As you evolve a model, you’ll add and remove and rename latent variables. We find it useful to require inits for all latent variables, add a message to remind yourself to udpate the init_loc_fn whenever the model changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_loc_fn(site):\n",
    "    shape = site[\"fn\"].shape()\n",
    "    if site[\"name\"].endswith(\"_scale\"):\n",
    "        return torch.ones(shape)\n",
    "    if site[\"name\"] == \"coef\":\n",
    "        return torch.randn(shape).sub_(0.5).mul(0.01)\n",
    "    if site[\"name\"] == \"init\":\n",
    "        return data_set[\"counts\"].mean(0).add(0.01).log()\n",
    "    if site['name'] == \"rate\":\n",
    "        return torch.zeros(shape)\n",
    "    raise NotImplementedError(f\"TODO initialize latent variable {site['name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "guide = AutoNormal(model, init_loc_fn = init_loc_fn, init_scale = 0.01)\n",
    "fit_svi(model, guide, lr=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model, guide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_volcano(guide)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Reparametrize the model](http://pyro.ai/examples/workflow.html#Reparametrize-the-model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reparametrizing a model preserves its distribution while changing its geometry. Reparametrizing is simply a change of coordinates. When reparametrizing we aim to warp a model’s geometry to remove correlations and to lift inconvenient topological manifolds into simpler higher dimensional flat Euclidean space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Whereas many probabilistic programming languages require users to rewrite models to change coordinates, Pyro implements a library of about 15 different reparametrization effects including decentering (Gorinova et al. 2020), Haar wavelet transforms, and neural transport (Hoffman et al. 2019), as well as strategies to automatically apply effects and machinery to create custom reparametrization effects. Using these reparametrizers you can separate modeling from inference: first specify a model in a form that is natural to domain experts, then in inference code, reparametrize the model to have geometry that is more amenable to variational inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "reparam_model = poutine.reparam(\n",
    "    model,\n",
    "    {\n",
    "        \"rate\": LocScaleReparam(),\n",
    "        \"init\": LocScaleReparam()\n",
    "    }\n",
    ")\n",
    "\n",
    "guide = AutoNormal(reparam_model, init_loc_fn=init_loc_fn, init_scale=0.01)\n",
    "fit_svi(reparam_model, guide, lr=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model, guide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_volcano(guide)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Customize the variational family](http://pyro.ai/examples/workflow.html#Customize-the-variational-family)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### When creating a new model, we recommend starting with mean field variational inference using an `AutoNormal <>`__ guide. This mean field guide is good at finding the neighborhood of your model’s mode, but naively it ignores correlations between latent variables. A first step in capturing correlations is to reparametrize the model as above: using a LocScaleReparam or HaarReparam (where appropriate) already allows the guide to capture some correlations among latent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Start with an `AutoNormal <>`__ guide. \n",
    "2. Try `AutoLowRankMultivariateNormal <>`, which can model the principle components of correlated uncertainty. (For models with only ~100 latent variables you might also try `AutoMultivariateNormal <>`__ or `AutoGaussian <>`__). \n",
    "3. Try combining multiple guides using `AutoGuideList <>`. For example if `AutoLowRankMultivariateNormal <>`__ is too expensive for all the latent variables, you can use `AutoGuideList <>`__ to combine an `AutoLowRankMultivariateNormal <>`__ guide over a few top-level global latent variables, together with a cheaper `AutoNormal <>`__ guide over more numerous local latent variables. \n",
    "4. Try using `AutoGuideList <>`__ to combine a autoguide together with a custom guide function built using pyro.sample, pyro.param, and pyro.plate. Given a partial_guide() function that covers just a few latent variables, you can AutoGuideList.append(partial_guide) just as you append autoguides. \n",
    "5. Consider customizing one of Pyro’s autoguides that leverage model structure, e.g. AutoStructured, AutoNormalMessenger, AutoHierarchicalNormalMessenger AutoRegressiveMessenger. \n",
    "6. For models with local correlations, consider building on EasyGuide, a framework for building guides over groups of variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The author recommends avoiding completely low-level guides and instead using AutoGuide or EasyGuide for at least some parts of the model, thereby speeding up model iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "reparam_model = poutine.reparam(\n",
    "    model,\n",
    "    {\n",
    "        \"rate\": LocScaleReparam(),\n",
    "        \"init\": LocScaleReparam()\n",
    "    }\n",
    ")\n",
    "\n",
    "guide = AutoLowRankMultivariateNormal(\n",
    "    reparam_model, init_loc_fn=init_loc_fn, init_scale = 0.01, rank = 100\n",
    ")\n",
    "\n",
    "fit_svi(reparam_model, guide, num_steps=100, log_every=10, plot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "reparam_model = poutine.reparam(\n",
    "    model,\n",
    "    {\n",
    "        \"rate\": LocScaleReparam(),\n",
    "        \"init\": LocScaleReparam()\n",
    "    }\n",
    ")\n",
    "\n",
    "guide = AutoGuideList(reparam_model)\n",
    "mvn_vars = [\"coef\", \"rate_scale\", \"coef_scale\"]\n",
    "guide.add(\n",
    "    AutoLowRankMultivariateNormal(\n",
    "        poutine.block(reparam_model, expose= mvn_vars),\n",
    "        init_loc_fn=init_loc_fn,\n",
    "        init_scale = 0.01\n",
    "    )\n",
    ")\n",
    "\n",
    "guide.add(\n",
    "    AutoNormal(\n",
    "        poutine.block(reparam_model, hide=mvn_vars),\n",
    "        init_loc_fn=init_loc_fn,\n",
    "        init_scale = 0.01\n",
    "    )\n",
    ")\n",
    "\n",
    "fit_svi(reparam_model, guide, lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(reparam_model, guide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_volcano(guide)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Next let’s create a custom guide for part of the model, just the rate and init parts. Since we’ll want to use this with reparametrizers, we’ll make the guide use the auxiliary latent variables created by poutine.reparam, rather than the original rate and init variables. Let’s see what these variables are named:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rate_scale\n",
      "init_scale\n",
      "mutation\n",
      "coef\n",
      "strain\n",
      "place\n",
      "rate_decentered\n",
      "init_decentered\n"
     ]
    }
   ],
   "source": [
    "for name, site in poutine.trace(reparam_model).get_trace(data_set).iter_stochastic_nodes():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### It looks like these new auxiliary variables are called rate_decentered and init_decentered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
