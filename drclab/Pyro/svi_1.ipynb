{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Conditional Independence, Subsampling, and Amortization](http://pyro.ai/examples/svi_part_ii.html#SVI-Part-II:-Conditional-Independence,-Subsampling,-and-Amortization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The model has observations x and latent random variables z as well as parameters $\\theta$. It has a joint probability density of the form: $$p_{\\theta}({\\bf x}, {\\bf z}) = p_{\\theta}({\\bf x}|{\\bf z}) p_{\\theta}({\\bf z})$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\log p_{\\theta}({\\bf x}) = \\log \\int\\! d{\\bf z}\\; p_{\\theta}({\\bf x}, {\\bf z})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\theta_{\\rm{max}} = \\underset{\\theta}{\\operatorname{argmax}} \\log p_{\\theta}({\\bf x})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p_{\\theta_{\\rm{max}}}({\\bf z} | {\\bf x}) = \\frac{p_{\\theta_{\\rm{max}}}({\\bf x} , {\\bf z})}{\n",
    "\\int \\! d{\\bf z}\\; p_{\\theta_{\\rm{max}}}({\\bf x} , {\\bf z}) }$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic idea is that we introduce a parameterized distribution $q_{\\phi}({\\bf z})$, where  are known as the variational parameters. This distribution is called the variational distribution in much of the literature, and in the context of Pyro it’s called the **guide** (one syllable instead of nine!). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pyro enforces that model() and guide() have the same call signature, i.e. both callables should take the same arguments**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning will be setup as an optimization problem where each iteration of training takes a step in $\\theta-\\phi$ space that moves the guide closer to the exact posterior. To do this we need to define an appropriate objective function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **ELBO**, which is a function of both $\\theta$ and $\\phi$, is defined as an expectation w.r.t. to samples from the guide:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$${\\rm ELBO} \\equiv \\mathbb{E}_{q_{\\phi}({\\bf z})} \\left [\n",
    "\\log p_{\\theta}({\\bf x}, {\\bf z}) - \\log q_{\\phi}({\\bf z})\n",
    "\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\log p_{\\theta}({\\bf x}) - {\\rm ELBO} =\n",
    "\\rm{KL}\\!\\left( q_{\\phi}({\\bf z}) \\lVert p_{\\theta}({\\bf z} | {\\bf x}) \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\sum_{i=1}^N \\log p({\\bf x}_i | {\\bf z}) \\approx  \\frac{N}{M}\n",
    "\\sum_{i\\in{\\mathcal{I}_M}} \\log p({\\bf x}_i | {\\bf z})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a user wants to do this sort of thing in Pyro, he or she first needs to make sure that the model and guide are written in such a way that Pyro can leverage the relevant conditional independencies. Let’s see how this is done. Pyro provides two language primitives for marking conditional independencies: plate and markov. Let’s start with the simpler of the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pyro\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro.distributions as dist\n",
    "import torch.distributions.constraints as constraints\n",
    "from pyro.distributions.testing.fakes import NonreparameterizedBeta\n",
    "from pyro.infer import SVI, Trace_ELBO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert pyro.__version__.startswith('1.8.2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_abs_error(name, target):\n",
    "    return torch.sum(torch.abs(target - pyro.param(name))).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.optim import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This simply tells Pyro to use a learning rate of 0.010 for the Pyro parameter my_special_parameter and a learning rate of 0.001 for all other parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro.distributions as dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this model the observations are conditionally independent given the latent random variable latent_fairness. To explicitly mark this in Pyro we basically just need to replace the Python builtin range with the Pyro construct plate:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, pyro.plate is **not appropriate for temporal models** where each iteration of a loop depends on the previous iteration; in this case a range or **pyro.markov** should be used instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variational parameters are torch.tensors. The **requires_grad flag is automatically set to True by pyro.param**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p({\\bf x}, {\\bf z}, \\beta) = p(\\beta)\n",
    "\\prod_{i=1}^N p({\\bf x}_i | {\\bf z}_i) p({\\bf z}_i | \\beta)$$\n",
    "\n",
    "$$q({\\bf z}, \\beta) = q(\\beta) \\prod_{i=1}^N q({\\bf z}_i | \\beta, \\lambda_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " where subsampling appears in both the model and guide. To make things simple let’s keep the discussion somewhat abstract and avoid writing a complete model and guide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amortization $$q(\\beta) \\prod_{n=1}^N q({\\bf z}_i | f({\\bf x}_i))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ELBO Gradient Estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$${\\rm ELBO} \\equiv \\mathbb{E}_{q_{\\phi}({\\bf z})} \\left [\n",
    "\\log p_{\\theta}({\\bf x}, {\\bf z}) - \\log q_{\\phi}({\\bf z})\n",
    "\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reparameterizable Random Variables $\\mathbb{E}_{q_{\\phi}({\\bf z})} \\left [f_{\\phi}({\\bf z}) \\right]=\\mathbb{E}_{q({\\bf \\epsilon})} \\left [f_{\\phi}(g_{\\phi}({\\bf \\epsilon})) \\right]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\nabla_{\\phi}\\mathbb{E}_{q({\\bf \\epsilon})} \\left [f_{\\phi}(g_{\\phi}({\\bf \\epsilon})) \\right]=\n",
    "\\mathbb{E}_{q({\\bf \\epsilon})} \\left [\\nabla_{\\phi}f_{\\phi}(g_{\\phi}({\\bf \\epsilon})) \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-reparameterizable Random Variables¶\n",
    "What if we can’t do the above reparameterization? Unfortunately this is the case for many distributions of interest, for example all discrete distributions. In this case our estimator takes a bit more complicated form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\nabla_{\\phi}\\mathbb{E}_{q_{\\phi}({\\bf z})} \\left [\n",
    "f_{\\phi}({\\bf z}) \\right]=\n",
    "\\nabla_{\\phi} \\int d{\\bf z} \\; q_{\\phi}({\\bf z}) f_{\\phi}({\\bf z})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\int d{\\bf z} \\; \\left \\{ (\\nabla_{\\phi}  q_{\\phi}({\\bf z})) f_{\\phi}({\\bf z}) + q_{\\phi}({\\bf z})(\\nabla_{\\phi} f_{\\phi}({\\bf z}))\\right \\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\nabla_{\\phi}  q_{\\phi}({\\bf z}) =\n",
    "q_{\\phi}({\\bf z})\\nabla_{\\phi} \\log q_{\\phi}({\\bf z})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mathbb{E}_{q_{\\phi}({\\bf z})} \\left [\n",
    "(\\nabla_{\\phi} \\log q_{\\phi}({\\bf z})) f_{\\phi}({\\bf z}) + \\nabla_{\\phi} f_{\\phi}({\\bf z})\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$${\\rm surrogate \\;objective} \\equiv\n",
    "\\log q_{\\phi}({\\bf z}) \\overline{f_{\\phi}({\\bf z})} + f_{\\phi}({\\bf z})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\nabla_{\\phi} {\\rm ELBO} = \\mathbb{E}_{q_{\\phi}({\\bf z})} \\left [\n",
    "\\nabla_{\\phi} ({\\rm surrogate \\; objective}) \\right]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BernoulliBeta:\n",
    "    def __init__(self, max_steps):\n",
    "        self.max_steps = max_steps\n",
    "        self.alpha0 = 10.0\n",
    "        self.beta0 = 10.0\n",
    "        self.data = torch.zeros(10)\n",
    "        self.data[0:6] = torch.ones(6)\n",
    "        self.n_data = self.data.size(0)\n",
    "\n",
    "        self.alpha_n = self.data.sum() + self.alpha0\n",
    "        self.beta_n = -self.data.sum() + torch.tensor(self.beta0 + self.n_data)\n",
    "\n",
    "        self.alpha_q_0 = 15.0\n",
    "        self.beta_q_0 = 15.0\n",
    "    def model(self, use_decaying_avg_baseline):\n",
    "        f = pyro.sample(\"latent_fairness\", dist.Beta(self.alpha0, self.beta0))\n",
    "        with pyro.plate(\"data_plate\"):\n",
    "            pyro.sample(\"obs\", dist.Bernoulli(f), obs=self.data)\n",
    "\n",
    "    def guide(self, use_decaying_avg_baseline):\n",
    "        alpha_q = pyro.param(\"alpha_q\", torch.tensor(self.alpha_q_0), constraint=constraints.positive)\n",
    "        beta_q = pyro.param(\"beta_q\", torch.tensor(self.beta_q_0), constraint=constraints.positive)\n",
    "        baseline_dict = {\n",
    "            'use_decaying_avg_baseline': use_decaying_avg_baseline,\n",
    "            'baseline_beta': 0.90\n",
    "        }\n",
    "\n",
    "        pyro.sample(\"latent_fairness\", NonreparameterizedBeta(alpha_q, beta_q), infer=dict(baseline=baseline_dict))\n",
    "\n",
    "    def do_inference(self, use_decaying_avg_baseline, tolorance=0.8):\n",
    "        pyro.clear_param_store()\n",
    "        optimizer = Adam({'lr': 0.0005, 'betas': (0.93, 0.999)})\n",
    "        svi = SVI(self.model, self.guide, optimizer, loss=Trace_ELBO())\n",
    "\n",
    "        print(\"Doing inference with use_decaying_avg_baseline=%s\" % use_decaying_avg_baseline )\n",
    "\n",
    "        for k in range(self.max_steps):\n",
    "            svi.step(use_decaying_avg_baseline)\n",
    "            if k % 100 == 0:\n",
    "                print('.', end=\"\")\n",
    "                sys.stdout.flush()\n",
    "            alpha_error = param_abs_error(\"alpha_q\", self.alpha_n)\n",
    "            beta_error = param_abs_error(\"beta_q\", self.beta_n)\n",
    "\n",
    "            if alpha_error < tolorance and beta_error < tolorance:\n",
    "                break\n",
    "        \n",
    "        print(\"\\nDid %d steps of inference.\" % k)\n",
    "        print(\"Final absolute erros for the two variational parameters \" + \n",
    "            \"are %.4f & %.4f\" % (alpha_error, beta_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbe = BernoulliBeta(max_steps=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing inference with use_decaying_avg_baseline=True\n",
      "..\n",
      "Did 146 steps of inference.\n",
      "Final absolute erros for the two variational parameters are 0.7997 & 0.7949\n"
     ]
    }
   ],
   "source": [
    "bbe.do_inference(use_decaying_avg_baseline=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing inference with use_decaying_avg_baseline=False\n",
      "....\n",
      "Did 381 steps of inference.\n",
      "Final absolute erros for the two variational parameters are 0.7987 & 0.7978\n"
     ]
    }
   ],
   "source": [
    "bbe.do_inference(use_decaying_avg_baseline=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
