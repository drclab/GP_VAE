{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Conditional Independence, Subsampling, and Amortization](http://pyro.ai/examples/svi_part_ii.html#SVI-Part-II:-Conditional-Independence,-Subsampling,-and-Amortization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The model has observations x and latent random variables z as well as parameters $\\theta$. It has a joint probability density of the form: $$p_{\\theta}({\\bf x}, {\\bf z}) = p_{\\theta}({\\bf x}|{\\bf z}) p_{\\theta}({\\bf z})$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\log p_{\\theta}({\\bf x}) = \\log \\int\\! d{\\bf z}\\; p_{\\theta}({\\bf x}, {\\bf z})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\theta_{\\rm{max}} = \\underset{\\theta}{\\operatorname{argmax}} \\log p_{\\theta}({\\bf x})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p_{\\theta_{\\rm{max}}}({\\bf z} | {\\bf x}) = \\frac{p_{\\theta_{\\rm{max}}}({\\bf x} , {\\bf z})}{\n",
    "\\int \\! d{\\bf z}\\; p_{\\theta_{\\rm{max}}}({\\bf x} , {\\bf z}) }$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic idea is that we introduce a parameterized distribution $q_{\\phi}({\\bf z})$, where  are known as the variational parameters. This distribution is called the variational distribution in much of the literature, and in the context of Pyro it’s called the **guide** (one syllable instead of nine!). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pyro enforces that model() and guide() have the same call signature, i.e. both callables should take the same arguments**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning will be setup as an optimization problem where each iteration of training takes a step in $\\theta-\\phi$ space that moves the guide closer to the exact posterior. To do this we need to define an appropriate objective function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **ELBO**, which is a function of both $\\theta$ and $\\phi$, is defined as an expectation w.r.t. to samples from the guide:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$${\\rm ELBO} \\equiv \\mathbb{E}_{q_{\\phi}({\\bf z})} \\left [\n",
    "\\log p_{\\theta}({\\bf x}, {\\bf z}) - \\log q_{\\phi}({\\bf z})\n",
    "\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\log p_{\\theta}({\\bf x}) - {\\rm ELBO} =\n",
    "\\rm{KL}\\!\\left( q_{\\phi}({\\bf z}) \\lVert p_{\\theta}({\\bf z} | {\\bf x}) \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\sum_{i=1}^N \\log p({\\bf x}_i | {\\bf z}) \\approx  \\frac{N}{M}\n",
    "\\sum_{i\\in{\\mathcal{I}_M}} \\log p({\\bf x}_i | {\\bf z})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a user wants to do this sort of thing in Pyro, he or she first needs to make sure that the model and guide are written in such a way that Pyro can leverage the relevant conditional independencies. Let’s see how this is done. Pyro provides two language primitives for marking conditional independencies: plate and markov. Let’s start with the simpler of the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pyro\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro.distributions as dist\n",
    "import torch.distributions.constraints as constraints\n",
    "from pyro.distributions.testing.fakes import NonreparameterizedBeta\n",
    "from pyro.infer import SVI, TraceGraph_ELBO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert pyro.__version__.startswith('1.8.2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This simply tells Pyro to use a learning rate of 0.010 for the Pyro parameter my_special_parameter and a learning rate of 0.001 for all other parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this model the observations are conditionally independent given the latent random variable latent_fairness. To explicitly mark this in Pyro we basically just need to replace the Python builtin range with the Pyro construct plate:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, pyro.plate is **not appropriate for temporal models** where each iteration of a loop depends on the previous iteration; in this case a range or **pyro.markov** should be used instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variational parameters are torch.tensors. The **requires_grad flag is automatically set to True by pyro.param**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p({\\bf x}, {\\bf z}, \\beta) = p(\\beta)\n",
    "\\prod_{i=1}^N p({\\bf x}_i | {\\bf z}_i) p({\\bf z}_i | \\beta)$$\n",
    "\n",
    "$$q({\\bf z}, \\beta) = q(\\beta) \\prod_{i=1}^N q({\\bf z}_i | \\beta, \\lambda_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " where subsampling appears in both the model and guide. To make things simple let’s keep the discussion somewhat abstract and avoid writing a complete model and guide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amortization $$q(\\beta) \\prod_{n=1}^N q({\\bf z}_i | f({\\bf x}_i))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [ELBO Gradient Estimators](http://pyro.ai/examples/svi_part_iii.html#SVI-Part-III:-ELBO-Gradient-Estimators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$${\\rm ELBO} \\equiv \\mathbb{E}_{q_{\\phi}({\\bf z})} \\left [\n",
    "\\log p_{\\theta}({\\bf x}, {\\bf z}) - \\log q_{\\phi}({\\bf z})\n",
    "\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reparameterizable Random Variables $\\mathbb{E}_{q_{\\phi}({\\bf z})} \\left [f_{\\phi}({\\bf z}) \\right]=\\mathbb{E}_{q({\\bf \\epsilon})} \\left [f_{\\phi}(g_{\\phi}({\\bf \\epsilon})) \\right]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\nabla_{\\phi}\\mathbb{E}_{q({\\bf \\epsilon})} \\left [f_{\\phi}(g_{\\phi}({\\bf \\epsilon})) \\right]=\n",
    "\\mathbb{E}_{q({\\bf \\epsilon})} \\left [\\nabla_{\\phi}f_{\\phi}(g_{\\phi}({\\bf \\epsilon})) \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-reparameterizable Random Variables¶\n",
    "What if we can’t do the above reparameterization? Unfortunately this is the case for many distributions of interest, for example all discrete distributions. In this case our estimator takes a bit more complicated form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\nabla_{\\phi}\\mathbb{E}_{q_{\\phi}({\\bf z})} \\left [\n",
    "f_{\\phi}({\\bf z}) \\right]=\n",
    "\\nabla_{\\phi} \\int d{\\bf z} \\; q_{\\phi}({\\bf z}) f_{\\phi}({\\bf z})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\int d{\\bf z} \\; \\left \\{ (\\nabla_{\\phi}  q_{\\phi}({\\bf z})) f_{\\phi}({\\bf z}) + q_{\\phi}({\\bf z})(\\nabla_{\\phi} f_{\\phi}({\\bf z}))\\right \\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\nabla_{\\phi}  q_{\\phi}({\\bf z}) =\n",
    "q_{\\phi}({\\bf z})\\nabla_{\\phi} \\log q_{\\phi}({\\bf z})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mathbb{E}_{q_{\\phi}({\\bf z})} \\left [\n",
    "(\\nabla_{\\phi} \\log q_{\\phi}({\\bf z})) f_{\\phi}({\\bf z}) + \\nabla_{\\phi} f_{\\phi}({\\bf z})\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$${\\rm surrogate \\;objective} \\equiv\n",
    "\\log q_{\\phi}({\\bf z}) \\overline{f_{\\phi}({\\bf z})} + f_{\\phi}({\\bf z})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\nabla_{\\phi} {\\rm ELBO} = \\mathbb{E}_{q_{\\phi}({\\bf z})} \\left [\n",
    "\\nabla_{\\phi} ({\\rm surrogate \\; objective}) \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\log p_{\\theta}({\\bf x} | {\\rm Pa}_p ({\\bf x})) +\n",
    "\\sum_i \\log p_{\\theta}({\\bf z}_i | {\\rm Pa}_p ({\\bf z}_i))\n",
    "- \\sum_i \\log q_{\\phi}({\\bf z}_i | {\\rm Pa}_q ({\\bf z}_i))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Pyro, all of this logic is taken care of automatically by the SVI class. In particular as long as we use a TraceGraph_ELBO loss, Pyro will keep track of the dependency structure within the execution traces of the model and guide and construct a surrogate objective that has all the unnecessary terms removed:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " A **gaussian mixture model with  K components**. For each data point we:  \n",
    " 1. first sample the component distribution $k \\in [1,...,K]$ ; \n",
    " 2. observe the data point using the  component distribution. The simplest way to write down a model of this sort is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model():\n",
    "    with pyro.plate(\"foo\", data.size(-1)):\n",
    "        ks = pyro.sample(\"k\", dist.Categorical(probs).to_event(1))\n",
    "        pyro.sample(\"obs\", dist.Normal(locs[ks], scale).to_event(1), obs=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducing Variance with Data-Dependent Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " instead of removing terms with zero expectation that tend to contribute to the variance, we’re going to add specially chosen terms with zero expectation that work to reduce the variance. As such, this is a control variate strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mathbb{E}_{q_{\\phi}({\\bf z})} \\left [\\nabla_{\\phi}\n",
    "\\log q_{\\phi}({\\bf z}) \\right]=\n",
    " \\int \\!d{\\bf z} \\; q_{\\phi}({\\bf z}) \\nabla_{\\phi}\n",
    "\\log q_{\\phi}({\\bf z})=\n",
    " \\int \\! d{\\bf z} \\; \\nabla_{\\phi} q_{\\phi}({\\bf z})=\n",
    "\\nabla_{\\phi} \\int \\! d{\\bf z} \\;  q_{\\phi}({\\bf z})=\\nabla_{\\phi} 1 = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\log q_{\\phi}({\\bf z}_i) \\overline{f_{\\phi}({\\bf z})} \\rightarrow  \\log q_{\\phi}({\\bf z}_i) \\left(\\overline{f_{\\phi}({\\bf z})}-b\\right)$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing so doesn’t affect the mean of our gradient estimator but it does affect the variance. If we choose  wisely, we can hope to reduce the variance. In fact,  need not be a constant: it can depend on any of the random choices upstream (or sidestream) of $z_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_abs_error(name, target):\n",
    "    return torch.sum(torch.abs(target - pyro.param(name))).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = pyro.sample(\"z\", dist.Bernoulli(...),\n",
    "                infer=dict(baseline={'use_decaying_avg_baseline': True,\n",
    "                                     'baseline_beta': 0.95}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BernoulliBeta:\n",
    "    def __init__(self, max_steps):\n",
    "        self.max_steps = max_steps\n",
    "        self.alpha0 = 10.0\n",
    "        self.beta0 = 10.0\n",
    "        self.data = torch.zeros(10)\n",
    "        self.data[0:6] = torch.ones(6)\n",
    "        self.n_data = self.data.size(0)\n",
    "\n",
    "        self.alpha_n = self.data.sum() + self.alpha0\n",
    "        self.beta_n = -self.data.sum() + torch.tensor(self.beta0 + self.n_data)\n",
    "\n",
    "        self.alpha_q_0 = 15.0\n",
    "        self.beta_q_0 = 15.0\n",
    "    def model(self, use_decaying_avg_baseline):\n",
    "        f = pyro.sample(\"latent_fairness\", dist.Beta(self.alpha0, self.beta0))\n",
    "        with pyro.plate(\"data_plate\"):\n",
    "            pyro.sample(\"obs\", dist.Bernoulli(f), obs=self.data)\n",
    "\n",
    "    def guide(self, use_decaying_avg_baseline):\n",
    "        alpha_q = pyro.param(\"alpha_q\", torch.tensor(self.alpha_q_0), constraint=constraints.positive)\n",
    "        beta_q = pyro.param(\"beta_q\", torch.tensor(self.beta_q_0), constraint=constraints.positive)\n",
    "        baseline_dict = {\n",
    "            'use_decaying_avg_baseline': use_decaying_avg_baseline,\n",
    "            'baseline_beta': 0.90\n",
    "        }\n",
    "\n",
    "        pyro.sample(\"latent_fairness\", NonreparameterizedBeta(alpha_q, beta_q), infer=dict(baseline=baseline_dict))\n",
    "\n",
    "    def do_inference(self, use_decaying_avg_baseline, tolorance=0.8):\n",
    "        pyro.clear_param_store()\n",
    "        optimizer = Adam({'lr': 0.0005, 'betas': (0.93, 0.999)})\n",
    "        svi = SVI(self.model, self.guide, optimizer, loss=TraceGraph_ELBO())\n",
    "\n",
    "        print(\"Doing inference with use_decaying_avg_baseline=%s\" % use_decaying_avg_baseline )\n",
    "\n",
    "        for k in range(self.max_steps):\n",
    "            svi.step(use_decaying_avg_baseline)\n",
    "            if k % 100 == 0:\n",
    "                print('.', end=\"\")\n",
    "                sys.stdout.flush()\n",
    "            alpha_error = param_abs_error(\"alpha_q\", self.alpha_n)\n",
    "            beta_error = param_abs_error(\"beta_q\", self.beta_n)\n",
    "\n",
    "            if alpha_error < tolorance and beta_error < tolorance:\n",
    "                break\n",
    "        \n",
    "        print(\"\\nDid %d steps of inference.\" % k)\n",
    "        print(\"Final absolute erros for the two variational parameters \" + \n",
    "            \"are %.4f & %.4f\" % (alpha_error, beta_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbe = BernoulliBeta(max_steps=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing inference with use_decaying_avg_baseline=True\n",
      "..\n",
      "Did 138 steps of inference.\n",
      "Final absolute erros for the two variational parameters are 0.7896 & 0.7993\n"
     ]
    }
   ],
   "source": [
    "bbe.do_inference(use_decaying_avg_baseline=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing inference with use_decaying_avg_baseline=False\n",
      ".......\n",
      "Did 634 steps of inference.\n",
      "Final absolute erros for the two variational parameters are 0.7765 & 0.7993\n"
     ]
    }
   ],
   "source": [
    "bbe.do_inference(use_decaying_avg_baseline=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
