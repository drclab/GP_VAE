{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [VAE in Pyro](http://pyro.ai/examples/vae.html#VAE-in-Pyro)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s see how we implement a VAE in Pyro. The dataset we’re going to model is MNIST, a collection of images of handwritten digits. Since this is a popular benchmark dataset, we can make use of PyTorch’s convenient data loader functionalities to reduce the amount of boilerplate code we need to write:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "#from torchvision.datasets import MNIST\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.contrib.examples.util import MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_path = os.path.join(os.environ['HOME']+'/mnist/pyro/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(pixel_path):\n",
    "    os.mkdir(pixel_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = T.ToTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = MNIST(root=pixel_path, train=True, transform=trans, download=False)\n",
    "test_set = MNIST(root=pixel_path, train=False, transform=trans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main thing to draw attention to here is that we use transforms.ToTensor() to normalize the pixel intensities to the range [0, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = False\n",
    "kwargs = {'num_workers': 1, 'pin_memory': use_cuda}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(dataset=train_set, batch_size=128, shuffle=True)\n",
    "test_dl = DataLoader(dataset=test_set, batch_size=128, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "x11 = train_set[11][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x11.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.1216, 0.1569, 0.5059, 0.9176, 0.9176, 0.6235, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2667,\n",
       "        0.5882, 0.9373, 0.9961, 0.9922, 0.9922, 0.9922, 0.8431, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6118, 0.7882,\n",
       "        0.9961, 0.9961, 0.9961, 0.9451, 0.5882, 0.3843, 0.0314, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0745, 0.6039, 0.9961,\n",
       "        0.9255, 0.7961, 0.3255, 0.1529, 0.1176, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5647, 0.9922,\n",
       "        0.5686, 0.0471, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0392, 0.5059, 0.8706, 0.3059,\n",
       "        0.3098, 0.0314, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5255, 0.9922, 0.6549,\n",
       "        0.0314, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.9961,\n",
       "        0.3059, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7882,\n",
       "        0.9922, 0.8863, 0.2706, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2157, 0.0235, 0.0000,\n",
       "        0.0706, 0.5020, 0.9922, 0.9451, 0.1608, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0980, 0.8039, 0.9216, 0.3608,\n",
       "        0.0000, 0.0000, 0.0784, 0.9922, 0.9922, 0.2275, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9059, 0.9608, 0.4235,\n",
       "        0.0000, 0.0000, 0.0000, 0.5176, 0.9922, 0.7255, 0.0549, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4745, 0.9608,\n",
       "        0.9961, 0.9961, 0.9961, 0.8510, 0.9961, 0.8745, 0.1961, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.4549, 0.6471, 0.9137, 0.9137, 0.9176, 0.7059, 0.1529, 0.0118, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x11.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f2461776dd0>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGdCAYAAABU0qcqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/av/WaAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAa9ElEQVR4nO3df3DU9b3v8ddCwgKaLA0h2awEDPgDFUhvEdIMSrFkSOIdDyD3HPzRGXAYGGnwFlN/XDoq2vbetHgudfRSOHemJXVGUDkjMDKnzNFgwrVNcIhwKVPNIWksOCShMmU3BAmBfO4fXLeuJNDvupt3sjwfM98Zsvv95Pv263d8+mU3G59zzgkAgAE2zHoAAMC1iQABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATadYDfFVvb69OnDihjIwM+Xw+63EAAB4559TZ2alQKKRhw/q/zxl0ATpx4oTy8/OtxwAAfE3Hjx/X+PHj+31+0AUoIyNDknSX7lWa0o2nAQB4dUE9el//Fv3veX+SFqCNGzfqxRdfVHt7uwoLC/XKK69o1qxZV133xV+7pSldaT4CBABDzv//hNGrvYySlDchvPHGG6qsrNS6dev04YcfqrCwUKWlpTp58mQyDgcAGIKSEqANGzZoxYoVeuSRR3T77bdr8+bNGj16tH79618n43AAgCEo4QE6f/68GhsbVVJS8reDDBumkpIS1dfXX7Z/d3e3IpFIzAYASH0JD9Bnn32mixcvKjc3N+bx3Nxctbe3X7Z/VVWVAoFAdOMdcABwbTD/QdS1a9cqHA5Ht+PHj1uPBAAYAAl/F1x2draGDx+ujo6OmMc7OjoUDAYv29/v98vv9yd6DADAIJfwO6ARI0ZoxowZqqmpiT7W29urmpoaFRcXJ/pwAIAhKik/B1RZWamlS5fqzjvv1KxZs/TSSy+pq6tLjzzySDIOBwAYgpISoCVLlugvf/mLnnvuObW3t+ub3/ym9uzZc9kbEwAA1y6fc85ZD/FlkUhEgUBAc7WAT0IAgCHogutRrXYpHA4rMzOz3/3M3wUHALg2ESAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwkP0PPPPy+fzxezTZkyJdGHAQAMcWnJ+KZ33HGH3n333b8dJC0phwEADGFJKUNaWpqCwWAyvjUAIEUk5TWgo0ePKhQKadKkSXr44Yd17Nixfvft7u5WJBKJ2QAAqS/hASoqKlJ1dbX27NmjTZs2qbW1VXfffbc6Ozv73L+qqkqBQCC65efnJ3okAMAg5HPOuWQe4PTp05o4caI2bNig5cuXX/Z8d3e3uru7o19HIhHl5+drrhYozZeezNEAAElwwfWoVrsUDoeVmZnZ735Jf3fAmDFjdMstt6i5ubnP5/1+v/x+f7LHAAAMMkn/OaAzZ86opaVFeXl5yT4UAGAISXiAnnjiCdXV1emTTz7R73//ey1atEjDhw/Xgw8+mOhDAQCGsIT/Fdynn36qBx98UKdOndK4ceN01113qaGhQePGjUv0oQAAQ1jCA/T6668n+lsCAFIQnwUHADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJhI+i+kA4BEGn77LZ7X9F4X3y+9PPrwdZ7XbFvwSlzH8mpZ4yNxrcv/L0cSPEn8uAMCAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACT4NG0BCnPnHIs9r2hec97xm910bPa+5JX2k5zWS1CsXx6qB+f/6/3r7e3Gt26FxCZ4kftwBAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAm+DBSIIV98sb0uNb9w81/8LzmZ7mb4jqWd94/WPSTC2fjOtL8//OY5zXXHRzlec0Nm/+v5zW9XV2e1ww23AEBAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACb4MFLAQNoNIc9rjv7zOM9rPrpri+c1kvSH8z2e1zx7cqbnNf++cbbnNdmHOj2vGdbV7XmNJN300cG41nnVOyBHGXy4AwIAmCBAAAATngO0b98+3XfffQqFQvL5fNq5c2fM8845Pffcc8rLy9OoUaNUUlKio0ePJmpeAECK8Bygrq4uFRYWauPGjX0+v379er388svavHmz9u/fr+uuu06lpaU6d+7c1x4WAJA6PL8Joby8XOXl5X0+55zTSy+9pGeeeUYLFiyQJL366qvKzc3Vzp079cADD3y9aQEAKSOhrwG1traqvb1dJSUl0ccCgYCKiopUX1/f55ru7m5FIpGYDQCQ+hIaoPb2dklSbm5uzOO5ubnR576qqqpKgUAguuXn5ydyJADAIGX+Lri1a9cqHA5Ht+PHj1uPBAAYAAkNUDAYlCR1dHTEPN7R0RF97qv8fr8yMzNjNgBA6ktogAoKChQMBlVTUxN9LBKJaP/+/SouLk7koQAAQ5znd8GdOXNGzc3N0a9bW1t16NAhZWVlacKECVqzZo1++tOf6uabb1ZBQYGeffZZhUIhLVy4MJFzAwCGOM8BOnDggO65557o15WVlZKkpUuXqrq6Wk899ZS6urq0cuVKnT59WnfddZf27NmjkSNHJm5qAMCQ53POOeshviwSiSgQCGiuFijNl249DpAU//HrO72vKf0Xz2tu+feVntdI0m2Vf/K85uJf/xrXsZB6Lrge1WqXwuHwFV/XN38XHADg2kSAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATnn8dAzAUDI/zN+s2/fh2z2uq7t3mec0//3fvv6Bx9r7VntdM2X7Y8xpJutjVFdc6wAvugAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAE3wYKVLSx1W3xbWuaeFGz2u+/eGDntfk/Kv3DwntjeMDQns9rwAGDndAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJPowUKelPi/4lrnUXnc/zmuH/Otbzmt6u//C8Bkg13AEBAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACb4MFKkpCfb/1Nc6/5H7gHPa9Y9u8X7cT5f5nnN9W82eF4DDGbcAQEATBAgAIAJzwHat2+f7rvvPoVCIfl8Pu3cuTPm+WXLlsnn88VsZWVliZoXAJAiPAeoq6tLhYWF2rhxY7/7lJWVqa2tLbpt27btaw0JAEg9nt+EUF5ervLy8ivu4/f7FQwG4x4KAJD6kvIaUG1trXJycnTrrbdq1apVOnXqVL/7dnd3KxKJxGwAgNSX8ACVlZXp1VdfVU1NjX7+85+rrq5O5eXlunjxYp/7V1VVKRAIRLf8/PxEjwQAGIQS/nNADzzwQPTP06ZN0/Tp0zV58mTV1tZq3rx5l+2/du1aVVZWRr+ORCJECACuAUl/G/akSZOUnZ2t5ubmPp/3+/3KzMyM2QAAqS/pAfr000916tQp5eXlJftQAIAhxPNfwZ05cybmbqa1tVWHDh1SVlaWsrKy9MILL2jx4sUKBoNqaWnRU089pZtuukmlpaUJHRwAMLR5DtCBAwd0zz33RL/+4vWbpUuXatOmTTp8+LB+85vf6PTp0wqFQpo/f75+8pOfyO/3J25qAMCQ53POOeshviwSiSgQCGiuFijNl249Dq7gfOmdnteMrDvieU3vuXOe16TlxfdzaB8/daP3Nf/U/w9l9+fYhc89r/n+P67yvEYf/MH7GuBruuB6VKtdCofDV3xdn8+CAwCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgImE/0pu2EqbdKPnNXfuOBrXsf4h85ee1yzfsMbzmtxXfu95zYW2ds9rJGnK/xzufdE/eV8yIW2U5zXd2SM9r+GXoGAw4w4IAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADDBh5GmmKff3el5zc1pZ+I61rz//ZTnNflxfLDoQPro6fEDcpwlLWWe14z+4E+e11z0vAIYONwBAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAm+DDSFLP8zVWe1+x76MW4jvWHVf/L+yLv48WlOhKKa92yzE2e1+zs+obnNZF1+Z7XDP/sQ89rgMGMOyAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQfRppiJv23es9r5l54Mq5jjZ72V89rNk17La5jeTVt5PG41v3npoXeFz3l/cNI0w4d9rzGeV4BDG7cAQEATBAgAIAJTwGqqqrSzJkzlZGRoZycHC1cuFBNTU0x+5w7d04VFRUaO3asrr/+ei1evFgdHR0JHRoAMPR5ClBdXZ0qKirU0NCgd955Rz09PZo/f766urqi+zz++ON6++23tX37dtXV1enEiRO6//77Ez44AGBo8/QmhD179sR8XV1drZycHDU2NmrOnDkKh8P61a9+pa1bt+q73/2uJGnLli267bbb1NDQoG9/+9uJmxwAMKR9rdeAwuGwJCkrK0uS1NjYqJ6eHpWUlET3mTJliiZMmKD6+r7fndXd3a1IJBKzAQBSX9wB6u3t1Zo1azR79mxNnTpVktTe3q4RI0ZozJgxMfvm5uaqvb29z+9TVVWlQCAQ3fLz8+MdCQAwhMQdoIqKCh05ckSvv/761xpg7dq1CofD0e348fh+fgMAMLTE9YOoq1ev1u7du7Vv3z6NHz8++ngwGNT58+d1+vTpmLugjo4OBYPBPr+X3++X3++PZwwAwBDm6Q7IOafVq1drx44d2rt3rwoKCmKenzFjhtLT01VTUxN9rKmpSceOHVNxcXFiJgYApARPd0AVFRXaunWrdu3apYyMjOjrOoFAQKNGjVIgENDy5ctVWVmprKwsZWZm6rHHHlNxcTHvgAMAxPAUoE2bNkmS5s6dG/P4li1btGzZMknSL37xCw0bNkyLFy9Wd3e3SktL9ctf/jIhwwIAUofPOTeoPuMwEokoEAhorhYozZduPQ4AwKMLrke12qVwOKzMzMx+9+Oz4AAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAlPAaqqqtLMmTOVkZGhnJwcLVy4UE1NTTH7zJ07Vz6fL2Z79NFHEzo0AGDo8xSguro6VVRUqKGhQe+88456eno0f/58dXV1xey3YsUKtbW1Rbf169cndGgAwNCX5mXnPXv2xHxdXV2tnJwcNTY2as6cOdHHR48erWAwmJgJAQAp6Wu9BhQOhyVJWVlZMY+/9tprys7O1tSpU7V27VqdPXu23+/R3d2tSCQSswEAUp+nO6Av6+3t1Zo1azR79mxNnTo1+vhDDz2kiRMnKhQK6fDhw3r66afV1NSkt956q8/vU1VVpRdeeCHeMQAAQ5TPOefiWbhq1Sr99re/1fvvv6/x48f3u9/evXs1b948NTc3a/LkyZc9393dre7u7ujXkUhE+fn5mqsFSvOlxzMaAMDQBdejWu1SOBxWZmZmv/vFdQe0evVq7d69W/v27btifCSpqKhIkvoNkN/vl9/vj2cMAMAQ5ilAzjk99thj2rFjh2pra1VQUHDVNYcOHZIk5eXlxTUgACA1eQpQRUWFtm7dql27dikjI0Pt7e2SpEAgoFGjRqmlpUVbt27Vvffeq7Fjx+rw4cN6/PHHNWfOHE2fPj0p/wAAgKHJ02tAPp+vz8e3bNmiZcuW6fjx4/re976nI0eOqKurS/n5+Vq0aJGeeeaZK/494JdFIhEFAgFeAwKAISoprwFdrVX5+fmqq6vz8i0BANcoPgsOAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGAizXqAr3LOSZIuqEdyxsMAADy7oB5Jf/vveX8GXYA6OzslSe/r34wnAQB8HZ2dnQoEAv0+73NXS9QA6+3t1YkTJ5SRkSGfzxfzXCQSUX5+vo4fP67MzEyjCe1xHi7hPFzCebiE83DJYDgPzjl1dnYqFApp2LD+X+kZdHdAw4YN0/jx46+4T2Zm5jV9gX2B83AJ5+ESzsMlnIdLrM/Dle58vsCbEAAAJggQAMDEkAqQ3+/XunXr5Pf7rUcxxXm4hPNwCefhEs7DJUPpPAy6NyEAAK4NQ+oOCACQOggQAMAEAQIAmCBAAAATQyZAGzdu1I033qiRI0eqqKhIH3zwgfVIA+7555+Xz+eL2aZMmWI9VtLt27dP9913n0KhkHw+n3bu3BnzvHNOzz33nPLy8jRq1CiVlJTo6NGjNsMm0dXOw7Jlyy67PsrKymyGTZKqqirNnDlTGRkZysnJ0cKFC9XU1BSzz7lz51RRUaGxY8fq+uuv1+LFi9XR0WE0cXL8Pedh7ty5l10Pjz76qNHEfRsSAXrjjTdUWVmpdevW6cMPP1RhYaFKS0t18uRJ69EG3B133KG2trbo9v7771uPlHRdXV0qLCzUxo0b+3x+/fr1evnll7V582bt379f1113nUpLS3Xu3LkBnjS5rnYeJKmsrCzm+ti2bdsATph8dXV1qqioUENDg9555x319PRo/vz56urqiu7z+OOP6+2339b27dtVV1enEydO6P777zecOvH+nvMgSStWrIi5HtavX280cT/cEDBr1ixXUVER/frixYsuFAq5qqoqw6kG3rp161xhYaH1GKYkuR07dkS/7u3tdcFg0L344ovRx06fPu38fr/btm2bwYQD46vnwTnnli5d6hYsWGAyj5WTJ086Sa6urs45d+nffXp6utu+fXt0n48++shJcvX19VZjJt1Xz4Nzzn3nO99xP/jBD+yG+jsM+jug8+fPq7GxUSUlJdHHhg0bppKSEtXX1xtOZuPo0aMKhUKaNGmSHn74YR07dsx6JFOtra1qb2+PuT4CgYCKioquyeujtrZWOTk5uvXWW7Vq1SqdOnXKeqSkCofDkqSsrCxJUmNjo3p6emKuhylTpmjChAkpfT189Tx84bXXXlN2dramTp2qtWvX6uzZsxbj9WvQfRjpV3322We6ePGicnNzYx7Pzc3Vxx9/bDSVjaKiIlVXV+vWW29VW1ubXnjhBd199906cuSIMjIyrMcz0d7eLkl9Xh9fPHetKCsr0/3336+CggK1tLToRz/6kcrLy1VfX6/hw4dbj5dwvb29WrNmjWbPnq2pU6dKunQ9jBgxQmPGjInZN5Wvh77OgyQ99NBDmjhxokKhkA4fPqynn35aTU1NeuuttwynjTXoA4S/KS8vj/55+vTpKioq0sSJE/Xmm29q+fLlhpNhMHjggQeif542bZqmT5+uyZMnq7a2VvPmzTOcLDkqKip05MiRa+J10Cvp7zysXLky+udp06YpLy9P8+bNU0tLiyZPnjzQY/Zp0P8VXHZ2toYPH37Zu1g6OjoUDAaNphocxowZo1tuuUXNzc3Wo5j54hrg+rjcpEmTlJ2dnZLXx+rVq7V792699957Mb++JRgM6vz58zp9+nTM/ql6PfR3HvpSVFQkSYPqehj0ARoxYoRmzJihmpqa6GO9vb2qqalRcXGx4WT2zpw5o5aWFuXl5VmPYqagoEDBYDDm+ohEItq/f/81f318+umnOnXqVEpdH845rV69Wjt27NDevXtVUFAQ8/yMGTOUnp4ecz00NTXp2LFjKXU9XO089OXQoUOSNLiuB+t3Qfw9Xn/9def3+111dbX74x//6FauXOnGjBnj2tvbrUcbUD/84Q9dbW2ta21tdb/73e9cSUmJy87OdidPnrQeLak6OzvdwYMH3cGDB50kt2HDBnfw4EH35z//2Tnn3M9+9jM3ZswYt2vXLnf48GG3YMECV1BQ4D7//HPjyRPrSuehs7PTPfHEE66+vt61tra6d999133rW99yN998szt37pz16AmzatUqFwgEXG1trWtra4tuZ8+eje7z6KOPugkTJri9e/e6AwcOuOLiYldcXGw4deJd7Tw0Nze7H//4x+7AgQOutbXV7dq1y02aNMnNmTPHePJYQyJAzjn3yiuvuAkTJrgRI0a4WbNmuYaGBuuRBtySJUtcXl6eGzFihLvhhhvckiVLXHNzs/VYSffee+85SZdtS5cudc5deiv2s88+63Jzc53f73fz5s1zTU1NtkMnwZXOw9mzZ938+fPduHHjXHp6ups4caJbsWJFyv1PWl///JLcli1bovt8/vnn7vvf/777xje+4UaPHu0WLVrk2tra7IZOgqudh2PHjrk5c+a4rKws5/f73U033eSefPJJFw6HbQf/Cn4dAwDAxKB/DQgAkJoIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABP/D3Yeicg6V/afAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x11[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([784])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x11.reshape(-1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, z_dim, hidden_dim, out_dim) -> None:\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(z_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, out_dim)\n",
    "        self.softplus = nn.Softplus()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, z):\n",
    "        hidden = self.softplus(self.fc1(z))\n",
    "        loc_img = self.sigmoid(self.fc2(hidden))\n",
    "        return loc_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, z_dim) -> None:\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc21 = nn.Linear(hidden_dim, z_dim)\n",
    "        self.fc22 = nn.Linear(hidden_dim, z_dim)\n",
    "\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.reshape(-1, self.input_dim)\n",
    "        hidden = self.softplus(self.fc1(x))\n",
    "        z_loc = self.fc21(hidden)\n",
    "        z_scale = torch.exp(self.fc22(hidden))\n",
    "        return z_loc, z_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(784, 400, 50)\n",
    "decoder = Decoder(50, 400, 784)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given an image  the forward call of Encoder returns a mean and covariance that together parameterize a (diagonal) Gaussian distribution in latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim = 784, hidden_dim = 400, z_dim = 50, out_dim = 784, use_cuda = False) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(input_dim, hidden_dim, z_dim)\n",
    "        self.decoder = Decoder(z_dim, hidden_dim, out_dim)\n",
    "\n",
    "        if use_cuda:\n",
    "            self.cuda()\n",
    "        self.use_cuda = use_cuda\n",
    "        self.z_dim = z_dim\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "    def model(self, x):\n",
    "        pyro.module(\"decoder\", self.decoder)\n",
    "        with pyro.plate(\"data\", x.shape[0]):\n",
    "            #p(z)\n",
    "            z_loc = x.new_zeros(torch.Size((x.shape[0], self.z_dim)))\n",
    "            #we use new_zeros and new_ones to ensure that newly created tensors are on the same GPU device.\n",
    "            z_scale = x.new_ones(torch.Size((x.shape[0], self.z_dim)))\n",
    "            # sample code from prior\n",
    "            z = pyro.sample(\"latent\", dist.Normal(z_loc, z_scale).to_event(1))\n",
    "            # use of .to_event(1) when sampling from the latent z - this ensures that instead of \n",
    "            # treating our sample as being generated from a univariate normal with batch_size = z_dim, \n",
    "            # we treat them as being generated from a multivariate normal distribution with diagonal covariance. \n",
    "            # As such, the log probabilities along each dimension is summed out when we evaluate .log_prob for \n",
    "            # a “latent” sample.\n",
    "            loc_img = self.decoder(z)\n",
    "            pyro.sample(\"obs\", dist.Bernoulli(loc_img).to_event(1), obs=x.reshape(-1, self.input_dim)) \n",
    "\n",
    "    \n",
    "    def guide(self, x):\n",
    "        pyro.module(\"encoder\", self.encoder)\n",
    "        with pyro.plate(\"data\", x.shape[0]):\n",
    "            z_loc, z_scale = self.encoder(x)\n",
    "            pyro.sample(\"latent\", dist.Normal(z_loc, z_scale).to_event(1))\n",
    "\n",
    "\n",
    "\n",
    "    def reconstruct(self, x):\n",
    "        z_loc, z_scale = self.encoder(x)\n",
    "        z = dist.Normal(z_loc, z_scale).sample()\n",
    "        loc_img = self.decoder(z)\n",
    "        return loc_img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The point we’d like to make here is that the two Modules encoder and decoder are attributes of VAE (which itself inherits from nn.Module). This has the consequence they are both automatically registered as belonging to the VAE module. So, for example, when we call parameters() on an instance of VAE, PyTorch will know to return all the relevant parameters. It also means that if we’re running on a GPU, the call to cuda() will move all the parameters of all the (sub)modules into GPU memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.optim import Adam\n",
    "from pyro.infer import SVI, Trace_ELBO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1.0e-3\n",
    "USE_CUDA = False\n",
    "NUM_EPOCKS = 1\n",
    "TEST_FREQUENCY = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.clear_param_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VAE(use_cuda=USE_CUDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam_args = {\"lr\": LR}\n",
    "optimizer = Adam(adam_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "svi = SVI(vae.model, vae.guide, optimizer, loss=Trace_ELBO())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error while computing log_prob at site 'obs':\nExpected value argument (Tensor of shape (1, 784)) to be within the support (Boolean()) of the distribution Bernoulli(probs: torch.Size([1, 784])), but found invalid values:\ntensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.1216, 0.1569, 0.5059, 0.9176, 0.9176, 0.6235, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2667,\n         0.5882, 0.9373, 0.9961, 0.9922, 0.9922, 0.9922, 0.8431, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6118, 0.7882,\n         0.9961, 0.9961, 0.9961, 0.9451, 0.5882, 0.3843, 0.0314, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0745, 0.6039, 0.9961,\n         0.9255, 0.7961, 0.3255, 0.1529, 0.1176, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5647, 0.9922,\n         0.5686, 0.0471, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0392, 0.5059, 0.8706, 0.3059,\n         0.3098, 0.0314, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5255, 0.9922, 0.6549,\n         0.0314, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.9961,\n         0.3059, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7882,\n         0.9922, 0.8863, 0.2706, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2157, 0.0235, 0.0000,\n         0.0706, 0.5020, 0.9922, 0.9451, 0.1608, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0980, 0.8039, 0.9216, 0.3608,\n         0.0000, 0.0000, 0.0784, 0.9922, 0.9922, 0.2275, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9059, 0.9608, 0.4235,\n         0.0000, 0.0000, 0.0000, 0.5176, 0.9922, 0.7255, 0.0549, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4745, 0.9608,\n         0.9961, 0.9961, 0.9961, 0.8510, 0.9961, 0.8745, 0.1961, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.4549, 0.6471, 0.9137, 0.9137, 0.9176, 0.7059, 0.1529, 0.0118, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000]])\n       Trace Shapes:            \n        Param Sites:            \ndecoder$$$fc1.weight 400  50    \n  decoder$$$fc1.bias     400    \ndecoder$$$fc2.weight 784 400    \n  decoder$$$fc2.bias     784    \n       Sample Sites:            \n         latent dist   1   |  50\n               value   1   |  50\n            log_prob   1   |    \n            obs dist   1   | 784\n               value   1   | 784",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyro/poutine/trace_struct.py:230\u001b[0m, in \u001b[0;36mTrace.compute_log_prob\u001b[0;34m(self, site_filter)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 230\u001b[0m     log_p \u001b[39m=\u001b[39m site[\u001b[39m\"\u001b[39;49m\u001b[39mfn\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mlog_prob(\n\u001b[1;32m    231\u001b[0m         site[\u001b[39m\"\u001b[39;49m\u001b[39mvalue\u001b[39;49m\u001b[39m\"\u001b[39;49m], \u001b[39m*\u001b[39;49msite[\u001b[39m\"\u001b[39;49m\u001b[39margs\u001b[39;49m\u001b[39m\"\u001b[39;49m], \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msite[\u001b[39m\"\u001b[39;49m\u001b[39mkwargs\u001b[39;49m\u001b[39m\"\u001b[39;49m]\n\u001b[1;32m    232\u001b[0m     )\n\u001b[1;32m    233\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/distributions/independent.py:99\u001b[0m, in \u001b[0;36mIndependent.log_prob\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlog_prob\u001b[39m(\u001b[39mself\u001b[39m, value):\n\u001b[0;32m---> 99\u001b[0m     log_prob \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbase_dist\u001b[39m.\u001b[39;49mlog_prob(value)\n\u001b[1;32m    100\u001b[0m     \u001b[39mreturn\u001b[39;00m _sum_rightmost(log_prob, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreinterpreted_batch_ndims)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/distributions/bernoulli.py:102\u001b[0m, in \u001b[0;36mBernoulli.log_prob\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_args:\n\u001b[0;32m--> 102\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_sample(value)\n\u001b[1;32m    103\u001b[0m logits, value \u001b[39m=\u001b[39m broadcast_all(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogits, value)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/distributions/distribution.py:294\u001b[0m, in \u001b[0;36mDistribution._validate_sample\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m valid\u001b[39m.\u001b[39mall():\n\u001b[0;32m--> 294\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    295\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mExpected value argument \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    296\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(value)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m of shape \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtuple\u001b[39m(value\u001b[39m.\u001b[39mshape)\u001b[39m}\u001b[39;00m\u001b[39m) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    297\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mto be within the support (\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mrepr\u001b[39m(support)\u001b[39m}\u001b[39;00m\u001b[39m) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    298\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mof the distribution \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mrepr\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    299\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut found invalid values:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mvalue\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    300\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Expected value argument (Tensor of shape (1, 784)) to be within the support (Boolean()) of the distribution Bernoulli(probs: torch.Size([1, 784])), but found invalid values:\ntensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.1216, 0.1569, 0.5059, 0.9176, 0.9176, 0.6235, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2667,\n         0.5882, 0.9373, 0.9961, 0.9922, 0.9922, 0.9922, 0.8431, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6118, 0.7882,\n         0.9961, 0.9961, 0.9961, 0.9451, 0.5882, 0.3843, 0.0314, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0745, 0.6039, 0.9961,\n         0.9255, 0.7961, 0.3255, 0.1529, 0.1176, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5647, 0.9922,\n         0.5686, 0.0471, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0392, 0.5059, 0.8706, 0.3059,\n         0.3098, 0.0314, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5255, 0.9922, 0.6549,\n         0.0314, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.9961,\n         0.3059, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7882,\n         0.9922, 0.8863, 0.2706, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2157, 0.0235, 0.0000,\n         0.0706, 0.5020, 0.9922, 0.9451, 0.1608, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0980, 0.8039, 0.9216, 0.3608,\n         0.0000, 0.0000, 0.0784, 0.9922, 0.9922, 0.2275, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9059, 0.9608, 0.4235,\n         0.0000, 0.0000, 0.0000, 0.5176, 0.9922, 0.7255, 0.0549, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4745, 0.9608,\n         0.9961, 0.9961, 0.9961, 0.8510, 0.9961, 0.8745, 0.1961, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.4549, 0.6471, 0.9137, 0.9137, 0.9176, 0.7059, 0.1529, 0.0118, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000]])",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [108], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m svi\u001b[39m.\u001b[39mstep(x11)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyro/infer/svi.py:145\u001b[0m, in \u001b[0;36mSVI.step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[39m# get loss and compute gradients\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[39mwith\u001b[39;00m poutine\u001b[39m.\u001b[39mtrace(param_only\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39mas\u001b[39;00m param_capture:\n\u001b[0;32m--> 145\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloss_and_grads(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mguide, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    147\u001b[0m params \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(\n\u001b[1;32m    148\u001b[0m     site[\u001b[39m\"\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39munconstrained() \u001b[39mfor\u001b[39;00m site \u001b[39min\u001b[39;00m param_capture\u001b[39m.\u001b[39mtrace\u001b[39m.\u001b[39mnodes\u001b[39m.\u001b[39mvalues()\n\u001b[1;32m    149\u001b[0m )\n\u001b[1;32m    151\u001b[0m \u001b[39m# actually perform gradient steps\u001b[39;00m\n\u001b[1;32m    152\u001b[0m \u001b[39m# torch.optim objects gets instantiated for any params that haven't been seen yet\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyro/infer/trace_elbo.py:140\u001b[0m, in \u001b[0;36mTrace_ELBO.loss_and_grads\u001b[0;34m(self, model, guide, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[1;32m    139\u001b[0m \u001b[39m# grab a trace from the generator\u001b[39;00m\n\u001b[0;32m--> 140\u001b[0m \u001b[39mfor\u001b[39;00m model_trace, guide_trace \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_traces(model, guide, args, kwargs):\n\u001b[1;32m    141\u001b[0m     loss_particle, surrogate_loss_particle \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_differentiable_loss_particle(\n\u001b[1;32m    142\u001b[0m         model_trace, guide_trace\n\u001b[1;32m    143\u001b[0m     )\n\u001b[1;32m    144\u001b[0m     loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss_particle \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_particles\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyro/infer/elbo.py:182\u001b[0m, in \u001b[0;36mELBO._get_traces\u001b[0;34m(self, model, guide, args, kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    181\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_particles):\n\u001b[0;32m--> 182\u001b[0m         \u001b[39myield\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_trace(model, guide, args, kwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyro/infer/trace_elbo.py:57\u001b[0m, in \u001b[0;36mTrace_ELBO._get_trace\u001b[0;34m(self, model, guide, args, kwargs)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_trace\u001b[39m(\u001b[39mself\u001b[39m, model, guide, args, kwargs):\n\u001b[1;32m     53\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[39m    Returns a single trace from the guide, and the model that is run\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[39m    against it.\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     model_trace, guide_trace \u001b[39m=\u001b[39m get_importance_trace(\n\u001b[1;32m     58\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mflat\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_plate_nesting, model, guide, args, kwargs\n\u001b[1;32m     59\u001b[0m     )\n\u001b[1;32m     60\u001b[0m     \u001b[39mif\u001b[39;00m is_validation_enabled():\n\u001b[1;32m     61\u001b[0m         check_if_enumerated(guide_trace)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyro/infer/enum.py:75\u001b[0m, in \u001b[0;36mget_importance_trace\u001b[0;34m(graph_type, max_plate_nesting, model, guide, args, kwargs, detach)\u001b[0m\n\u001b[1;32m     72\u001b[0m guide_trace \u001b[39m=\u001b[39m prune_subsample_sites(guide_trace)\n\u001b[1;32m     73\u001b[0m model_trace \u001b[39m=\u001b[39m prune_subsample_sites(model_trace)\n\u001b[0;32m---> 75\u001b[0m model_trace\u001b[39m.\u001b[39;49mcompute_log_prob()\n\u001b[1;32m     76\u001b[0m guide_trace\u001b[39m.\u001b[39mcompute_score_parts()\n\u001b[1;32m     77\u001b[0m \u001b[39mif\u001b[39;00m is_validation_enabled():\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyro/poutine/trace_struct.py:236\u001b[0m, in \u001b[0;36mTrace.compute_log_prob\u001b[0;34m(self, site_filter)\u001b[0m\n\u001b[1;32m    234\u001b[0m     _, exc_value, traceback \u001b[39m=\u001b[39m sys\u001b[39m.\u001b[39mexc_info()\n\u001b[1;32m    235\u001b[0m     shapes \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mformat_shapes(last_site\u001b[39m=\u001b[39msite[\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m--> 236\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    237\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mError while computing log_prob at site \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    238\u001b[0m             name, exc_value, shapes\n\u001b[1;32m    239\u001b[0m         )\n\u001b[1;32m    240\u001b[0m     )\u001b[39m.\u001b[39mwith_traceback(traceback) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    241\u001b[0m site[\u001b[39m\"\u001b[39m\u001b[39munscaled_log_prob\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m log_p\n\u001b[1;32m    242\u001b[0m log_p \u001b[39m=\u001b[39m scale_and_mask(log_p, site[\u001b[39m\"\u001b[39m\u001b[39mscale\u001b[39m\u001b[39m\"\u001b[39m], site[\u001b[39m\"\u001b[39m\u001b[39mmask\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyro/poutine/trace_struct.py:230\u001b[0m, in \u001b[0;36mTrace.compute_log_prob\u001b[0;34m(self, site_filter)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mlog_prob\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m site:\n\u001b[1;32m    229\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 230\u001b[0m         log_p \u001b[39m=\u001b[39m site[\u001b[39m\"\u001b[39;49m\u001b[39mfn\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mlog_prob(\n\u001b[1;32m    231\u001b[0m             site[\u001b[39m\"\u001b[39;49m\u001b[39mvalue\u001b[39;49m\u001b[39m\"\u001b[39;49m], \u001b[39m*\u001b[39;49msite[\u001b[39m\"\u001b[39;49m\u001b[39margs\u001b[39;49m\u001b[39m\"\u001b[39;49m], \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msite[\u001b[39m\"\u001b[39;49m\u001b[39mkwargs\u001b[39;49m\u001b[39m\"\u001b[39;49m]\n\u001b[1;32m    232\u001b[0m         )\n\u001b[1;32m    233\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    234\u001b[0m         _, exc_value, traceback \u001b[39m=\u001b[39m sys\u001b[39m.\u001b[39mexc_info()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/distributions/independent.py:99\u001b[0m, in \u001b[0;36mIndependent.log_prob\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlog_prob\u001b[39m(\u001b[39mself\u001b[39m, value):\n\u001b[0;32m---> 99\u001b[0m     log_prob \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbase_dist\u001b[39m.\u001b[39;49mlog_prob(value)\n\u001b[1;32m    100\u001b[0m     \u001b[39mreturn\u001b[39;00m _sum_rightmost(log_prob, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreinterpreted_batch_ndims)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/distributions/bernoulli.py:102\u001b[0m, in \u001b[0;36mBernoulli.log_prob\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlog_prob\u001b[39m(\u001b[39mself\u001b[39m, value):\n\u001b[1;32m    101\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_args:\n\u001b[0;32m--> 102\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_sample(value)\n\u001b[1;32m    103\u001b[0m     logits, value \u001b[39m=\u001b[39m broadcast_all(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogits, value)\n\u001b[1;32m    104\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m-\u001b[39mbinary_cross_entropy_with_logits(logits, value, reduction\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mnone\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/distributions/distribution.py:294\u001b[0m, in \u001b[0;36mDistribution._validate_sample\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    292\u001b[0m valid \u001b[39m=\u001b[39m support\u001b[39m.\u001b[39mcheck(value)\n\u001b[1;32m    293\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m valid\u001b[39m.\u001b[39mall():\n\u001b[0;32m--> 294\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    295\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mExpected value argument \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    296\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(value)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m of shape \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtuple\u001b[39m(value\u001b[39m.\u001b[39mshape)\u001b[39m}\u001b[39;00m\u001b[39m) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    297\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mto be within the support (\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mrepr\u001b[39m(support)\u001b[39m}\u001b[39;00m\u001b[39m) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    298\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mof the distribution \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mrepr\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    299\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut found invalid values:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mvalue\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    300\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Error while computing log_prob at site 'obs':\nExpected value argument (Tensor of shape (1, 784)) to be within the support (Boolean()) of the distribution Bernoulli(probs: torch.Size([1, 784])), but found invalid values:\ntensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.1216, 0.1569, 0.5059, 0.9176, 0.9176, 0.6235, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2667,\n         0.5882, 0.9373, 0.9961, 0.9922, 0.9922, 0.9922, 0.8431, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6118, 0.7882,\n         0.9961, 0.9961, 0.9961, 0.9451, 0.5882, 0.3843, 0.0314, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0745, 0.6039, 0.9961,\n         0.9255, 0.7961, 0.3255, 0.1529, 0.1176, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5647, 0.9922,\n         0.5686, 0.0471, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0392, 0.5059, 0.8706, 0.3059,\n         0.3098, 0.0314, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5255, 0.9922, 0.6549,\n         0.0314, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.9961,\n         0.3059, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7882,\n         0.9922, 0.8863, 0.2706, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2157, 0.0235, 0.0000,\n         0.0706, 0.5020, 0.9922, 0.9451, 0.1608, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0980, 0.8039, 0.9216, 0.3608,\n         0.0000, 0.0000, 0.0784, 0.9922, 0.9922, 0.2275, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9059, 0.9608, 0.4235,\n         0.0000, 0.0000, 0.0000, 0.5176, 0.9922, 0.7255, 0.0549, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4745, 0.9608,\n         0.9961, 0.9961, 0.9961, 0.8510, 0.9961, 0.8745, 0.1961, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.4549, 0.6471, 0.9137, 0.9137, 0.9176, 0.7059, 0.1529, 0.0118, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000]])\n       Trace Shapes:            \n        Param Sites:            \ndecoder$$$fc1.weight 400  50    \n  decoder$$$fc1.bias     400    \ndecoder$$$fc2.weight 784 400    \n  decoder$$$fc2.bias     784    \n       Sample Sites:            \n         latent dist   1   |  50\n               value   1   |  50\n            log_prob   1   |    \n            obs dist   1   | 784\n               value   1   | 784"
     ]
    }
   ],
   "source": [
    "svi.step(x11)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 784])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_loc = x.new_zeros(torch.Size((128,50)))\n",
    "            #we use new_zeros and new_ones to ensure that newly created tensors are on the same GPU device.\n",
    "z_scale = x.new_ones(torch.Size((128, 50)))\n",
    "z = pyro.sample(\"latent\", dist.Normal(z_loc, z_scale))\n",
    "img = vae.decoder(z)\n",
    "pyro.sample(\"data\", dist.Bernoulli(img)).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
