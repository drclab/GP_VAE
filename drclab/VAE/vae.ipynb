{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[VAE](https://en.wikipedia.org/wiki/Variational_autoencoder)\n",
    "\n",
    "[Understanding Variational Autoencoders (VAEs)](https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GP 1D Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os,sys\n",
    "#sys.path.append(os.path.join(os.path.dirname(__file__), '../'))\n",
    "import numpy as np  \n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, Matern\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GP1D(Dataset):\n",
    "    def __init__(self, dataPoints=100, samples=10000, ingrid=True, x_lim = 1,\n",
    "                        seed=np.random.randint(20), kernel='rbf',ls = 0.1, nu=2.5):\n",
    "        self.dataPoints = dataPoints\n",
    "        self.samples = samples\n",
    "        self.ingrid = ingrid\n",
    "        self.x_lim = x_lim\n",
    "        self.seed = seed\n",
    "        self.Max_Points = 2 * dataPoints\n",
    "        self.ls = ls\n",
    "        self.nu = nu\n",
    "        self.kernel = kernel\n",
    "        np.random.seed(self.seed)\n",
    "        self.evalPoints, self.data = self.__simulatedata__()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.samples\n",
    "    \n",
    "    def __getitem__(self, idx=0):\n",
    "        return(self.evalPoints[:,idx], self.data[:,idx])\n",
    "\n",
    "\n",
    "    def __simulatedata__(self):\n",
    "        if self.kernel=='rbf':\n",
    "            gp = GaussianProcessRegressor(kernel=RBF(length_scale=self.ls))\n",
    "        elif self.kernel=='matern':\n",
    "            gp = GaussianProcessRegressor(kernel=Matern(length_scale=self.ls, nu=self.nu))\n",
    "        else:\n",
    "            return None\n",
    "        if (self.ingrid):\n",
    "            X_ = np.linspace(-self.x_lim, self.x_lim, self.dataPoints)\n",
    "            y_samples = gp.sample_y(X_[:, np.newaxis], self.samples)\n",
    "            # print(X_.shape, y_samples.shape)\n",
    "            return (X_.repeat(self.samples).reshape(X_.shape[0],self.samples) ,\n",
    "                        y_samples)\n",
    "        else:\n",
    "            X_ = np.linspace(-self.x_lim, self.x_lim, self.Max_Points)\n",
    "            X_ = np.random.choice(X_, (self.dataPoints,self.samples))\n",
    "            X_.sort(axis=0)\n",
    "            y_samples = np.zeros((self.dataPoints,self.samples))\n",
    "            for idx in range(self.samples):\n",
    "                x_ = X_[:,idx]\n",
    "                y_samples[:,idx] = gp.sample_y(x_[:, np.newaxis]).reshape(self.dataPoints,)\n",
    "            # print(X_.shape, y_samples.shape)\n",
    "            return (X_, y_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE = Encoder + Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden1_dim, hidden2_dim, z_dim) -> None:\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(input_dim, hidden1_dim)\n",
    "        self.linear2 = nn.Linear(hidden1_dim, hidden2_dim)\n",
    "        self.mu = nn.Linear(hidden2_dim, z_dim)\n",
    "        self.std = nn.Linear(hidden2_dim, z_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        hidden1 = torch.tanh(self.linear1(x))\n",
    "        hidden2 = torch.tanh_(self.linear2(hidden1))\n",
    "        z_mu = self.mu(hidden2)\n",
    "        z_std = self.std(hidden2)\n",
    "\n",
    "        return z_mu, z_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, z_dim, hidden1_dim, hidden2_dim, input_dim) -> None:\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(z_dim, hidden1_dim)\n",
    "        self.linear2 = nn.Linear(hidden1_dim, hidden2_dim)\n",
    "        self.out = nn.Linear(hidden2_dim, input_dim)\n",
    "\n",
    "    def forward(self, z):\n",
    "        hidden1 = torch.tanh(self.linear1(z))\n",
    "        hidden2 = torch.tanh(self.linear2(hidden1))\n",
    "        pred = self.out(hidden2)\n",
    "\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.randn_like(input, *, dtype=None, layout=None, device=None, requires_grad=False, memory_format=torch.preserve_format) \n",
    "\n",
    "Returns a tensor with the same size as input filled with random numbers from a normal **distribution with mean 0 and variance 1**\n",
    "\n",
    "torch.randn_like(input) is equivalent to torch.randn(input.size(), dtype=input.dtype, layout=input.layout, device=input.device)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden1_dim, hidden2_dim, z_dim) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(input_dim, hidden1_dim, hidden2_dim, z_dim)\n",
    "        self.decoder = Decoder(z_dim, hidden1_dim, hidden2_dim, input_dim)\n",
    "\n",
    "    def reparameterize(self, z_mu, z_std):\n",
    "        if self.training:\n",
    "            sd = torch.exp(z_std / 2)\n",
    "            eps = torch.randn_like(sd)\n",
    "            return eps.mul(sd).add_(z_mu)\n",
    "        else:\n",
    "            return z_mu\n",
    "\n",
    "    def forward(self, x):\n",
    "        z_mu, z_std = self.encoder(x)\n",
    "        z_sample = self.reparameterize(z_mu, z_std)\n",
    "        pred = self.decoder(z_sample)\n",
    "        return pred, z_mu, z_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(x, x_pred, mean, log_sd):\n",
    "    RCL = F.mse_loss(x_pred, x, reduction='sum')\n",
    "    KLD = -0.5*torch.sum(1+log_sd - mean.pow(2) - log_sd.exp())\n",
    "    return RCL + KLD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   ###### intializing data and model parameters\n",
    "input_dim = 100\n",
    "batch_size = 500    \n",
    "hidden_dim1 = 64\n",
    "hidden_dim2 = 32\n",
    "z_dim = 20\n",
    "samples = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### creating data, model and optimizer\n",
    "train_ds = GP1D(dataPoints=input_dim, samples=samples, ls=0.1)\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,10))\n",
    "ax = fig.add_subplot(111)\n",
    "for no, dt in enumerate(train_dl):\n",
    "        ax.plot(dt[0].reshape(-1,1), dt[1].reshape(-1,1), marker='o', markersize=3)\n",
    "        if no > 9: break\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y=f(x)$')\n",
    "ax.set_title('10 different function realizations at fixed 100 points\\n'\n",
    "    'sampled from a Gaussian process with RBF')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE(input_dim, hidden_dim1, hidden_dim2, z_dim)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-2)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' \n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = trange(200)\n",
    "for e in t:\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for i, x in enumerate(train_dl):\n",
    "        x = x[1].float().to(device)\n",
    "        optimizer.zero_grad()\n",
    "        x_pred, z_mu, z_sd = model(x)\n",
    "        loss = compute_loss(x, x_pred, z_mu, z_sd)\n",
    "        loss.backward()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        optimizer.step()\n",
    "    t.set_description(f'Loss is {total_loss/(samples*input_dim):.3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "z = torch.randn(5, z_dim).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    x_sample = model.decoder(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,10))\n",
    "ax = fig.add_subplot(111)\n",
    "for no, y in enumerate(x_sample):\n",
    "    ax.plot(train_ds.evalPoints[:,0], y.cpu().numpy(), marker='o', markersize=3)\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y=f(x)$')\n",
    "ax.set_title('5 different function realizations at fixed 100 points\\n'\n",
    "    'sampled from a VAE learned with prior as GP (RBF)')\n",
    "    #plt.savefig('plots/sample_prior_vae_1d_fixed.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observed = GP1D(input_dim, 1, ls=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observed[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_dict = model.decoder.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = observed[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_dict['linear1.bias'].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = f + np.random.randn(input_dim) * 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stan_data = {\n",
    "    \"p\": z_dim,\n",
    "    \"p1\": hidden_dim1,\n",
    "    \"p2\": hidden_dim2,\n",
    "    'n': input_dim,\n",
    "    \"W1\": decoder_dict['linear1.weight'].T.numpy(),\n",
    "    'B1': decoder_dict['linear1.bias'].T.numpy(),\n",
    "    'W2': decoder_dict['linear2.weight'].T.numpy(),\n",
    "    \"B2\": decoder_dict['linear2.bias'].T.numpy(),\n",
    "    \"W3\": decoder_dict['out.weight'].T.numpy(),\n",
    "    \"B3\": decoder_dict['out.bias'].T.numpy(),\n",
    "    'y': y\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cmdstanpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmdstanpy.install_cmdstan()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = cmdstanpy.CmdStanModel(stan_file='stan_1D.stan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit = sm.sample(data= stan_data, iter_sampling=2000, iter_warmup=500, chains=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = fit.stan_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit.diagnose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit.save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posterior Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(fit, var_names=[\"z\"], figsize=(22,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_forest(fit, var_names=\"z\", combined=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapoints = observed[0][0]\n",
    "df = pd.DataFrame(out['y2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(22,16))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(datapoints, f.reshape(-1,1), color=\"black\", label=\"Truth\")\n",
    "ax.scatter(datapoints, y.reshape(-1,1), s=46, label=\"Observations\")\n",
    "ax.fill_between(datapoints, \n",
    "                df.quantile(0.025).to_numpy(), \n",
    "                df.quantile(0.975).to_numpy(),\n",
    "                #facecolor = \"blue\",\n",
    "                color=\"blue\",\n",
    "                alpha= 0.2,\n",
    "                label = \"95% CI\"\n",
    ")\n",
    "ax.plot(datapoints, df.mean().to_numpy().reshape(-1,1), color=\"red\", alpha=0.7,\n",
    "    label=\"Posterior Mean\"\n",
    ")\n",
    "    \n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y=f(x)$')\n",
    "ax.set_title('Inference fit')\n",
    "ax.legend()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Aug 10 2022, 11:40:04) [GCC 11.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
